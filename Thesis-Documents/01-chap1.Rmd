<!--
This is for including Chapter 1.  Notice that it's also good practice to name your chunk.  This will help you debug potential issues as you knit.  The chunk above is called intro and the one below is called chapter1.  Feel free to change the name of the Rmd file as you wish, but don't forget to change it here from chap1.Rmd.
-->

<!--
The {#rmd-basics} text after the chapter declaration will allow us to link throughout the document back to the beginning of Chapter 1.  These labels will automatically be generated (if not specified) by changing the spaces to hyphens and capital letters to lowercase.  Look for the reference to this label at the beginning of Chapter 2.
-->

# Introduction to Trees, Random Forests, and the Bootstrap {#rmd-basics}

```{r}

library(rpart)

```


## CART

  We begin our discussion of CART (Classification and Regression Trees) by considering the following problem. Suppose we have data generated from the piecewise function $Y=f(X)+\varepsilon$ where [at this point insert a very non-linear piecewise function.] with Gaussian error $\varepsilon$. One approach to fitting a model to this data would be to fit a linear regression model. However, the particular form of the data is not well suited for linear regression due to the non-linearity of the data. Another approach we might try as follows. We might try to find a method that can fit a regression line to each piecewise component of the data. One such method is to fit CART trees. We fit a CART tree to the data and perform cost-complexity pruning to output the model shown in figure [figure not yet made]. In this example, we produced a regression surface which was able to take into account the piecewise, non-linear nature of the data and produce a reasonable estimate of the underlying data generating mechanism. \par
	
  The CART methodology allows us to fit a model that can take into account non-linear regression or classification surfaces. The basic idea of CART is that if we can split the predictors into roughly homogenous partitions in terms of the response $Y$, then we can fit a simple model to predict the response of each partition. CART trees were first introduced by @breiman1984 and are a flexible method, capable of handling classification and regression settings.  \par

  More formally, suppose we have a training data set $Z=\{Z_1,\ldots,Z_n\}$, where $Z_i=(X_i,Y_i)$ is an $p+1$ dimensional vector in $\mathbb{R}^{p+1}$. Here we have $X_i$ is a $p$-dimensional predictor variable and $Y_i$ is the response. In particular, we can consider $Z$ to be a $n\times (p+1)$ array where the rows are observations and the columns are the response and predictor variables. CART works by partitioning the data through binary recursive splits via optimizing some loss function. Trees in CART are called trees because partitioning the data through binary recursive splits forms a tree-like structure. We adopt notation evocative of this tree structure. Any subset of the training data $Z$ is called a node while the entire data set $Z$ is called the root node. Nodes are of two kinds: they are either terminal (sometimes called leaves) or not terminal. Non-terminal nodes are nodes that are split on in the tree growing process, while terminal nodes are nodes which are not split on. Generally, a node is a terminal node if some stopping rule is reached. Note that terminal nodes form a partition of the training data $Z$. \par
  
  
  We aim to grow a tree with roughly homogenous terminal nodes in terms of $Y$. As @breiman1984 note, there are several factors that we need to consider:
	\begin{enumerate}
		\item How to select splits of the data.
		
		\item When to stop splitting the data.
		
		\item How to assign classes or values to terminal nodes. 
	\end{enumerate}
	
	
  CART address point 1 decreasing the nodal impurity of a node $t$. Nodal impurity is defined through some nodal impurity measure $i(s,t)$, usually the GINI index in the classification setting and the residual sum of squares in the regression setting. We present the regression setting and then the classification setting. If we have a continuous response $Y$, then we could try to predict $Y$ by partitioning the data using the decision tree structure and predicting that points that fall within a particular partition will take on the average response on that partition. To choose the best binary split on the data, we need to search across the splitting variables and splitting points for the split which maximizes the reduction in the RSS between the parent node and daughter nodes. In particular, we want to find the split which maximizes $RSS_l(j,s)+RSS_r(j,s)$ where $j$ is the proposed splitting variable and $s$ is the proposed splitting point, and $RSS_l$ is the RSS of the proposed left node and $RSS_r$ is the RSS of the proposed right node. The assignment of response within a node is given by the average of the response. We present the following algorithm. \par
	
  \begin{algorithm}
		\caption{Construction of Regression tree}\label{regression tree}
		\begin{algorithmic}[1]
			\While {minimum node size not reached}
				\For {each node $t$ }
					\For {$j=1,\ldots,p$ and $s=1,\ldots,n$ }
					\State Compute $RSS_l(j,s)+RSS_r(j,s)$.
					\EndFor
					\State Pick the $(j,s)$ which maximizes $RSS_l(j,s)+RSS_r(j,s)$.
					\State Split the current node into left and right nodes according to $(j,s)$.
					\State Compute left and right averages, $\text{ave}(y_i|x_i\in t_l)$ and $\text{ave}(y_i|x_i\in t_r)$, respectively. 
				\EndFor
				\State Output the tree $T$.
			\EndWhile
		\end{algorithmic}
	\end{algorithm}
	
  The construction of classification trees is similar to the construction of regression trees except a different impurity measure must be used. For the node $t$ containing $N_t$ observations, let $$\hat{p}_{tk}=\frac{1}{N_t}\sum_{x_i\in t} \mathbb{I}(y_i=k)$$ where $k=0$ or $k=1$. Then $\hat{p}_{tk}$ measures the proportion of observations of class $k$ in node $t$. Using the two-class example, there are several impurity measures available in the classification setting. The most common measure is perhaps the Gini index  defined by $$2p(1-p).$$ Other options include misclassification error $$1-\max(p,1-p),$$ and cross-entropy $$-p\log(p)-(1-p)\log(1-p).$$ If $t_L$ and $t_R$ are left and right nodes proposed under the split, let $\hat{p}_{tL}$ and $\hat{p}_{tR}$ be proportion of observations falling into $t_L$ and $t_R$, respectively. Denote the Gini index of the left node $t_L$ by $G_{L}$ and denote the Gini index of the right node $t_R$ by $G_{R}$. Then our splitting criterion is to seek the spliting variable and splitting point which minimizes $$\hat{p}_{tL}G_L+\hat{p}_{tR}G_R.$$ The case is similar when using misclassification rate and cross-entropy as the impurity measures. \par
  
  Some issues with CART trees include overfitting and variability. While there is a stopping criterion for growing the CART trees, often times naively growing a tree can result in overfitting to the data. One method of alleviating overfitting is to employ cost-complexity pruning, which searches for an optimal tree that balances fitting a good predicitve tree model with overfitting to the data. We do not go into details here, but point the reader to @esl2009. \par
  
  A larger issue with trees is their sensitivity to small perturbations in the data. Allowing the data to vary even a bit can result in a different tree structure upon refitting. As a statistical learning method, this means that CART trees are not robust and suffer from high variance, even when constructed using cost-complexity pruning. To get around this issue of sensitivity the best solution is to employ bagging and use the random forest algorithm. \par
  
  
## Random Forests

  Among the limitations of CART discussed in the preceding sections, the biggest issues ARE perhaps the variability of the method and the issue of collinearity among predictors. While overfitting can be addressed using cost-complexity pruning, variability and collinearity are not fixed by pruning. Random forests deal with these two issues of CART by introducing a resampling and randomization mechanism. The natural order is to first discuss bagged forests before turning to random forests. \par
  
  One method of improving CART trees is to bag them. Bagging, which stand for bootstrap aggregating, is a variation reduction technique particularly useful for improving the predictive power of weak learners. We are interested in bagging CART trees to reduce the variability of single trees under slight perturbations of the data. Generally bagged ensembles of learners produce robust predictions in comparison to running the learner once. \par

  
  \begin{algorithm}
		\caption{Bagged Forest algorithm}\label{bagged forest}
		\begin{algorithmic}[1]
			\For {$b=1,\ldots, B$ }
			\State Draw a bootstrap sample $Z_b^*$ of size $N$ from the training data $Z$.
			\State Grow a CART tree $T_b$ on each bootstrap sample $Z_b^*$.
			\EndFor
			\State Output the bagged forest ensemble $\{T_b\}_{b=1}^B$.
		\end{algorithmic}
	\end{algorithm}
  
### How Bagged Forests Work 

  Bagged forests are an ensemble obtained by taking many bootstrap samples of the data and fitting a tree to each bootstrapped dataset.  In this case, we grow the trees quite deep and do not employ cost-complexity pruning. The idea behind this decision is that we want to sufficiently explore the feature space using a single baggedtree of the forest ensemble, and since we are also taking an average of the trees, we are fine with overfitting at least a little bit. As the algorithm above for bagged forests indicates, the output is an ensemble of trees $\{T_b\}_{b=1}^B$. Given a test data point, we form a prediction by taking the average of predictions given by the tree ensemble:
	$$\hat{f}_{bf}^B(x)=\frac{1}{B}\sum_{b=1}^B T_b(x),$$ where $\hat{f}_{bf}^B$ indicates we are taking the bagged forest estimate of the underlying regression or classification function using $B$ many trees. Note that it is important to be consistent in the choice of splitting criterion throughout the bagging process. \par

  While bagging is a general method, that can be applied, for example, to methods such as ordinary least squares or general linear models, it has been shown by @hall1999 and @chen2003 that bagging is especially effective when used on highly non-linear models such as CART trees. Under ideal conditions, bagged estimates of non-linear estimators reduces both the bias and variance of the estimator. So bagging trees seem to be effective because of the reduction to the variance of the estimator, which produces a more robust prediction. \par

  When we bag trees, each observation in the data is not used within each individual tree. A bootstrap replicate $Z_b$ of the data $Z$ will likely exclude some of the observations within the data. The observations used within the bootstrap replicate $Z_b$ is called the in-bag data  while the observations not used within the bootstrap replicate $Z_b$ is called the out-of-bag (OOB) data and is denoted by $\bar{Z}_b$. The OOB data allows us to approximate the test error of the ensemble as follows. For simplicity suppose we are in the regression setting (the classification setting is similar). Then the OOB estimate of the MSE of the bagged forest is given by $$MSE_{\textup{OOB}}(T;Z)=\frac{1}{B}\sum_{b=1}^B MSE(T_b;\bar{Z}_b).$$ As the number of trees in the ensemble grows, the OOB estimate of the MSE for the bagged forest converges to the LOOCV estimate of the MSE for forest @esl2009. \par

  A weakness of bagged forests is collinearity between trees @esl2009. This collinearity between trees grown from the bootstrap sample can be addressed by adding a randomization mechanism in the tree growing process. When we grow a tree from a bootstrap sample, even with the randomness induced by resampling from the data, certain features may be explored at the expense of other just as interesting features. This is a particular issue with collinear predictors. If two predictors are collinear, then the bagged forest might consistently choose one predictor over another even if the predictor not chosen leads to splits that are just as informative. This is due in part to the greedy nature of the CART algorithm when searching for optimal splits over the feature space (Cite ESL or some similar textbook for this point). The algorithm does not take into account the second best or third best splits. Furthermore, it is not difficult to see that bagging will generally produce an ensemble of trees that are quite similar to one another, subject to some perturbations. These trees will be strongly correlated with other trees in the ensemble, so if there is a less explored part of the feature space, then the ensemble will struggle to produce good predictions over that region. \par
  
### The Random Forest Algorithm

  Random Forests deal with this issue of correlated trees and collinearity among predictors by choosing at random only $m\leq p$ of the predictors to be considered as candidate splitting variables at each split in each tree in the ensemble. This randomness further reduces the variance of the bagged forest by decorrelating the trees in the ensemble. Furthermore, while individually the trees may perform worse than a single pruned tree, collectively the ensemble has a better chance of exploring the feature space fully. \par
	\begin{algorithm}
		\caption{Random Forest algorithm}\label{random forest}
		\begin{algorithmic}[1]
			\For {$b=1,\ldots, B$ }
			\State Draw a bootstrap sample $Z_b^*$ of size $N$ from the training data $Z$.
			\State Grow the CART tree $T_b$ on $Z_b^*$ with the following modification:
			\While {minimum node size $n_{\min}$ not reached across $T_b$} 
			\State Select $m\leq p$ candidate splitting variables at random.
			\State Pick the best splitting variable and splitting point among the $m$ variables selected at random.
			\State Split the node into two daughter nodes.
			\EndWhile
			\EndFor
			\State Output the random forest ensemble $\{T_b\}_{b=1}^B$.
		\end{algorithmic}
	\end{algorithm}

  To form a prediction at a test point $x$, we have in the regression setting $$\hat{f}_{rf}^B(x)=\frac{1}{B}\sum_{b=1}^B T_b(x).$$ \par
  
### Variable Importance Measures
	
  One of the outputs of random forests is the variable importance (VI) measure. There are two main variable importance measures in common use, with the choice of VI measure varying depending on the setting and splitting criterion chosen. The first choice is the Mean Decrease in Impurity (MDI) which is typically used in the classification setting where the GINI index or Shannon entropy is used as the splitting criterion. The second choice is Mean Decrease in Accuracy (MDA) which is typically used in the regression setting where RSS has been used as the splitting criterion.  The idea of MDI is to find how much the nodal impurity $p(t)\Delta i(s,t)$ decreases for all nodes $t$ in which the variable of interest $X_j$ is used and to take that average over all trees in the ensemble. Note that $p(t)$ is the proportion of observations of a particular class in the node $t$ and $\Delta i(s,t)$ is the decrease in impurity over the node $t$ when variable $X_j$ is chosen.	More important variables are those which are on average more often chose for splits and which also contribute most to reducing the nodal impurity of the trees. \par
  
  \begin{algorithm}
		\caption{MDI Variable Importance}\label{mdi variable importance}
		\begin{algorithmic}[1]
			\State Grow a random forest $\{T_b\}_{b=1}^B$.
			\For {$j=1,\ldots,p$ }
				\For {$b=1,\ldots,B$ }
				\State Compute the importance of $X_j$ in $T_b$ as $VI_b(X_j)=\sum_{t\in T_b} \mathbb{I}(j_t=j)p(t)\Delta i(s,t)$ to be the sum of the decrease in impurity over nodes where variable $X_j$ is used .
				\EndFor
				\State Compute the importance of $X_j$ in the random forest to be $VI(X_j)=\frac{1}{B}\sum_{b=1}^B VI_b(X_j).$
			\EndFor
		\end{algorithmic}
	\end{algorithm}

  The idea of MDA is to measure for each variable $X_j$, on average how much the predictive accuracy of the forest as measured using RSS suffers when the $X_j$ component is permuted across observations within the OOB dataset $\bar{Z}_b$ of each tree $T_b$ in the ensemble.  \par
  
  \begin{algorithm}
		\caption{MDA Variable Importance}\label{mda variable importance}
		\begin{algorithmic}[1]
			\State Grow a random forest $\{T_b\}_{b=1}^B$.
			\For {$j=1,\ldots,p$ }
			\For {$b=1,\ldots,B$ }
			\State Permute the $X_j$ component of $Z_b$ to obtain the dataset $Z_b^j$, where $X_j$ has been permuted.
			\State Compute the importance of $X_j$ in $T_b$ to be $VI_b(X_j)=\frac{1}{|\bar{Z}_b|}(RSS(T_b,Z_b)-RSS(T_b,Z_b^j)).$
			\EndFor
			\State Compute the importance of $X_j$ in the random forest to be $VI(X_j)=\frac{1}{B}\sum_{b=1}^B VI_b(X_j).$
			\EndFor
		\end{algorithmic}
	\end{algorithm}

  More important variables in the random forests are those for which the variable importance is large, as those are the variables for which the predictive accuracy of the random forest suffers the most when the out of bag data is permuted. Note that as variable importance measures currently defined and used, the threshold for importance of a variable is something which the researcher has to decide.  \par

### Issues with Random Forests

  While random forests are good out of the box predictors, there are situations where the random forest algorithm can fail to produce good predictions. If the underlying regression function is linear, if the predictors are highly correlated, or if the data cannot be bootstrapped, then the random forest will not perform well. Relative to other methods, random forests are especially adept at handling highly non-linear functions, but can struggle with linear response compared to say linear regression. \par

  If the predictors are highly correlated, then it has been shown by @strobl2007 and @strobl2008 that the trees within the forest ensemble will be biased and the variable importance measures will not be reliable due to confounding between similar looking variables. While the randomization step in the random forest algorithm can alleviate the correlation between trees in the ensemble, the collinearity between predictors can cause the predictive performance of the random forest to suffer. This is an issue with the underlying CART algorithm. Alternative algorithms from CART exist which try to alleviate issues of collinearity and correlation in predictors, but we focus primarily on the forest ensembles grown using CART in this thesis. \par

  Finally, there are situations where the data cannot be bootstrapped. This could be due to a number of factors including that the data has a heavily-tailed distribution or if there are particularly extreme values. In this case, a different bootstrap scheme could perhaps remedy the situation where the naive bootstrap fails. One common important resampling scheme used within random forests is to use the m-out-of-n bootstrap (also referred to subsampling or subagging in the literature). The m-out-of-n bootstrap is a resampling scheme which resamples with or without replacement $m\leq n$ observations from the data to form datasets with $m$ many data points to run the bootstrap computation. Bickel et al. have shown that in important cases, subsampling can succeed where the bootstrap fails. Of course, using less of the available data is less efficient, but in the context of random forests this loss of efficiency may not be an issue. In @strobl2008, simulation results showed that subsampling instead of using the standard nonparametric bootstrap can improve the performance of random forests. The authors suggested that subsampling further reduces the variance of the ensemble by producing trees that are even more decorrelated. Their heuristic is that there is a lower probability of duplicate data points being chosen using subsampling (this probability is zero if we are subsampling without replacement), furthermore there is a lower probability of highly correlated data points being chosen as one of the $m\leq n$ points. This certainly seems plausible, but we would like to see further simulation results or a technical result that explains why subsampling works well for forests. We would also like to note that most consistency results (Biau et al., Scornet et al., Ishwaran et al., and Wager et al.) make this subsampling assumption in their analysis of the random forest model, as subsampling constructions make the forest ensemble more amenable to mathematical analysis. \par
	
## Focus of this Thesis

  In the previous three sections we discussed the basics of trees, the bootstrap, and random forests. We would like to now discuss the direction we are going to take this thesis. We are interested in developing inferential tools using the random forest algorithm. The random forest often produces good predictions out of the box with little tuning required except growing a sufficient number of trees and choosing a good value of $m\leq p$ to try at each split in the tree growing process. The ease of fitting random forests is one of the advantages of the random forest compared to more complex methods like neural networks or support vector machines. Even if the random forest is easier to fit than neural nets or SVMs, the underlying random forest mechanism is quite complex. This complexity makes developing inferential tools with random forests difficult. In the next chapter we will discuss some approaches that have been developed recently by researchers. Of particular interest for us are the variable importance measures for random forests. There is the MDI variable importance and the MDA variable importance measure. While there are concerns about the bias in the MDI and MDA VI measures with random forests using CART tree's due to how CART tree's are constructed (Strobl et al., 2008), we would like to extend the inferential capabilities of random forests using CART as this is perhaps the most popular version of the algorithm in use. \par

  The random forest VI can be viewed as a random variable with an unknown probability distribution. While determining the distribution of random forest VI's would be ideal towards developing inference for random forests, finding the distribution of random forest VI's is difficult and unclear in general. However, note that the bootstrap provides a computational method of approximating the sampling distribution of random forest VI measures. We could then proceed to obtaining estimates of the standard error and to form approximate interval estimates. We could grow a random forest once, obtain an estimate of the VI of each variable $X_j$ and then run a bootstrap to obtain estimates of the standard error of each variable. Inference could then proceed using the VI measures along with the estimate of standard error. Note that in this scheme there is two levels of bootstrap at play. There is the bootstrap within each random forest and then the bootstrapping to obtain the standard error estimate. The estimate we produce of the VI of a variable $X_j$ can be erratic. Running the random forest again can produce a new estimate of the VI of variable $X_j$. As Efron suggests in his 2014 article "Estimation and Accuracy after Model Selection," we could remedy the erratic, non-smooth nature of the VI estimate by bagging $VI(X_j)$. However, when we bag $VI(X_j)$ we would require a third-level of bootstrapping to produce an estimate of the standard error of the bagged estimate of $VI(X_j)$. This is computationally expensive, especially considering that the computational cost of the random forest depends on the number of observations, number of tree's grown, and number of variables present. However, applying Efron's infinitesimal jackknife (IJ) technique, which we will discuss in the next chapter, we can efficiently estimate the standard error of the bagged estimate of $VI(X_j)$ only using the two levels of bootstrapping. In this thesis we will be exploring the properties of IJ estimates of the standard error of the bagged estimate $VI(X_j)$ utilizing different bootstrap schemes. Utilizing the IJ, we can then produce interval estimates and confidence intervals of the variable importance of variables in the random forest. This allows us to characterize the uncertainty of $VI(X_j)$ when using variable importance measures for description, variable selection, and inference.
   \par
## Outline of Remaining Chapters

  Chapter 2: Inference and variable importance for random forests. In this chapter we will discuss the various approaches to inference for random forests that have been developed by researchers. These include Ishwaran and Louppe's analysis of VI measures of RF. Ishwaran's analysis is of the variable importance scheme he devised. Ishwaran's scheme is amenable to theoretical analysis and he produces asymptotic results for his scheme. Louppe analyzes the MDI variable importance in the context of a construction called totally randomized forests in the setting of finite sample spaces. Louppe investigates the effect of relevant and irrelevant variables to MDI VI in this setting and provides asymptotic results. These are two of themain papers investigating properties of VI.

  The other schemes include Mentch and Hooker's prediction intervals which are produced by interpreting the random forest ensemble as a particular type of U-statistic and applying the U-statistics theory to produce prediction intervals. They then can produce hypothesis tests and confidence intervals in this context. 

  A second approach towards prediction intervals is Wager's application of the IJ to the random forest estimator to produce prediction intervals and confidence intervals. Wager has a couple of papers in this area which we will discuss. In this part we will also go into greater detail about the IJ, it's properties, and why it works. 

  We will also discuss the conditional inference trees and conditional variable importance developed by Strobl, Hothorn, and their colleagues to deal with the biased manner in which the trees in random forests are constructed. In this part we will also discuss the Infforest variable importance measure developed by Aurora.

  Chapter 3: In this chapter we will go into great detail about the bootstrap and why it works. We will discuss the technical details of different methods of estimating the standard error of functional statistics, and we will go into the construction of different bootstrap confidence intervals along with properties of these confidence intervals. We will also discuss different bootstrap schemes and in particular subsampling, and why subsampling works well, especially in the random forest context. 

  Chapter 4: We will present our different approaches to bootstrapping variable importance measures and provide simulation results. The first approach is the cases bootstrap, the second approach is the bootstrapping residuals of the random forest approach. Within the cases approach, there are two ways we will try to constrain the variability of bootstrap replicates. Adopting the notation from general linear models, we could fit a pruned tree predicting $Y\sim X_1,\ldots,X_p$. Then we would bootstrap from the terminal nodes of this single tree to try to produce bootstrap replicates which capture the relationship between the variables. Another approach is for each predictor $X_j$, fit a single well pruned tree $X_j\sim X_1,\ldots,X_{j-1},X_{j+1},\ldots,X_p$. We would then draw bootstrap samples from the terminal nodes of this tree which respect the partition produced by these terminal nodes. The idea is to fix the relationship between every predictor not $X_j$ and allow the $X_j$'s to be exchanged across the bootstrap samples. We would produce simulation results of each approach and compare the statistical properties of these different constructions. 

  Chapter 5: If all goes well there might be a chapter 5 where we explore large sample properties and distribution of VI measures. This is likely going to be a difficult mathematical problem to solve in general, so a tangible course of action might be to pick a generating mechanism and easy VI measure from which we could figure out the VI distribution under some set of strong assumptions.




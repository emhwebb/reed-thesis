<!--
This is for including Chapter 1.  Notice that it's also good practice to name your chunk.  This will help you debug potential issues as you knit.  The chunk above is called intro and the one below is called chapter1.  Feel free to change the name of the Rmd file as you wish, but don't forget to change it here from chap1.Rmd.
-->

<!--
The {#rmd-basics} text after the chapter declaration will allow us to link throughout the document back to the beginning of Chapter 1.  These labels will automatically be generated (if not specified) by changing the spaces to hyphens and capital letters to lowercase.  Look for the reference to this label at the beginning of Chapter 2.
-->

# Introduction to Trees, Random Forests, and the Bootstrap {#rmd-basics}

```{r include = FALSE, warning = FALSE, message = FALSE, echo = FALSE}

library(rpart)
library(randomForest)
library(tidyverse)
library(grid)
library(gridExtra)
library(mvtnorm)
library(knitr)
library(kableExtra)
library(highlight)
load(file = "data/siml_examples.Rdata")

plot_AVPI_ex <-function(i, df){
  AVPI_vec <- df[,i]
  var_name <- sapply(1:3, FUN = function(i) paste0("V", as.character(i)))
  x_axis_order <- sapply(1:3, FUN = function(i) paste0("V", as.character(4-i)))
  plot_df <- data.frame(variable = var_name, AVPI = AVPI_vec)
  
  plot_obj <- ggplot(plot_df, aes(x = variable, y = AVPI))+
  geom_bar(stat = 'identity')+
  coord_flip()+
  scale_x_discrete(limits = x_axis_order)+
  labs(y = paste("Simulation", as.character(i), sep = " "), x = "")+
  theme(text = element_text(size = 10))
  #theme(axis.title.y=element_blank())
  plot_obj
}

plot_add_var_excep <- function(i, df.list){
  df <- df.list[[i]]
  x.name <- colnames(df)[1]
  y.name <- colnames(df)[2]
  plot.obj <- ggplot(df, aes_string(x = x.name, y = y.name))+
    geom_point(alpha = 0.3, size = 1)+
    theme(text = element_text(size = 10))
  plot.obj
}




```


## CART

  We begin our discussion of random forests by considering CART (Classification and Regression Trees). The CART methodology allows us to fit a model that can take into account non-linear regression or classification surfaces. The basic idea of CART is that if we can split the predictors into roughly homogeneous partitions in terms of the response $Y$, then we can fit a simple model to predict the response of each partition. CART trees were first introduced by @breiman1984 and are a flexible method, capable of handling classification and regression settings.  \par

  More formally, suppose we have a training data set $Z=\{Z_1,\ldots,Z_n\}$, where $Z_i=(X_i,Y_i)$ is a $p+1$ dimensional vector in $\mathbb{R}^{p+1}$. Here we have $X_i$ is a $p$-dimensional predictor variable and $Y_i$ is the response. In particular, we can consider $Z$ to be an $n\times (p+1)$ array where the rows are observations and the columns are the response and predictor variables. CART works by partitioning the data through binary recursive splits via optimizing some loss function. Trees in CART are called trees because partitioning the data through binary recursive splits forms a tree-like structure. We adopt notation evocative of this tree structure. Any subset of the training data $Z$ is called a node while the entire data set $Z$ is called the root node. Nodes are of two kinds: they are either terminal (sometimes called leaves) or not terminal. Non-terminal nodes are nodes that are split on in the tree growing process, while terminal nodes are nodes which are not split on. Generally, a node is a terminal node if some stopping rule is reached. Note that terminal nodes form a partition of the training data $Z$. \par
  
  
  We aim to grow a tree with roughly homogeneous terminal nodes in terms of $Y$. As @breiman1984 note, there are several factors that we need to consider:
	\begin{enumerate}
		\item How to select splits of the data.
		
		\item When to stop splitting the data.
		
		\item How to assign classes or values to terminal nodes. 
	\end{enumerate}
	
	
  The CART algorithm addresses point 1 by decreasing the *nodal impurity* of a node $t$. Nodal impurity is defined through some nodal impurity measure $i(s,t)$, usually the GINI index in the classification setting and the residual sum of squares in the regression setting. We present the regression setting and then the classification setting. If we have a continuous response $Y$, then we could try to predict $Y$ by partitioning the data using the decision tree structure and predicting that points that fall within a particular partition will take on the average response on that partition. To choose the best binary split on the data, we need to search across the splitting variables and splitting points for the split which maximizes the reduction in the RSS between the parent node and daughter nodes. In particular, we want to find the split which maximizes $RSS_l(j,s)+RSS_r(j,s)$ where $j$ is the proposed splitting variable and $s$ is the proposed splitting point, and $RSS_l$ is the RSS of the proposed left node and $RSS_r$ is the RSS of the proposed right node. The assignment of response within a node is given by the average of the response. We present the following algorithm. \par
	
  \begin{algorithm}
		\caption{Construction of Regression tree}\label{regression tree}
		\begin{algorithmic}[1]
			\While {minimum node size not reached}
				\For {each node $t$ }
					\For {$j=1,\ldots,p$ and $s=1,\ldots,n$ }
					\State Compute $RSS_l(j,s)+RSS_r(j,s)$.
					\EndFor
					\State Pick the $(j,s)$ which maximizes $RSS_l(j,s)+RSS_r(j,s)$.
					\State Split the current node into left and right nodes according to $(j,s)$.
					\State Compute left and right averages, $\text{ave}(y_i|x_i\in t_l)$ and $\text{ave}(y_i|x_i\in t_r)$, respectively. 
				\EndFor
				\State Output the tree $T$.
			\EndWhile
		\end{algorithmic}
	\end{algorithm}
	
  
### Example of Regression Tree

We now provide an example of a regression tree using a simulated data set we will use throughout this thesis to illustrate important examples. We made 2000 draws of 3 independent standard normals $X_1,X_2,X_3\sim N(0,1)$ and defined the response by the linear equation $$Y=10X_1-5X_2+\varepsilon,$$ where $\varepsilon\sim N(0,1)$. Our data set is thus a $2000\times 4$ array where each row is of the form $(X_1,X_2,X_3,Y)$. Hence we have two informative predictors ($X_1$ and $X_2$) and one uninformative predictor ($X_3$) in our simulated data set. For this data set, we ran CART to produce figure \ref{tree_ex}. \par  

```{r results="asis", echo = FALSE, warning = FALSE, message = FALSE, fig.env = 'marginfigure', fig.cap= "\\label{tree_ex}Example of a regression tree"}

siml_data_ex <- siml.ex.list[[1]]
tree_ex <- rpart(Y~., data = siml_data_ex)
plot(tree_ex)
text(tree_ex, cex = 0.8, use.n = TRUE, xpd = TRUE)

```

We would like to note that the tree structure produced by CART is quite easy to interpret. Given some point $x=(x_1,x_2,x_3)$, to determine what the prediction for $x$ should be, we simply follow the left-right paths down to a terminal node of the tree. We also note that this regression tree generally reflects the structure of our data set in the sense that important splits near the base of the tree tend to be made on $X_1$, while splits further down in the tree tend to be on $X_2$. Furthermore, no splits are made on $X_3$. In this scenario, we can visually inspect the tree to determine which predictors are informative towards predicting the response $Y$. 
  	
### Classification Trees and Issues with CART

  The construction of classification trees is similar to the construction of regression trees except a different impurity measure must be used. For the node $t$ containing $N_t$ observations, let $$\hat{p}_{tk}=\frac{1}{N_t}\sum_{x_i\in t} \mathbb{I}(y_i=k)$$ where $k=0$ or $k=1$. Then $\hat{p}_{tk}$ measures the proportion of observations of class $k$ in node $t$. Using the two-class example, there are several impurity measures available in the classification setting. The most common measure is perhaps the Gini index  defined by $$2p(1-p).$$ Other options include misclassification error $$1-\max(p,1-p),$$ and cross-entropy $$-p\log(p)-(1-p)\log(1-p).$$ If $t_L$ and $t_R$ are left and right nodes proposed under the split, let $\hat{p}_{tL}$ and $\hat{p}_{tR}$ be the proportion of observations falling into $t_L$ and $t_R$, respectively. Denote the Gini index of the left node $t_L$ by $G_{L}$ and denote the Gini index of the right node $t_R$ by $G_{R}$. Then our splitting criterion is to seek the splitting variable and splitting point which minimizes $$\hat{p}_{tL}G_L+\hat{p}_{tR}G_R.$$ The case is similar when using misclassification rate and cross-entropy as the impurity measures. \par
  
  Some issues with CART trees include overfitting and variability. While there is a stopping criterion for growing the CART trees, often times naively growing a tree can result in overfitting to the data. One method of alleviating overfitting is to employ cost-complexity pruning, which searches for an optimal tree that balances fitting a good predictive tree model with overfitting to the data. We do not go into details here, but point the reader to @esl2009. \par
  
  A larger issue with trees is their sensitivity to small perturbations in the data. Allowing the data to vary even a bit can result in a different tree structure upon refitting. As a statistical learning method, this means that CART trees are not robust and suffer from high variance, even when constructed using cost-complexity pruning. To get around this issue of sensitivity the best solution is to employ bagging and use the random forest algorithm. \par
  
  
## Random Forests

  Among the limitations of CART discussed in the preceding sections, the biggest issues are perhaps the variability of the method and the issue of collinearity among predictors. While overfitting can be addressed using cost-complexity pruning, variability and collinearity are not fixed by pruning. Random forests deal with these two issues of CART by introducing a resampling and randomization mechanism. The natural order is to first discuss bagged forests before turning to random forests. \par
  
  One method of improving CART trees is to bag them. Bagging, which stands for bootstrap aggregating, is a variances reduction technique particularly useful for improving the predictive power of weak learners. We are interested in bagging CART trees to reduce the variability of single trees under slight perturbations of the data. Generally bagged ensembles of learners produce robust predictions in comparison to running the learner once. \par

  
  \begin{algorithm}
		\caption{Bagged Forest algorithm}\label{bagged forest}
		\begin{algorithmic}[1]
			\For {$b=1,\ldots, B$ }
			\State Draw a bootstrap sample $Z_b^*$ of size $n$ from the training data $Z$.
			\State Grow a CART tree $T_b$ on each bootstrap sample $Z_b^*$.
			\EndFor
			\State Output the bagged forest ensemble $\{T_b\}_{b=1}^B$.
		\end{algorithmic}
	\end{algorithm}
  
### How Bagged Forests Work 

  Bagged forests are an ensemble obtained by taking many bootstrap samples of the data and fitting a tree to each bootstrapped data set.  In this case, we grow the trees quite deep and do not employ cost-complexity pruning. The idea behind this decision is that we want to sufficiently explore the feature space using a single bagged tree of the forest ensemble, and since we are also taking an average of the trees, we are fine with overfitting at least a little bit. As the algorithm above for bagged forests indicates, the output is an ensemble of trees $\{T_b\}_{b=1}^B$. Given a test data point, we form a prediction by taking the average of predictions given by the tree ensemble:
	$$\hat{\theta}_{BF}^B(x)=\frac{1}{B}\sum_{b=1}^B T_b(x),$$ where $\hat{f}_{bf}^B$ indicates we are taking the bagged forest estimate of the underlying regression or classification function using $B$ many trees. Note that it is important to be consistent in the choice of splitting criterion throughout the bagging process. \par

  While bagging is a general method that can be applied, for example, to methods such as ordinary least squares or generalized linear models, it has been shown by @hall1999 and @chen2003 that bagging is especially effective when used on highly non-linear models such as CART trees. Under ideal conditions, bagged estimates of non-linear estimators reduces both the bias and variance of the estimator. So bagging trees seem to be effective because of the reduction to the variance of the estimator, which produces a more robust prediction. \par

  When we bag trees, each observation in the data is not used within each individual tree. A bootstrap replicate $Z_b$ of the data $Z$ will likely exclude some of the observations within the data. The observations used within the bootstrap replicate $Z_b$ is called the in-bag data  while the observations not used within the bootstrap replicate $Z_b$ is called the out-of-bag (OOB) data and is denoted by $\bar{Z}_b$. The OOB data allows us to approximate the test error of the ensemble as follows. For simplicity suppose we are in the regression setting (the classification setting is similar). Then the OOB estimate of the mean squared error (MSE) of the bagged forest is given by $$MSE_{\textup{OOB}}(T;Z)=\frac{1}{B}\sum_{b=1}^B MSE(T_b;\bar{Z}_b).$$ As the number of trees in the ensemble grows, the OOB estimate of the MSE for the bagged forest converges to the LOOCV estimate of the MSE (@esl2009). \par

  A weakness of bagged forests is collinearity between trees (@esl2009). This collinearity between trees grown from the bootstrap sample can be addressed by adding a randomization mechanism in the tree growing process. When we grow a tree from a bootstrap sample, even with the randomness induced by resampling from the data, certain features may be explored at the expense of other just as interesting features. This is a particular issue with collinear predictors. If two predictors are collinear, then the bagged forest might consistently choose one predictor over another even if the predictor not chosen leads to splits that are just as informative. This is due in part to the greedy nature of the CART algorithm when searching for optimal splits over the feature space. The algorithm does not take into account the second best or third best splits. Furthermore, it is not difficult to see that bagging will generally produce an ensemble of trees that are quite similar to one another, subject to some perturbations. These trees will be strongly correlated with other trees in the ensemble, so if there is a less explored part of the feature space, then the ensemble will struggle to produce good predictions over that region. \par
  
### The Random Forest Algorithm

  Random Forests deal with this issue of correlated trees and collinearity among predictors by choosing at random only $m\leq p$ of the predictors to be considered as candidate splitting variables at each split in each tree in the ensemble. This randomness further reduces the variance of the bagged forest by decorrelating the trees in the ensemble. Furthermore, while individually the trees may perform worse than a single pruned tree, collectively the ensemble has a better chance of exploring the feature space fully. \par
	\begin{algorithm}
		\caption{Random Forest algorithm}\label{random forest}
		\begin{algorithmic}[1]
			\For {$b=1,\ldots, B$ }
			\State Draw a bootstrap sample $Z_b^*$ of size $N$ from the training data $Z$.
			\State Grow the CART tree $T_b$ on $Z_b^*$ with the following modification:
			\While {minimum node size $n_{\min}$ not reached across $T_b$} 
			\State Select $m\leq p$ candidate splitting variables at random.
			\State Pick the best splitting variable and splitting point among the $m$ variables selected at random.
			\State Split the node into two daughter nodes.
			\EndWhile
			\EndFor
			\State Output the random forest ensemble $\{T_b\}_{b=1}^B$.
		\end{algorithmic}
	\end{algorithm}

  To form a prediction at a test point $x$, we have in the regression setting $$\hat{\theta}_{RF}^B(x)=\frac{1}{B}\sum_{b=1}^B T_b(x).$$ \par
  
### Variable Importance Measures
	
  One of the outputs of random forests is the variable importance (VI) measure. There are two main variable importance measures in common use, with the choice of VI measure varying depending on the setting and splitting criterion chosen. The first choice is the Mean Decrease in Impurity (MDI) which is typically used in the classification setting where the GINI index or Shannon entropy is used as the splitting criterion. The second choice is Mean Decrease in Accuracy (MDA) which is typically used in the regression setting where RSS has been used as the splitting criterion.  The idea of MDI is to find how much the nodal impurity $p(t)\Delta i(s,t)$ decreases for all nodes $t$ in which the variable of interest $X_j$ is used and to take that average over all trees in the ensemble. Note that $p(t)$ is the proportion of observations of a particular class in the node $t$ and $\Delta i(s,t)$ is the decrease in impurity over the node $t$ when variable $X_j$ is chosen.	More important variables are those which are on average more often chosens for splits and which also contribute most to reducing the nodal impurity of the trees. \par 
  
  \begin{algorithm}
		\caption{MDI Variable Importance}\label{mdi variable importance}
		\begin{algorithmic}[1]
			\State Grow a random forest $\{T_b\}_{b=1}^B$.
			\For {$j=1,\ldots,p$ }
				\For {$b=1,\ldots,B$ }
				\State Compute the importance of $X_j$ in $T_b$ as $VI_b(X_j)=\sum_{t\in T_b} \mathbb{I}(j_t=j)p(t)\Delta i(s,t)$ to be the sum of the decrease in impurity over nodes where variable $X_j$ is used .
				\EndFor
				\State Compute the importance of $X_j$ in the random forest to be $VI(X_j)=\frac{1}{B}\sum_{b=1}^B VI_b(X_j).$
			\EndFor
		\end{algorithmic}
	\end{algorithm}

  The idea of MDA is to measure, for each variable $X_j$, how much the predictive accuracy of the forest as measured using RSS suffers when the $X_j$ component is permuted across observations within the OOB dataset $\bar{Z}_b$ of each tree $T_b$ in the ensemble.  \par 
  
  \begin{algorithm}
		\caption{MDA Variable Importance}\label{mda variable importance}
		\begin{algorithmic}[1]
			\State Grow a random forest $\{T_b\}_{b=1}^B$.
			\For {$j=1,\ldots,p$ }
			\For {$b=1,\ldots,B$ }
			\State Permute the $X_j$ component of $Z_b$ to obtain the dataset $Z_b^j$, where $X_j$ has been permuted.
			\State Compute the importance of $X_j$ in $T_b$ to be $VI_b(X_j)=\frac{1}{|\bar{Z}_b|}(RSS(T_b,Z_b)-RSS(T_b,Z_b^j)).$
			\EndFor
			\State Compute the importance of $X_j$ in the random forest to be $VI(X_j)=\frac{1}{B}\sum_{b=1}^B VI_b(X_j).$
			\EndFor
		\end{algorithmic}
	\end{algorithm}

  More important variables in the random forests are those for which the variable importance is large, as those are the variables for which the predictive accuracy of the random forest suffers the most when the out of bag data is permuted. Note that as variable importance measures currently are defined and used, the threshold for importance of a variable is something which the researcher has to decide. \par
  
### Example of MDA and MDI variable importance

We ran a random forest using 1000 trees on our example dataset and plotted bar charts of the MDA and MDI variable importance scores, respectively, in figure \ref{mda_ex}. \par

```{r results="asis", echo = FALSE, warning = FALSE, message = FALSE, fig.env = 'marginfigure', fig.cap= "\\label{mda_ex}Example of MDA and MDI variable importance", fig.height = 3}

rf_ex <- siml.ex.list[[3]]

rf_ex_imp <- data.frame(Variable = c("V1", "V2", 'V3'), rf_ex$importance)

mda_ex_plot <- ggplot(rf_ex_imp, aes(x = Variable, y = X.IncMSE))+
  geom_bar(stat = 'identity')+
  coord_flip()+
  labs( y = "PercIncMSE")+
  scale_x_discrete(limits = c("V3", 'V2', 'V1'))
mdi_ex_plot <- ggplot(rf_ex_imp, aes(x = Variable, y = IncNodePurity))+
  geom_bar(stat = 'identity')+
  coord_flip()+
  labs(y = "IncNodePurity")+
  scale_x_discrete(limits = c("V3", 'V2', 'V1'))

do.call("grid.arrange", c(list(mda_ex_plot, mdi_ex_plot), nrow = 1, ncol = 2))


```

Note that for both variable importance measures, the contribution of each predictor is properly captured. That is, $X_1$ is the most important variable followed by $X_2$, with $X_3$ having no contribution with the MDA importance measure. With MDI, the contribution of $X_3$ is non-zero likely due to the randomization introduced in the splitting variable step. 

### Issues with Random Forests

  While random forests are good out of the box predictors, there are situations where the random forest algorithm can fail to produce good predictions. If the underlying regression function is linear, if the predictors are highly correlated, or if the data cannot be bootstrapped, then the random forest will not perform well. Relative to other methods, random forests are especially adept at handling highly non-linear functions, but can struggle with linear response compared to say linear regression. \par

  If the predictors are highly correlated, then it has been shown by @strobl2007 and @strobl2008 that the trees within the forest ensemble will be biased and the variable importance measures will not be reliable due to confounding between similar looking variables. While the randomization step in the random forest algorithm can alleviate the correlation between trees in the ensemble, the collinearity between predictors can cause the predictive performance of the random forest to suffer. This is an issue with the underlying CART algorithm. Alternative algorithms from CART exist which try to alleviate issues of collinearity and correlation in predictors, but we focus primarily on the forest ensembles grown using CART in this thesis. \par

  Finally, there are situations where the data cannot be bootstrapped. This could be due to a number of factors including that the data has a heavily-tailed distribution or if there are particularly extreme values. In this case, a different bootstrap scheme could perhaps remedy the situation where the naive bootstrap fails. One common resampling scheme used within random forests is to use the m-out-of-n bootstrap (also referred to AS subsampling or subagging in the literature). The m-out-of-n bootstrap is a resampling scheme which resamples with or without replacement $m\leq n$ observations from the data to form datasets with $m$ many data points to run the bootstrap computation. Of course, using less of the available data is less efficient, but in the context of random forests this loss of efficiency may not be an issue. In @strobl2008, simulation results showed that subsampling instead of using the standard nonparametric bootstrap can improve the performance of random forests. The authors suggested that subsampling further reduces the variance of the ensemble by producing trees that are even more decorrelated. Their heuristic is that there is a lower probability of duplicate data points being chosen using subsampling (this probability is zero if we are subsampling without replacement), furthermore there is a lower probability of highly correlated data points being chosen as one of the $m\leq n$ points. This certainly seems plausible, but we would like to see further simulation results or a technical result that explains why subsampling works well for forests. We would also like to note that consistency results such as from @mentch2016a and @wager2017 make this subsampling assumption in their analysis of the random forest model, as subsampling constructions make the forest ensemble more amenable to mathematical analysis. \par

\newpage
	
## Focus of this Thesis

  An issue with random forest variable importance measures is bias due to the presence of correlated predictors. As will become clearer in the next chapter, correlated predictors can lead to unreliable MDA and MDI variable importance scores due to confounding between correlated predictors. The weakness of commonly used random forest variable importance measures to correlated predictors is unfortunate as random forests often produce good predictions with little tuning required except growing a sufficient number of trees and choosing a good value of $m\leq p$ to try at each split in the tree growing process. The ease of fitting random forests is one of the advantages of the random forest algorithm compared to more complex methods like neural networks or support vector machines. Since the random forest algorithm is a readily available and easy to use tool for statistical practitioners, we would like to develop random forest variable importance measures which can take into account correlated predictors. Our goal with this thesis is to:
  \begin{enumerate}
  \item Develop computationally tractable random forest variable importance measures which can handle correlated predictors in the data. 
  \item Extend our random forest variable importance measures to a hypothesis testing framework via permutation tests. 
  \end{enumerate}
  \par




<!--
This is for including Chapter 1.  Notice that it's also good practice to name your chunk.  This will help you debug potential issues as you knit.  The chunk above is called intro and the one below is called chapter1.  Feel free to change the name of the Rmd file as you wish, but don't forget to change it here from chap1.Rmd.
-->

<!--
The {#rmd-basics} text after the chapter declaration will allow us to link throughout the document back to the beginning of Chapter 1.  These labels will automatically be generated (if not specified) by changing the spaces to hyphens and capital letters to lowercase.  Look for the reference to this label at the beginning of Chapter 2.
-->

# Introduction to Trees, Random Forests, and the Bootstrap {#rmd-basics}

  This thesis is about understanding the variance in variable importance measures of random forests. Random Forests are a statistical learning algorithm developed by Leo Breiman and his collaborators in the early 2000's that leverages bagging and CART (Classification and Regression Trees) methodology to produce quite good predictions of an underlying classification or regression surface. The predictive capability of random forests is quite impressive and much random forest literature centers on exploring why random forests are effective. For this thesis we are interested in the use of random forests for statistical inference. More specifically we are interested in the use of random forest variable importance measures for statistical inference. 

  Machine learning approaches to regression can often produce models with good predictive accuracy compared to parametric modelling approaches. However, these machine learning algorithms often lack in the interpretability and inferential capabilities of more traditional statistical modelling. While predictive questions are indeed important in science and other applications, many scientific and statistical questions are concerned with description and inference. These questions could be about a causal mechanism, which hypothesis is correct concerning some phenomenon, and which are the important factors. The mechanisms that machine learning techniques use to form predictions on the data are often complicated and not easily amenable to mathematical analysis, but we believe one of Breiman's points in his 2001 paper "Statistical Modelling: Two Cultures" is that statisticians ought to work to develop inferential tools for machine learning techniques. Predictive accuracy is a good metric to measure how a model fits to the data, and without a method that can produce predictively accurate estimates of the underlying data mechanism, inferential conclusions seem less justifiable. Throughout this thesis we will be interested in both the regression and classification settings as random forests can handle response variables of both types. When the need arises to delineate between the setting we are working in, we will be explicit. Furthermore, we will be assuming that for the data at hand, there exists an underlying regression function $Y=f(X)+\varepsilon$ where $f(X)$ is the function and $\varepsilon$ is generally assumed to be Gaussian error, but may have some other error structure. 


## CART

  We begin our discussion of CART (Classification and Regression Trees) trees by considering the following problem. Suppose we have data generated from the piecewise function $Y=f(X)+\varepsilon$ where [at this point insert a very non-linear piecewise function.] with Gaussian error $\varepsilon$. One approach to fitting a model to this data would be to fit a linear regression model. However, the particular form of the data is not well suited for linear regression due to the non-linearity of the data. Another approach we might try to take is as follows. We might try to find a method that can fit a regression line to each piecewise componenet of the data. One such method is to fit CART trees. We fit a CART tree to the data and perform cost-complexity pruning to output the model shown in figure [figure not yet made]. In this example, we produced a regression surface which was able to take into account the piecewise, non-linear nature of the data and produce a reasonable estimate of the underlying data generating mechanism. 
	
  The CART methodology allows us to fit a model that can take into account non-linear regression or classification surfaces. The basic idea of CART is that if we can split the predictors into roughly homogenous partitions, or into partitions which minimize predictive error, then we can fit a simple model to predict the response of each partition. CART trees were first introduced by Breiman et al. in 1984 and are a flexible method, capable of handling classification and regression settings. CART is a product of computational statistics, it's development influenced by a data-centric algorithmic approach distinct from the models of classical statistics influenced by the contingency of small samples.

  More formally, suppose we have a training data set $Z=\{Z_1,\ldots,Z_n\}$, where $Z_i=(X_i,Y_i)$ is an $p+1$ dimensional vector in $\mathbb{R}^{p+1}$. Here we have $X_i$ is a $d$-dimensional predictor variable and $Y_i$ is the response. In particular, we can consider $Z$ to be a $n\times (p+1)$ array where the rows are observations and the columns are the response and predictor variables. CART works by partitioning the data through binary recursive splits via optimizing some loss function. CART trees are called trees because partitioning the data through binary recursive splits forms a tree-like structure. We adopt notation evocative of this tree structure. Any subset of the training data $Z$ is called a node while the entire data set $Z$ is called the root node. Nodes are of two kinds: they are either terminal (sometimes called leaves) or not terminal. Non-terminal nodes are nodes that are split on in the tree growing process, while terminal nodes are nodes which are not split on. Generally, a node is a terminal node if some stopping rule is reached. Note that terminal nodes form a partition of the training data $Z$. 
  
  
  We aim to grow a tree with roughly homogenous terminal nodes. As Breiman et al. (1984) note, there are several factors that we need to consider:
	\begin{enumerate}
		\item How to select splits of the data.
		
		\item When to stop splitting the data.
		
		\item How to assign classes or values to terminal nodes. 
	\end{enumerate}
	
	
  CART constructs trees by decreasing the nodal impurity of a node $t$. Nodal impurity is defined through some nodal impurity measure $i(s,t)$, usually the GINI index in the classification setting and the residual sum of squares in the regression setting. We present the regression setting and then the classification setting. If we have a continuous response $Y$, then we could try to predict $Y$ by partitioning the data using the decision tree structure and predicting that points that fall within a particular partition will take on the average response on that partition. To choose the best binary split on the data, we need to search across the splitting variables and splitting points for the split which maximizes the reduction in the RSS between the parent node and daughter nodes. In particular, we want to find the split which maximizes $RSS_l(j,s)+RSS_r(j,s)$ where $j$ is the proposed splitting variable and $s$ is the proposed splitting point, and $RSS_l$ is the RSS of the proposed left node and $RSS_r$ is the RSS of the proposed right node. The assignment of response within a node is given by the average of the response. We present the following algorithm. 
	
  \begin{algorithm}
		\caption{Construction of Regression tree}\label{regression tree}
		\begin{algorithmic}[1]
			\While {minimum node size not reached}
				\For {each node $t$ }
					\For {$j=1,\ldots,p$ and $s=1,\ldots,n$ }
					\State Compute $RSS_l(j,s)+RSS_r(j,s)$.
					\EndFor
					\State Pick the $(j,s)$ which maximizes $RSS_l(j,s)+RSS_r(j,s)$.
					\State Split the current node into left and right nodes according to $(j,s)$.
					\State Compute left and right averages, $\text{ave}(y_i|x_i\in t_l)$ and $\text{ave}(y_i|x_i\in t_r)$, respectively. 
				\EndFor
				\State Output the tree $T$.
			\EndWhile
		\end{algorithmic}
	\end{algorithm}
	
  The construction of classification trees is similar to the construction of regression trees except a different impurity measure must be used. For the node $t$ containing $N_t$ observations, let $$\hat{p}_{tk}=\frac{1}{N_t}\sum_{x_i\in t} \mathbb{I}(y_i=k)$$ where $k=0$ or $k=1$. Then $\hat{p}_{tk}$ measures the proportion of observations of class $k$ in node $t$. Using the two-class example, there are several impurity measures available in the classification setting. The most common measure is perhaps the Gini index  defined by $$2p(1-p).$$ Other options include misclassification error $$1-\max(p,1-p),$$ and cross-entropy $$-p\log(p)-(1-p)\log(1-p).$$ If $t_L$ and $t_R$ are left and right nodes proposed under the split, let $\hat{p}_{tL}$ and $\hat{p}_{tR}$ be proportion of observations falling into $t_L$ and $t_R$, respectively. Denote the Gini index of the left node $t_L$ by $G_{L}$ and denote the Gini index of the right node $t_R$ by $G_{R}$. Then our splitting criterion is to seek the spliting variable and splitting point which minimizes $$\hat{p}_{tL}G_L+\hat{p}_{tR}G_R.$$ The case for misclassification rate and cross-entropy as the impurity measures is similar. 
  
  Some issues with CART trees include overfitting and variability. While there is a stopping criterion for growing the CART trees, often times naively growing a tree can result in overfitting to the data. One method of alleviating overfitting is to employ cost-complexity pruning, which searches for an optimal tree that balances fitting a good tree model with overfitting to the data. We do not go into details here, but the point the reader to Friedman et al. (2009). A larger issue with trees is their variability to small perturbations in the data. [Provide computational example.] Allowing the data to vary even a bit can result in a different tree structure upon refitting. As a statistical learning method, this means that CART trees are not robust and suffer from high variance, even when constructed using cost-complexity pruning. To get around this issue of variability the best solution is to employ bagging and use the random forest algorithm. 
  
## Bootstrap

  We feel it is worth introducing some notation and theory for the bootstrap. In this line, for the most part we follow notation from Efron and Tibshirani (1993). As is well-known, the bootstrap is a resampling method first introduced by Efron in 1979 with links to earlier work of Tukey and Quenouille on the jackknife. It goes something like this. If we have a dataset $D$ of size $n$, then it is generally infeasible to obtain more data from the underlying data generating mechanism. Assuming that the data set is not too heavily distributed in the tails and that the sample we have is representative of the population, we can independently resample $n$ points from $D$ to form the bootstrap resampled dataset $D^*$. We repeat this procedure $B$ many times with $B$ generally quite large. We can then proceed to use the many $D^*$ we generated to obtain estimates, standard errors, and confidence intervals of whichever parameter is of interest. [Note: the bootstrap discussed here is called the non-parametric bootstrap. There is also the parametric bootstrap, but we do not go into that method here]. 
  
  What is going on? If we obtain an i.i.d. random sample of size $n$ from a probability distribution $F$, then we can form the empirical distribution function $\hat{F}$ which places a probability of equal mass $\frac{1}{n}$ of seeing any value of $x_i$ for $i=1,\ldots,n$. We do not want to go too deeply into bootstrap theory in this introduction, but we would like to mention the plug-in principle underlying applications of the bootstrap, as well as variance estimation.

  In statistics, we are interested in obtaining information about parameters defined on a probability distribution $F$ by dealing with statistics defined on the EDF $\hat{F}$. Often a parameter is a function of the probability distribution: $$\theta=t(F)$$ where in this context $t$ denotes the functional form of the parameter. Often the probability distribution $F$ is unknown or too complex to deal with directly, so we try to form estimates. One estimate we could form of the parameter $\theta=t(F)$ is the plug-in estimate (or plug-in statistic, or functional statistic), which is defined to be $$\hat{\theta}=t(\hat{F}).$$ It can be shown under suitable conditions, that the functional statistic $t(\hat{F})$ is the nonparametric maximum likelihood estimate of $t(F)$ (Efron and Tibshirani). Therefore there is good theoretical ground for proceeding with estimating parameters using the bootstrap. 

  The usefulness of the bootstrap extends beyond estimating functional statistics. Using the bootstrap, we can estimate the variance of the functional statistic using several methods. The most ubiquitous is perhaps the bootstrap estimate of the standard error. Say we have drawn $B$ many bootstrap samples of a dataset $Z$ to obtain $Z_b^*$ for $b=1,\ldots,B$. We compute an estimate $\hat{\theta}=s(Z)$ of $\theta=t(F)$, where Efron and Tibshirani emphasize that $s(Z)$ is not necessarily the plug-in estimate $t(\hat{F})$. To emphasize that the estimates computed from $Z_b^*$ are bootstrap replicates of $\hat{\theta}$, we write $$\hat{\theta}_b^*=s(Z_b^*).$$ In some settings we write $\hat{\theta}^*=s(Z^*)$ when we do not care as much about the actual subscripting from the bootstrap and just want to emphasize the fact we are estimating $\hat{\theta}$ using bootstrap replicate.

  Denote the standard error of the statistic $\hat{\theta}$ by $se_F(\hat{\theta})$. Following Efron and Tibshirani, the bootstrap estimate of the $se_F(\hat{\theta})$ is a plug-in estimate of the standard error of $\hat{\theta}$ replacing the distribution $F$ in the subscript by the corresponding empirical distribution $\hat{F}$ and is given by $se_{\hat{F}}(\hat{\theta}^*)$. Efron and Tibshirani call $se_{\hat{F}}(\hat{\theta})$ the ideal bootstrap estimate of the standard error of $\hat{\theta}$. Beyond simple examples like the mean, the exact form of $se_{\hat{F}}$ is often too difficult to compute exactly, but the bootstrap can be used to compute a good estimate of $se_{\hat{F}}$. Namely, once we have drawn $Z_1^*, \ldots, Z_B^*$ independent bootstrap samples from $Z$ and compute the $B$ many bootstrap replicates $\hat{\theta}_b^*=s(Z_b^*)$, then the estimate of the standard error $se_{F}(\hat{\theta})$ is given by $$\widehat{se}_B=\left(\frac{1}{B-1}\sum_{b=1}^B (\hat{\theta}_b^*-\bar{\theta}^*)\right)^{1/2},$$ where $\bar{\theta}^*=\frac{1}{B}\sum_{b=1}^B\hat{\theta}_b^*$ is the sample mean of the bootstrap replicates. Note $\widehat{se}_B$ is the familiar formula for the standard deviation of a sample. 
  
  There are several other estimates of the standard error commonly used when exact forms of the standard error are unavailable or infeasible. The first of these is the jackknife. As a computational tool, the jackknife takes a different approach from the bootstrap. Say for the parameter $\theta$, we have an estimate $\hat{\theta}=s(Z)$. Define the $i$-th ``leave one out'' sample $Z_{(i)}=(Z_1,\ldots,Z_{i-1},Z_{i+1},\ldots,Z_{n})$. Corresponding to $Z_{(i)}$ we can form $\hat{\theta}_{(i)}=s(Z_{(i)})$, the $i$th jackknife replicate of the estimator $\hat{\theta}$. Define $$\hat{\theta}_{(\cdot)}=\frac{1}{n}\sum_{i=1}^n \hat{\theta}_{(i)}$$ which is akin to the mean of the jackknife replications. From here we can obtain the jackknife estimate of the standard error, which is defined by $$\widehat{se}_\text{jack} =\left(\frac{n-1}{n}\sum_{i=1}^n (\hat{\theta}_{(i)}-\hat{\theta}_{(\cdot)})^2\right)^{1/2}.$$ The idea of the jackknife is that absent new realizations of the data, a simple procedure we can undertake is to see the effect on the accuracy of the estimator when we remove each observation sequentially. In particular, we could consider the jackknife as forming $n$ many datasets of size $n-1$ with the $i$th observation removed. The jackknife may be easier to compute than the bootstrap, but as the jackknife only uses limited information about the $n$ observations and does not simulate additional full datasets like the bootstrap, overall it is less efficient than the bootstrap. The jackknife is related to the bootstrap in the sense that the jackknife is a linear approximation to the bootstrap. As Efron and Tibshirani explain, for linear statistics the jackknife is as efficient as the bootstrap. However, for highly non-linear statistics (of which it turns out random forests are), it turns out that jackknife estimation is very inefficient compared to the bootstrap. There are other methods of estimating the standard error such as the Infinitesimal Jackknife and Non-parametric delta method, but we will address those methods in later chapters.    
	
  The bootstrap is a flexible method and applications of the bootstrap extend beyond simply sampling pairs of observations from the the data $Z=\{Z_1=(X_1,Y_1), Z_2=(X_2,Y_2),\ldots, Z_n=(X_n,Y_n)\}$. In particular, there are model-based approaches to the bootstrap that are often useful. A particularly important example for this thesis is bootstrapping the least squares linear regression model. In least squares, we model the response $Y$ observed at the vector $X$ as being given by $\mathbb{E}(Y|X)=X^T\beta+\varepsilon$, where $X=(1,X_1,\ldots,X_p)^T$ and $\beta=(\beta_0,\beta_1,\ldots,\beta_p)$ and $\varepsilon$ is an error term of length $n$. In practice, the vector of coefficients $\beta$ is unknown, so we form an estimate $\hat{\beta}$ through applying least squares regression. After applying least squares. For a point $X_j$, the least squares estimate of $Y_j$ is given by $\hat{Y_j}=X_j\hat{\beta}$. The raw residuals are defined by $e_j=Y_j-\hat{Y_j}$ with the estimate of the variance $\sigma^2$ is given by $$s^2=\frac{1}{n-2}\sum_{j=1}^n e_j^2$$.
  
  There are a couple approaches we could consider in applying the bootstrap to linear regression. The conceptually simpler method is to resample cases $(X_i,Y_i)$ to generate bootstrap replications of $Z$, $Z^*=\{(X_1^*,Y_1^*),\ldots,(X_n^*,Y_n^*)\}$ where the asterisks denote that we have resampled from the original data. With $B$ many such datasets $Z^*$, we can use the following algorithm adapted from Davison and Hinkley (1997). 
	
  \begin{algorithm}
		\caption{Linear Regression Cases Resampling}\label{cases resampling}
		\begin{algorithmic}[1]
			\For {$b=1,\ldots,B$ }
			\State Draw a bootstrap sample $Z_b^*$ of size $n$ from the cases $(X_i,Y_i)$ to obtain $Z_b^*=\{(X_1^*,Y_1^*),\ldots,(X_n^*,Y_n^*)\}$.
			\State Fit least squares regression to $(X_1^*,Y_1^*),\ldots,(X_n^*,Y_n^*)$ to obtain the estimates $\hat{\beta}_b^*$ and $\hat{s}_b^{*2}$.
			\EndFor
		\end{algorithmic}
	\end{algorithm}

  Another approach we could take is to resample the residuals of the linear model. To motivate this approach, note that the vector of raw residuals $(e_1,\ldots,e_n)$ approximate the error term $\varepsilon$. That is, assuming the bivariate distribution of $(X,Y)$ is such that the specification of the linear model is correct, then given the true value of $\beta$, the error terms would be given by $\varepsilon_i=Y_i-X_i\beta$. As $\beta$ is generally unknown, the best we can do is to form estimates of the error terms $\hat{\varepsilon}_i=Y_i-\hat{Y_i}=Y_i-X_i\hat{\beta}$. We now have a vector of estimated error terms $\hat{\varepsilon}=(\hat{\varepsilon}_1,\ldots,\hat{\varepsilon}_n)$ which we could consider as being an i.i.d. sample if the cases $(X_i,Y_i)$ were independently sampled from the distribution of $(X,Y)$. As Efron and Tibshirani note, we could consider obtaining any particular component of the vector $\hat{\varepsilon}$ as being from the EDF $\hat{F}$ of the true distribution of the error terms $F$, where we place a probability mass of $\frac{1}{n}$ to drawing any one of the $\hat{\varepsilon}_i$. Taking this conceptual approach, we could then draw bootstrap samples of the residuals $\hat{\varepsilon}_1,\ldots,\hat{\varepsilon}_n$. Consider the $X_i$ component of the cases $(X_i,Y_i)$ fixed and the estimate $\hat{\beta}$ as fixed. Resample with replacement the vector $\hat{\varepsilon}$ to obtain the bootstrapped vector of residuals, $\hat{\varepsilon}^*=(\hat{\varepsilon}_1^*,\ldots, \hat{\varepsilon}_n^*)$. Then for each $i=1,\ldots,n$ our bootstrapped response variable would be $Y_i^*=X_i\hat{\beta}+\hat{\varepsilon}_i^*$. Denoting $X_i^*=X_i$, we obtain the bootstrapped dataset $Z^*=\{(X_1^*,Y_1*),\ldots,(X_n^*,Y_n^*)\}$. We then run a linear regression again on $Z^*$ to obtain estimates $\hat{\beta}^*$ and $\hat{s}^{*2}$. We present what was just discussed in the following algorithm.
  
  \begin{algorithm}
		\caption{Bootstrapping Residuals of Linear Regression Model}\label{residuals resample}
		\begin{algorithmic}[1]
			\State Fit a linear regression model to the data $Z=\{(X_1,Y_1),\ldots,(X_n,Y_n)\}$ to obtain the estimate $\hat{\beta}$. 
			\State Compute the estimated residuals vector $\hat{\varepsilon}=(\hat{\varepsilon}_1,\ldots,\hat{\varepsilon}_n)$, where $\hat{\varepsilon}_i=Y_i-X_i\hat{\beta}$.
			\For {$b=1,\ldots, B$ }
				\State Sample with replacement $n$ times from $\hat{\varepsilon}$ to obtain the bootstrapped residuals $\hat{\varepsilon}^{*b}=(\hat{\varepsilon}_1^{*b},\ldots,\hat{\varepsilon}_n^{*b})$.
				\For {$i=1,\ldots, n$ }
					\State Set $X_i^{*b}=X_i$ and $Y_i^{*b}=X_i\hat{\beta}+\hat{\varepsilon}_i^{*b}$
				\EndFor
				\State Obtain the bootstrapped dataset $Z^{*b}=\{(X_1^{*b}, Y_1^{*b}),\ldots, (X_n^{*b},Y_n^{*b})\}$.
				\State Fit a linear regression model on the bootstrap replicate of the data $Z^{*b}$ to obtain the estimates $\hat{\beta}^{*b}$ and $(\hat{s}^2)^{*b}$.
			\EndFor
		\end{algorithmic}
	\end{algorithm}
  Note that the two methods of resampling the linear regression model rely on different assumptions on the data. In particular, we are assuming that the error structure of the data does not rely on our predictors $X_i$. By fixing the $X_i$, we are assuming the distribution of the error terms $F$ is invariant to the data we have observed. This is a fairly strong assumption. Furthermore, resampling the residuals works well only if the variance of the errors is homoskedastic. If the variance of the errors is heteroskedastic, then the modeling assumptions we made are invalid and a weighted linear regression approach or wild bootstrap approach may be necessary. The cases resampling is much less sensitive to the modeling assumptions we make as we do not generate the $Y_i^*$'s in the same way as with the residuals method. If either of these two methods are applicable, we could then proceed to form summary statistics such as the sample mean or sample standard deviation of the coefficients and the estimate of the variance of the model. The two methods we presented for bootstrapping the linear regression are easily adapted to other types of models. In particular, we could consider applying these methods to highly non-linear estimators, which is precisely what we do with bagged and random forests with respect to CART trees.
  
## Random Forests

  Among the limitations of CART discussed in the preceding sections, the biggest issues is perhaps the variability of the method and the issue of collinearity among predictors. While overfitting can be addressed using cost-complexity pruning, variability and collinearity are not fixed by pruning. Random forests deal with these two issues of CART by introducing a resampling and randomization mechanism. The natural order is to first discuss bagged forests before turning to random forests. 
  
  \begin{algorithm}
		\caption{Bagged Forest algorithm}\label{bagged forest}
		\begin{algorithmic}[1]
			\For {$b=1,\ldots, B$ }
			\State Draw a bootstrap sample $Z_b^*$ of size $N$ from the training data $Z$.
			\State Grow a CART tree $T_b$ on each bootstrap sample $Z_b^*$.
			\EndFor
			\State Output the bagged forest ensemble $\{T_b\}_{b=1}^B$.
		\end{algorithmic}
	\end{algorithm}

  One method of improving CART trees is to bag them. Bagging, which stand for bootstrap aggregating, is a variation reduction technique particularly useful for improving the predictive power of weak learners. We are interested in bagging CART trees to reduce the variability of single trees under slight perturbations of the data. Generally bagged ensembles of learners produce robust predictions in comparison to running the learner once.
  
### How Bagged Forests Work 

  In particular, bagged forests are an ensemble obtained by taking many bootstrap samples of the data and fitting a tree to each bootstrapped dataset.  In this case, we grow the trees quite deep and do not employ cost-complexity pruning. The idea behind this decision is that we want to sufficiently explore the feature space using the tree ensemble, and since we are also taking an average of the trees, we are fine with overfitting at least a little bit. As the algorithm above for bagged forests indicates, the output is an ensemble of trees $\{T_b\}_{b=1}^B$. Given a test data point, we form a prediction by taking the average of predictions given by the tree ensemble:
	$$\hat{f}_{bf}^B(x)=\frac{1}{B}\sum_{b=1}^B T_b(x),$$ where $\hat{f}_{bf}^B$ indicates we are taking the bagged forest estimate of the underlying regression or classification function using $B$ many trees. Note that it is important to be consistent in the choice of splitting criterion throughout the bagging process.

  It has been shown by Friedman and Hall (2000) and Chen and Hall (2003) that bagging is especially effective when used on highly non-linear models such as CART trees. Under ideal conditions, bagged estimates of non-linear estimators reduces both the bias and variance of the estimator. So bagging trees seem to be effective because of the reduction to the variance of the estimator, which produces a more robust prediction. 

  When we bag trees each observation in the data is not used within each individual tree. A bootstrap replicate $Z_b$ of the data $Z$ will likely exclude some of the observations within the data. The observations used within the bootstrap replicate $Z_b$ is called the in-bag data  while the observations not used within the bootstrap replicate $Z_b$ is called the out-of-bag (OOB) data and is denoted by $\bar{Z}_b$. The OOB data allows us to approximate the test error of the ensemble as follows. For simplicity suppose we are in the regression setting (the classification setting is similar). Then the OOB estimate of the MSE of the bagged forest is given by $$MSE_{\textup{OOB}}(T;Z)=\frac{1}{B}\sum_{b=1}^B MSE(T_b;\bar{Z}_b).$$ As the number of trees in the ensemble grows, the OOB estimate of the MSE for the bagged forest converges to the LOOCV estimate of the MSE for forest (Friedman et al.). 

  A weakness of bagged forests is collinearity between trees (Friedman et al.). This collinearity between trees grown from the bootstrap sample can be addressed by adding a randomization mechanism in the tree growing process. When we grow a tree from a bootstrap sample, even with the randomness induced by resampling from the data, certain features may be explored at the expense of other just as interesting features. This is a particular issue with collinear predictors. If two predictors are collinear, then the bagged forest might consistently choose one predictor over another even if the predictor not chosen leads to splits that are just as informative. This is due in part to the greedy nature of the CART algorithm when searching for optimal splits over the feature space (Cite ESL or some similar textbook for this point). The algorithm does not take into account the second best or third best splits. Furthermore, it is not difficult to see that bagging will generally produce an ensemble of trees that are quite similar to one another, subject to some perturbations. These trees will be strongly correlated with other trees in the ensemble, so if there is a less explored part of the feature space, then the ensemble will struggle to produce good predictions over that part of the feature space. 
  
### The Random Forest Algorithm

  Random Forests try to deal with this issue of correlated trees and collinearity among predictors by choosing at random only $m\leq p$ of the predictors to be considered as candidate splitting variables at each split in each tree in the ensemble. This randomness further reduces the variance of the bagged forest by decorrelating the trees in the ensemble. Furthermore, while individually the trees may perform worse than a single pruned tree, collectively the ensemble has a better chance of exploring the feature space fully. 
	\begin{algorithm}
		\caption{Random Forest algorithm}\label{random forest}
		\begin{algorithmic}[1]
			\For {$b=1,\ldots, B$ }
			\State Draw a bootstrap sample $Z_b^*$ of size $N$ from the training data $Z$.
			\State Grow the CART tree $T_b$ on $Z_b^*$ with the following modification:
			\While {minimum node size $n_{\min}$ not reached across $T_b$} 
			\State Select $m\leq p$ candidate splitting variables at random.
			\State Pick the best splitting variable and splitting point among the $m$ variables selected at random.
			\State Split the node into two daughter nodes.
			\EndWhile
			\EndFor
			\State Output the random forest ensemble $\{T_b\}_{b=1}^B$.
		\end{algorithmic}
	\end{algorithm}

  To form a prediction at a test point $x$, we have in the regression setting $$\hat{f}_{rf}^B(x)=\frac{1}{B}\sum_{b=1}^B T_b(x).$$ [Add in the classification setting].
  
### Variable Importance Measures
	
  One of the advertised outputs of random forests is the variable importance (VI) measure. There are two main variable importance measures in common use, with the choice of VI measure varying depending on the setting and splitting criterion chosen. The first choice is the Mean Decrease in Impurity (MDI) which is typically used in the classification setting where the GINI index or Shannon entropy is used as the splitting criterion. The second choice is Mean Decrease in Accuracy (MDA) which is typically used in the regression setting where RSS has been used as the splitting criterion.  The idea of MDI is to find how much the nodal impurity $p(t)\Delta i(s,t)$ decreases for all nodes $t$ in which the variable of interest $X_j$ is used and to take that average over all trees in the ensemble. 	More important variables are those which are on average more often chose for splits and which also contribute most to reducing the nodal impurity of the trees.
  
  \begin{algorithm}
		\caption{MDI Variable Importance}\label{mdi variable importance}
		\begin{algorithmic}[1]
			\State Grow a random forest $\{T_b\}_{b=1}^B$.
			\For {$j=1,\ldots,p$ }
				\For {$b=1,\ldots,B$ }
				\State Compute the importance of $X_j$ in $T_b$ as $VI_b(X_j)=\sum_{t\in T_b} \mathbb{I}(j_t=j)p(t)\Delta i(s,t)$ to be the sum of the decrease in impurity over nodes where variable $X_j$ is used .
				\EndFor
				\State Compute the importance of $X_j$ in the random forest to be $VI(X_j)=\frac{1}{B}\sum_{b=1}^B VI_b(X_j).$
			\EndFor
		\end{algorithmic}
	\end{algorithm}

  The idea of MDA is to measure for each variable $X_j$, on average how much the predictive accuracy of the forest as measured using RSS suffers when the $X_j$ component is permuted across observations within the OOB dataset $\bar{Z}_b$ of each tree $T_b$ in the ensemble.  
  
  \begin{algorithm}
		\caption{MDA Variable Importance}\label{mdi variable importance}
		\begin{algorithmic}[1]
			\State Grow a random forest $\{T_b\}_{b=1}^B$.
			\For {$j=1,\ldots,p$ }
			\For {$b=1,\ldots,B$ }
			\State Permute the $X_j$ component of $Z_b$ to obtain the dataset $Z_b^j$, where $X_j$ has been permuted.
			\State Compute the importance of $X_j$ in $T_b$ to be $VI_b(X_j)=\frac{1}{|\bar{Z}_b|}(RSS(T_b,Z_b)-RSS(T_b,Z_b^j)).$
			\EndFor
			\State Compute the importance of $X_j$ in the random forest to be $VI(X_j)=\frac{1}{B}\sum_{b=1}^B VI_b(X_j).$
			\EndFor
		\end{algorithmic}
	\end{algorithm}

  More important variables in the random forests are those for which the variable importance is large, as those are the variables for which the predictive accuracy of the random forest suffers the most. Note that as variable importance measures currently defined and used, the threshold for importance of a variable is something which the researcher has to decide.  

### Issues with Random Forests

  While random forests are good out of the box predictors, there are situations where the random forest algorithm can fail to produce good predictions. If the underlying regression function the random forest is linear, if the predictors are highly correlated, or if the data cannot be bootstrapped, then the random forest will not perform well. Random forests are especially adept at handling highly non-linear functions, but can struggle with linear response compared to say linear regression. 

  If the predictors are highly correlated, then ithas been shown by Strobl et al. that the trees within the forest ensemble will be biased and the variable importance measures will not be reliable due to confounding between similar looking variables. While the randomization step in random forest algorithm can alleviate the correlation between trees in the ensemble, the collinearity between predictors can cause the predictive performance of the random forest to suffer. This is an issue with the underlying CART algorithm and Hothorn, Strobl, and their collaborators developed Conditional Inference trees and forests to deal with the issue of collinearity in the data (which will be explained in the next chapter). While using random forest variable importance for variable selection certainly seems like a good idea, the issue of collinear data makes the applicability of variable importance measures less immediate. Perhaps a proper use of random forest variable importance measures as a variable selection procedure would be to run a random forest only after some dimension reduction techniques have been applied. If the collinearity of the predictors has been dealt with, then heuristically the variable importance measures should be able to more accurately estimate the importance of predictors within the ensemble.

  Finally, there are situations where the data cannot be bootstrapped. This could be due to a number of factors including that the data has a heavily-tailed distribution or if there are particularly extreme values. In this case, a different bootstrap scheme could perhaps remedy the situation where the naive bootstrap fails. One common important resampling scheme used within random forests is to use the m-out-of-n bootstrap (also referred to subsampling or subagging in the literature). The m-out-of-n bootstrap is a resampling scheme which resamples with or without replacement $m\leq n$ observations from the data to form datasets with $m$ many data points to run the bootstrap computation. Bickel et al. have shown that in important cases, subsampling can succeed where the bootstrap fails. Of course, using less of the available data is less efficient, but in the context of random forests this loss of efficiency may not be an issue. In [Cite paper comparing CI trees to CART trees using IJ], simulation results showed that subsampling instead of using the standard nonparametric bootstrap can improve the performance of random forests. The authors suggested that subsampling further reduces the variance of the ensemble by producing trees that are even more decorrelated. Their heuristic is that there is a lower probability of duplicate data points being chosen using subsampling (this probability is zero if we are subsampling without replacement), furthermore there is a lower probability of highly correlated data points being chosen as one of the $m\leq n$ points. This certainly seems plausible, but we would like to see further simulation results or a technical result that explains why subsampling works well for forests. We would also like to note that most consistency results (Biau et al., Scornet et al., Ishwaran et al., and Wager et al.) make this subsampling assumption in their analysis of the random forest model, as subsampling constructions make the forest ensemble more amenable to mathematical analysis.
	
## Focus of this Thesis

  In the previous three sections we discussed the basics of trees, the bootstrap, and random forests. We would like to now discuss the direction we are going to take this thesis. We are interested in developing inferential tools using the random forest algorithm. The random forest often produces good predictions out of the box with little tuning required except growing a sufficient number of trees and choosing a good value of $m\leq p$ to try at each split in the tree growing process. The ease of fitting random forests is one of the advantages of the random forest compared to more complex methods like neural networks or support vector machines. Even if the random forest is easier to fit than neural nets or SVMs, the underlying random forest mechanism is quite complex. This complexity makes developing inferential tools with random forests difficult. In the next chapter we will discuss some approaches that have been developed recently by researchers. Of particular interest for us are the variable importance measures for random forests. There is the MDI variable importance and the MDA variable importance measure. While there are concerns about the bias in the MDI and MDA VI measures with random forests using CART tree's due to how CART tree's are constructed (Strobl et al., 2008), we would like to extend the inferential capabilities of random forests using CART as this is perhaps the most popular version of the algorithm in use. 

  The random forest VI can be viewed as a random variable with an unknown probability distribution. While determining the distribution of random forest VI's would be ideal towards developing inference for random forests, finding the distribution of random forest VI's is difficult and unclear in general. However, note that the bootstrap provides a computational method of approximating the sampling distribution of random forest VI measures. We could then proceed to obtaining estimates of the standard error and to form approximate interval estimates. We could grow a random forest once, obtain an estimate of the VI of each variable $X_j$ and then run a bootstrap to obtain estimates of the standard error of each variable. Inference could then proceed using the VI measures along with the estimate of standard error. Note that in this scheme there is two levels of bootstrap at play. There is the bootstrap within each random forest and then the bootstrapping to obtain the standard error estimate. The estimate we produce of the VI of a variable $X_j$ can be erratic. Running the random forest again can produce a new estimate of the VI of variable $X_j$. As Efron suggests in his 2014 article "Estimation and Accuracy after Model Selection," we could remedy the erratic, non-smooth nature of the VI estimate by bagging $VI(X_j)$. However, when we bag $VI(X_j)$ we would require a third-level of bootstrapping to produce an estimate of the standard error of the bagged estimate of $VI(X_j)$. This is computationally expensive, especially considering that the computational cost of the random forest depends on the number of observations, number of tree's grown, and number of variables present. However, applying Efron's infinitesimal jackknife (IJ) technique, which we will discuss in the next chapter, we can efficiently estimate the standard error of the bagged estimate of $VI(X_j)$ only using the two levels of bootstrapping. In this thesis we will be exploring the properties of IJ estimates of the standard error of the bagged estimate $VI(X_j)$ utilizing different bootstrap schemes. Utilizing the IJ, we can then produce interval estimates and confidence intervals of the variable importance of variables in the random forest. This allows us to characterize the uncertainty of $VI(X_j)$ when using variable importance measures for description, variable selection, and inference.
  
## Outline of Remaining Chapters

  Chapter 2: Inference and variable importance for random forests. In this chapter we will discuss the various approaches to inference for random forests that have been developed by researchers. These include Ishwaran and Louppe's analysis of VI measures of RF. Ishwaran's analysis is of the variable importance scheme he devised. Ishwaran's scheme is amenable to theoretical analysis and he produces asymptotic results for his scheme. Louppe analyzes the MDI variable importance in the context of a construction called totally randomized forests in the setting of finite sample spaces. Louppe investigates the effect of relevant and irrelevant variables to MDI VI in this setting and provides asymptotic results. These are two of themain papers investigating properties of VI.

  The other schemes include Mentch and Hooker's prediction intervals which are produced by interpreting the random forest ensemble as a particular type of U-statistic and applying the U-statistics theory to produce prediction intervals. They then can produce hypothesis tests and confidence intervals in this context. 

  A second approach towards prediction intervals is Wager's application of the IJ to the random forest estimator to produce prediction intervals and confidence intervals. Wager has a couple of papers in this area which we will discuss. In this part we will also go into greater detail about the IJ, it's properties, and why it works. 

  We will also discuss the conditional inference trees and conditional variable importance developed by Strobl, Hothorn, and their colleagues to deal with the biased manner in which the trees in random forests are constructed. In this part we will also discuss the Infforest variable importance measure developed by Aurora.

  Chapter 3: In this chapter we will go into great detail about the bootstrap and why it works. We will discuss the technical details of different methods of estimating the standard error of functional statistics, and we will go into the construction of different bootstrap confidence intervals along with properties of these confidence intervals. We will also discuss different bootstrap schemes and in particular subsampling, and why subsampling works well, especially in the random forest context. 

  Chapter 4: We will present our different approaches to bootstrapping variable importance measures and provide simulation results. The first approach is the cases bootstrap, the second approach is the bootstrapping residuals of the random forest approach. Within the cases approach, there are two ways we will try to constrain the variability of bootstrap replicates. Adopting the notation from general linear models, we could fit a pruned tree predicting $Y\sim X_1,\ldots,X_p$. Then we would bootstrap from the terminal nodes of this single tree to try to produce bootstrap replicates which capture the relationship between the variables. Another approach is for each predictor $X_j$, fit a single well pruned tree $X_j\sim X_1,\ldots,X_{j-1},X_{j+1},\ldots,X_p$. We would then draw bootstrap samples from the terminal nodes of this tree which respect the partition produced by these terminal nodes. The idea is to fix the relationship between every predictor not $X_j$ and allow the $X_j$'s to be exchanged across the bootstrap samples. We would produce simulation results of each approach and compare the statistical properties of these different constructions. 

  Chapter 5: If all goes well there might be a chapter 5 where we explore large sample properties and distribution of VI measures. This is likely going to be a difficult mathematical problem to solve in general, so a tangible course of action might be to pick a generating mechanism and easy VI measure from which we could figure out the VI distribution under some set of strong assumptions.




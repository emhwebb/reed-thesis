# Variable Importance and Inference for Random Forests

## Introduction 

  In this chapter we focus on approaches of inference for random forests involving variable importance measures of random forests. Such approaches involve using variable importance measures of random forests to evaluate the relative importance of different variables in the construction of the forest. @louppe2014 and @ishwaran2007 focused on theoretical properties of variable importance measures while @owens2017, @strobl2007, and @strobl2008 are centered on developing less biased variable importance measures for hypothesis testing. 

## Theoretical Analysis of MDA Variable Importance Measures 

  The approach to variable importances adopted in @ishwaran2007 differs from the original variable importance measures for random forests, so we require some additional vocabulary. 

Suppose $T$ is a binary recursive tree and suppose that $T$ has $M$ many terminal nodes. For each point $\mathbf{x}$ in the feature space, $T$ maps $\mathbf{x}$ to one of the $M$-many terminal nodes. In particular, if we let $\mathcal{X}$ denote the feature space, then $T$ is a function $\mathcal{X}\rightarrow \{1,\ldots, M\}$ defined by the equation $$T(\mathbf{x})=\sum_{m=1}^M m B_m(\mathbf{x}),$$ where $B_m(\mathbf{x})$ is a $0-1$ basis function which partition the feature space $\mathcal{X}$. 

Let $Z=\{(\mathbf{x}_i,Y_i)|i=1,\ldots,n\}$ denote the training data, where $\mathbf{x}_i$ is a covariate in the feature space and $Y_i$ is the response. We call $T$ a binary regression tree if it is a binary recursive tree grown from $Z$ using using binary recursive splits of the form $x_j\leq c$ and $x_j> c$ where split values $c$ are chosen based on the observed $\mathbf{x}_i$ in the training data $Z$. The value $a_m$ in the terminal node is the average response of the training observations falling in the $m$th node. That is, $$a_m=\frac{\sum_{i=1}^n \mathbb{I}\{T(\mathbf{x}_i)=m\} Y_i}{\sum_{i=1}^n \mathbb{I}\{T(\mathbf{x}_i)=m\}}.$$ Note that $\mathbf{x_i}$ denotes a row of covariates for the training data $Z$, while $x_j$ denotes the $j$th variable along the columns of the training data. 

For a binary regression tree, the basis functions $B_m(\mathbf{x})$ are product splines of the following form: $$B_m(\mathbf{x})=\prod_{l = 1}^{L_m} [x_{l(m)}-c_{l, m}]_{s_{l,m}},$$ where $L_m$ denotes the number of splits used to construct $B_m(\mathbf{x})$. For each split $l$, there is a splitting variable $\mathbf{x}_{l(m)}$ which denotes the $l(m)$th coordinate of $\mathbf{x}$ and a splitting value $c_{l,m}$. The $s_{l,m}$ are binary $\pm 1$ values, where for a given scalar $x$, $[x]_{+1}=\mathbb{I}(x>0)$ and $[x]_{-1}=\mathbb{I}(x\leq 0).$ Note that the basis functions satisfy an orthogonality property, which gives $B_m(\mathbf{x})B_{m'}(\mathbf{x})=0$ if $m\neq m'$. Note also that given a tree $T$, the predictor associated with the tree can be written as a linear combination of basis functions: $$\hat{\mu}(\mathbf{x})=\sum_{m=1}^M a_m B_m(\mathbf{x}).$$ We are now prepared to define Ishwaran's variable importance measure. 

Informally, the $MDA$ variable importance of a variable $x_j$ is the difference between $MSE$ of the tree $T$ when $x_j$ is randomly permuted and $MSE$ of the tree $T$ when $x_j$ is not permuted. As such a scheme of variable importance is difficult to analyze, Ishwaran proposes a surrogate measure. For the variable $x_j$, we drop $\mathbf{x}$ down the tree and follow the binary splits until either a terminal node is reached or a node with a split depending on $x_j$ is reached. If a node with a split depending on $x_j$ is reached, we then subsequently assign $\mathbf{x}$ randomly to either the left or right daughter node, whenever there is a split, until we reach a terminal node. The difference in $MSE$ between noising up $x_j$ and not noising up $x_j$ to be the variable importance of $x_j$ in the tree $T$. Denote the tree that results from noising up $x_j$ by $T_j$.

Such a scheme relies on the following heuristic: if we chose an adequete splitting rule to construct our tree, then we expect that variables that are split  earlier in the tree are more important, since prediction will suffer the most from noising up a variable higher up in the tree than a variable close to a terminal node. This is a behavior observed in CART trees and random forests based on CART: splits closer to the root node are more influential than splits close to terminal nodes, so the MDA or MDI variable importance of variables split on close to the root node are expected to be higher than otherwise.   

### Maximal Subtrees and Theoretical Results

Defining a structure on binary regression trees called subtrees, we can write the predictor for the noised up tree as a deterministic component relying on terminal nodes for no parent nodes involve a split on $x_j$, and a random component involving terminal nodes for which there are parent nodes involving a split on $x_j$. The definition of the subtree is quite intuitive. We call $\tilde{T}_j$ a $j$-subtree of the tree $T$, if the root node of $\tilde{T}_j$ has daughters that depend on an $x_j$ split. A $j$-subtree $\tilde{T}_j$ is a maximal $j$-subtree of the tree $T$, if there are no larger $j$-subtrees continaing $\tilde{T}_j$. For a given tree $T$ and for each variable $x_j$, there is a set of $K_j$ many distinct maximal $j$-subtrees, which are denoted by $\tilde{T}_{1,j},\ldots, \tilde{T}_{K_j,j}$. Note each distinct $\tilde{T}_{k,j}$ maximal $j$-subtree contains a set of distinct terminal nodes $M_{k,j}$. Each $M_{k,j}$ is distinct for $k=1,\ldots,K_j$, since we are working with maximal $j$-subtrees. Define $$M_j=\bigcup_{k=1}^{K_j} M_{k,j}$$ to be the set of terminal nodes for which there is a parent node involving a split on $x_j$. @ishwaran2007 proves the following lemma about the functional form of the predictor for the tree $T_j$. 

\begin{lemma}
Let $\hat{\mu}_j(\mathbf{x})$ denote the predictor for $T_j$. Then $$\hat{\mu}_j(\mathbf{x})=\sum_{m\notin M_j}a_m B_m(\mathbf{x})+\sum_{k=1}^{K_j} \tilde{a}_{k,j}\mathbb{I}\{T(\mathbf{x})\in M_{k,j}\},$$ where $\tilde{a}_{k,j}$ is the random terminal value assigned by $\tilde{T}_{k,j}$ under the random left right path through $\tilde{T}_{k,j}$. We write $\tilde{P}_{k,j}$ to denote the distribution of $\tilde{a}_{k,j}$. 
\end{lemma}

For a proof, see @ishwaran2007. It is a bit surprising that the functional form of $\hat{mu}_j(\mathbf{x})$ can be separated into the two components. Given this lemma and the definition of $j$-subtrees, we can more formally define the variable importance of $x_j$ in the tree $T$. 

Let $g$ be a loss function. Often we use the squared error to evaluate loss, which corresponds to $MSE$, but there is no strict requirement in the definition. Denote the test data by $(Y,\mathbf{x})$. Then the prediction error of the predictor $\hat{\mu}$ is given by $\mathbb{E}(g(Y,\hat{\mu}(\mathbf{x})))$. As a reminder, we assume that there is an underlying regression function $$Y=\mu(\mathbf{x})+\varepsilon,$$ where $\varepsilon$ is independent error with zero mean and variance $\sigma^2>0$. Similarly, we can define the prediction error of the predictor $\hat{\mu}_j$ to be $\mathbb{E}(g(Y,\hat{\mu}_j(\mathbf{x}))).$ Set $g(Y,\hat{\mu}(\mathbf{x}))=(Y-\hat{\mu}(\mathbf{x}))^2$ to be the $L_2$ loss, which corresponds to $MSE$. Define the variable importance of the variable $x_j$ to be $$\Delta_j=\mathbb{E}((Y-\hat{\mu}_j(\mathbf{x}))^2)-\mathbb{E}((Y-\hat{\mu}(\mathbf{x}))^2).$$ Application of the lemma and some manipulation allows us to write $$\Delta_j=\mathbb{E}(R_j(\mathbf{x})^2)-2\mathbb{E}\left(R_j(\mathbf{x})[\mu(\mathbf{x})-\hat{\mu}(\mathbf{x})]\right),$$ where $$R_j(\mathbf{x})=\sum_{k=1}^{K_j}\sum_{m\in M_{k,j}} (\tilde{a}_{k,j} -a_m)B_m(\mathbf{x}).$$ 

As noted in @ishwaran2007, we make the assumption that the true regression function $\mu$ is of similar form to $T$. That is, assume $$\mu(\mathbf{x})=\sum_{m=1}^M a_{m,0} B_m(\mathbf{x}),$$ where $a_{m,0}$ are the true, but unknown, terminal values. Under this and some other large sample assumptions, Ishwaran finds that asymptotically, each maximal $j$-subtree will tend to contribute equally to the variable importance $\Delta_j$. In effect, nodes closer to the root of a maximal $j$-subtree will have a larger effect on $\Delta_j$ than nodes closer to the terminal nodes. 


### Extension to Forest Ensembles

The framework developed in @ishwaran2007 extends naturally to forest ensembles and his theoretical result regarding forest ensembles provides some information of the behavior of variable importance measures for random forests. First for some notation, recall that in the forest ensemble setting, we draw $B$ many bootstrap resamples of the training data to obtain the bootstrap replicates $Z^{b}=\{(\mathbf{x}_i^b, Y_i^b)|i=1,\ldots,n\}$ of the training data for $b=1,\ldots, B$. We then construct a binary regression tree $T(\mathbf{x};b)$ on each bootstrap replicate of the data and have the forest $\hat{\mu}_F$ as the average of predictions over the trees $T(\mathbf{x};b)$: $$\hat{\mu}_F(\mathbf{x})=\frac{1}{B}\sum_{b=1}^B \hat{\mu}(\mathbf{x};b),$$ where $\hat{\mu}(\mathbf{x};b)$ denotes the predictor for the tree $T(\mathbf{x};b)$. Given that each $\hat{\mu}(\mathbf{x};b)$ is a linear combination of basis functions, we can write $$\hat{\mu_F}(\mathbf{x})=\frac{1}{B}\sum_{b=1}^B\sum_{m=1}^{M^b}a_m^b B_m(\mathbf{x};b).$$ Again assume that the $\mu(\mathbf{x})$ has a similar structure to $\mu(\mathbf{x})$. That is, $\mu(\mathbf{x})=\frac{1}{B}\sum_{b=1}^B\sum_{m=1}^{M^b}a_{m,0}^b B_m(\mathbf{x};b)$, where $a_{m,0}$ are the true, but unknown, terminal values. We denote the noised up forest predictor for the variable $x_j$ by $\hat{\mu}_{F_j}(\mathbf{x})$. As with the normal forest predictor, the noised up forest predictor is the average of the noised up predictors $\hat{\mu}_j$ over the bootstrap resamples: $$\hat{\mu}_j(\mathbf{x})=\sum_{b=1}^B \hat{\mu}_j(\mathbf{x};b).$$ As forest predictors are simply the average of individual trees, we can extend the lemma to $\hat{\mu}_{F_j}$ with the use of $b$'s in appropriate places denoting the usage of the $b$th bootstrap resample. That is, we can write $$\hat{\mu}_{F_j}(\mathbf{x})=\frac{1}{B}\sum_{b=1}^B\left(\sum_{m\notin M_j^b} a_m^b B_m(\mathbf{x};b)+\sum_{k=1}^{K_j^b} \tilde{a}_{k,j}^b \mathbb{I}\{T(\mathbf{x};b)\in M_{k,j}^b\} \right).$$ The variable importance of the variable $x_j$ is defined to be $$\Delta_{F_j}=\mathbb{E}((Y-\hat{\mu}_{F_j}(\mathbf{x}))^2)-\mathbb{E}((Y-\hat{\mu}(\mathbf{x}))^2).$$ Similar to the single tree case, the variable importance of the variable $x_j$ in the forest ensemble can be written as $$\Delta_{F_j}=\mathbb{E}(R_{F_j}(\mathbf{x})^2)-2\mathbb{E}\left(R_{F_j}(\mathbf{x})[\mu(\mathbf{x})-\hat{\mu}_F(\mathbf{x})]\right),$$ where $$R_{F_j}(\mathbf{x})=\frac{1}{B}\sum_{b=1}^b\sum_{k=1}^{K_j^b} \sum_{m\in M_{k,j}^b} (\tilde{a}_{k,j}^b-a_m^b)B_m(\mathbf{x}; b).$$ 

We are now ready to state a result about the asymptotic form of $\Delta_{f,j}$:
\begin{theorem}
Let $R_{F_j, 0}(\mathbf{x})$ be the function $R_{F_j}(\mathbf{x})$, in which each instance of $a_m^b$ has been replaced with $a_{m,0}^b$. Assume that $Y$ can be written in terms of a regression model $$Y=\mu(\mathbf{x})+\varepsilon$$ and that $$\mu(\mathbf{x})=\frac{1}{B}\sum_{b=1}^B\sum_{m=1}^{M^b}a_{m,0}^b B_m(\mathbf{x};b).$$ If $a_m^b\rightarrow a_{m,0}^b$ for each $m$ and $b$, then $$\Delta_{F_j}\rightarrow \mathbb{E}\left(R_{F_j,0}(\mathbf{x})^2\right)\leq\mathbb{E}\left(\frac{1}{B}\sum_{b=1}^b\sum_{k=1}^{K_j^b}\theta_0(k,j,b)\right),$$ where $$\theta_0(k,j,b)=\sum_{m\in M_{k,j}^b} \pi_{m,b}\tilde{P}_{k,j,0}^b\left(\tilde{a}_{k,j,0}^b-a_{m,0}^b\right)^2$$ is the node mean squared error for the $k$th maximal $v$-subtree of $T(\mathbf{x};b)$and $\pi_{m,b}=\mathbb{P}(B_m(\mathbf{x};b)=1|Z)$. 
\end{theorem}

For a proof, see @ishwaran2007. As @ishwaran2007 notes, the bound in the above theorem becomes tighter as the trees in the forest become more and more orthogonal to each other. The above theorem is applicable if the forest predictor is consistent. This suggests that forest ensemble methods which are consistent and produce trees that are at least approximately orthogonal to each other allow for variable importance to be characterized through node mean squared error of subtrees. Variable importance as defined above differs from the $MDA$ variable importance for random forests, but the two follow a similar heuristic towards measuring variable importance via the loss of predictive accuracy when the variable $x_j$ is pertrubed. Therefore this result suggests that, perhaps, so long as the random forest estimator is consistent and the trees constructed are at least approximately orthogonal, the $MDA$ variable importance can be characterized via the mean squared error of some analogous structure to maximal subtrees. This is, of course, conjectural, but we are interested whether for simpler forest algorithms, results similar to the theorem can be proven. Asymptotic results for the $MDA$ and also $MDI$ variable importance measures are difficult to formulate and prove for several reasons. First, the $CART$ tree construction process combined with the bootstrapping and randomization procedure of the random forest is difficult to mathematically analyze. Second, it is still unknown whether random forests constructed via $CART$ are consistent. However, there are other random forest variants known to be consistent, so those variants may be a natural starting point to try to prove asymptotic results about $MDA$ and $MDI$ variable importances. Third, the definition of $MDA$ variable importance involves a permutation step that is difficult to analyze mathematically. The above results are mathematically amenable due to the noising-up procedure adopted allowing for a quite elegant analysis of the variable importance. One possible approach to dealing with this issue may be to adopt the above definition of variable importance in different random forest settings and see if similar results to @ishwaran2007 may be derived. Another approach, which is the one more or less taken by @ishwaran2007, is to to find a mathematically tractible definition of variable importance which is approximately the MDA variable importance measure and proceed with analysis from there.  


## Theoretical Analysis of MDI Variable Importance

Up to now in this chapter, we have been discussing theoretical results concerning the $MDA$ variable importance. @louppe2014 has explored some of the theory for $MDI$ variable importance of forest ensembles.

In order to discuss the following theoretical results regarding MDI variable importance, we make some modifications to the setting we are working in. Assume that we are working in a finite probability space [this assumption is necessary primarily for technical reasons] and that observations $Y,X_1,\ldots,X_p$ in the training set $Z$ are categorical variables. Recall that if a forest ensemble is grown by choosing splits and variables maximizing the decrease in nodal impurity where $i(s,t)$ denotes the impurity measure used, then the MDI variable importance of the variable $X_j$ is defined to be $$VI(X_j)=\frac{1}{B}\sum_{b=1}^B \sum_{t\in T_b} \mathbb{I}(j_t=j)p(t)\Delta i(s,t),$$ where $p(t)$ is the proportion of samples which reach the node $t$ in the tree $T_b$. Note that MDI variable importance is the average over the bootstrap resamples of the sum of the weighted decrease in impurity over the nodes of the tree using $X_j$ as the splitting variable. 

Rather than working with random forests, we will work with an infinite ensemble of totally randomized and fully developed trees. Totally randomized and fully developed trees are a variant of forest ensembles given by the following. A totally randomized and fully developed tree is a decision tree in which each node $t$ is partioned using a variable $X_j$ picked uniformly at random among those not yet used at the parent nodes of $t$, where each node $t$ is split into $|X_j|$ sub-trees, i.e., one for each possiblevalue of $X_j$, and where the recursive construction process halts only when all $p$ variables have been used along the current branch. 

Using an impurity measure called Shannon entropy, @louppe2014 is able to show that only relevant variables (that is variables whose information content, conditional on the other variables, is useful towards explaining the response $Y$) have non-zero infinite sample size variable importance. Furthermore, this means that irrelevant variables (which are variables whose information content, conditional on the other variables, is not useful towards explaining the response $Y$) have a infinite sample size variable importance of zero. In fact, in this context, a variable is irrelevant if and only if it has an infinite sample size variable importance of zero. Generalizing from Shannon entropy to other impurity measures, for the GINI index (used in commonly in classification problems) and variance (used in regression via MSE), only irrelevant variables result in no decrease in nodal impurity. We would like to reiterate that these results concerning irrelvant variables are valid particularly in the setting of infinite ensembles of totally randomized and fully developed trees. 

In the random forest setting, the results discussed in the previous paragraph may not be valid. In the random forest setting, a key step in the construction of trees is the randomization step in which we choose at random a subset of size $m\leq p$ of the variables to choose the next split from. If $m>1$, then it is possible for some variables to never be chosen at a node $t$, since there will always be other variables which have a larger decrease in nodal impurity. This results in trees in which the variables with the largest reduction in nodal impurity are centered in the trunk of the tree with variables with smaller reduction in nodal impurity are pushed to leaves, or otherwise not chosen to split on. This is an undesirable property to have since this can result in an ensemble which inadequately explores the feature space and is thus more consistently biased towards some variables over others, even if other variables can provide only slightly lower reductions in nodal impurity. As a consequence, if $m>1$, then it is not the case that a variable is irrelevant if and only if it has a variable importance of zero. One implication is that in data sets where there are correlated variables, random forest variable importance measures may result in variable importances which do not reflect the actually importance of variables in explaining the response $Y$. In particular, it could be the case that if $X_j$ and $X_{j'}$ are two correlated variables, then even if both $X_j$ and $X_{j'}$ are important in explaining the response $Y$, it may be the case that $X_j$ is consistently chosen as the splitting variable over $X_{j'}$ if the two variables are both possible splitting variables. 


## Dealing with Bias in Random Forest Variable Importance

In the previous two sections of this chapter we discussed theoretical results pertaining to MDA and MDI variable importance measures. For MDA variable importance, a consistent ensemble with orthogonal trees have a nice asymptotic form in the framework developed in @ishwaran2007. On the otherhand, with MDI variable importance, if the trees are grown in the random forest setting, then there are issues of the variable importance not capturing the actual importance of the variables in the training data. This could particularly arise as an issue in data sets with correlated variables where equally important variables receive different variable importances due to bias introduced by tree construction process. In using the term `bias,' we would like to emphasise that this is not bias in the sense of biased estimators, but bias in the sense of which variables are chosen to be split upon in the tree growing process. Bias in the variable importance measures of random forests is concerning as we would like to be able to use random forests for inferential statistics. A couple of methods have been proposed to construct more reliable variable importance measures, which we discuss in this section. 


### Bias in Random Forest Variable Importance Measures

Bias in random forest variable importance measures was first substantively analyzed in @strobl2007. @strobl2007 ran a simulation study comparing MDI variable importance using the GINI index, MDA variable importance, and a conditional variable importance measure in a classification setting. The conditional variable importance measure used by @strobl2007 is a variable importance method similar to MDA variable importance, but utilizing an ensemble of conditional inference trees. We will define conditional inference trees and the conditional variable importance measures below. For now, note that @strobl2007 found that MDI variable importance using the GINI index and MDA variable importance tends to be biased from the expected variable importance value within their simulation. They also found that subsampling without replacement tends to reduce the bias in variable importance in comparison to using bootstrap resampling with replacement. In their analysis, @strobl2007 claim that the GINI index tends to favor variables with more potential cutpoints. Hence the MDI variable importance measure using the GINI index is biased towards variables with more potential cutpoints. @strobl2007 also found that in using the MDA variable importance measure, since variables that are split upon closer to the root node affect the predictive accuracy of the ensemble more than variables split upon in leaves, the MDA variable importance measure will be affected by the variable selection bias of individual trees. Furthermore, @strobl2008 find that correlated variables tend to be overselected in the tree growing process. In order to utilize random forest variable importance measures as inferential tools, several methods have been proposed to deal with the issue of variable selection bias in random forests.  

### Conditional Variable Importance 

The MDA variable importance measure can be viewed in the context of permutation tests and hypothesis testing. In particular, we can consider that the MDA variable importance measure operates on the null hypothesis that permuting the values of the variable $X_j$ has no affect on the predictive accuracy of the random forest predictor. This corresponds to the null hypothesis that the variable $X_j$ is independent of the response $Y$ and the variables $X_1,\ldots,X_p$. If the predictive accuracy of the random forest suffers from permuting $X_j$, that is if the $VI(X_j)$ is nonzero, then if also $X_j\perp X_{-j}$, it would follow that $Y$ and $X_j$ are not independent. However, it is important to note that the null hypothesis under which the MDA variable importance measure operates is: $$H_0:X_j \perp Y,X_{-j}.$$ If on the otherhand, $X_j$  and $X_{-j}$ are not independent, then $VI(X_j)$ will be biased by the correlation between $X_j$ and $X_{-j}$. It could be that $Y$ and $X_j$ are not independent or that $Y$ and $X_j$ are independent, but in either case, if the predictor variables are correlated, we see that $VI(X_j)$ will not accurately capture the importance of $X_j$. @strobl2008 propose a permutation scheme corresponding to the null hypothesis $$H_0:(X_j\perp Y)|X_{-j},$$ that is that $X_j$ is independent of $Y$ conditional on the $X_{-j}$. Such a permutation scheme would take into account the correlation structure of the predictor variables and would allow for a less biased variable importance measure so long as the individual trees within the ensembles do not exhibit large variable selection bias. 

@strobl2008 propose defining a grid within which values of $X_j$ are permuted according to partitions of the feature space given by each tree. While @strobl2008 propose using unbiased conditional inference trees from @hothorn2006 to determine the partition, it is possible to use partitions given by CART trees. The algorithm is presented as follows. 

  \begin{algorithm}
    \caption{Conditional Variable Importance} \label{conditional variable importance}
      \begin{algorithmic}[1]
        \State Grow a forest ensemble $\{T_b\}_{b=1}^B$
          \For{ $j=1,\ldots,p$ }
            \For{ $b=1,\ldots,B$ }
            \State Compute $RSS(T_b,Z_b)$.
            \For{ all variables $X_i$ to be conditioned on }
            \State Extract cutpoints that split $X_i$ in the tree $T_b$ and create a grid $\mathcal{G}$ by bisecting the sample space in each cutpoint.
            \EndFor
            \State Within the grid $\mathcal{G}$, permute the values of $X_j$ and denote the permuted resample by $Z_b^j$.
            \State Compute $RSS(T_b,Z_b^j)$.
            \State Compute the conditional variable importance of $X_j$ in the tree $T_b$ to be $VI_b(X_j)=\frac{1}{|Z_b|}\left(RSS(T_b,Z_b)-RSS(T_b,Z_b^j)\right)$.
            \EndFor
            \State Compute the conditional variable importance of $X_j$ in the forest ensemble to be $VI(X_j)=\frac{1}{B}\sum_{b=1}^B VI_b(X_j)$.
          \EndFor
      \end{algorithmic}
  \end{algorithm}


To determine the variables $X_i$ to be conditioned on @strobl2008 suggest including only variables whose empirical correlation with the variable $X_j$ suggests some reasonable threshold. 

### Infforest Variable Importance 

While simulation results of @strobl2008 suggest that the conditional variable importance is more effective at capturing the importance of each variable $X_j$ compared to the marginal approach followed in MDA, they note that conditional variable importance cannot completely eliminate preference for correlated variables in random forests. @owens2017 instead suggests a partition then permute scheme which produces a sampling distribution for the variable importance of each variable $X_j$. Once a sampling distribution for $VI(X_j)$ has been obtained, hypothesis testing and other sorts of statistical inference can proceed. As the Infforest variable importance is quite complicated, some explanation is needed. 

After growing a forest ensemble $\{T_b\}_{b=1}^B$, infforest variable importance values are computed as follows. First, for each $b=1,\ldots,B$ and each variable $X_j$, grow a tree $X_j\sim X_1,\ldots,X_{j-1},X_{j+1},\ldots,X_p$ using all $p-1$ predictors using the in-bag sample used to grow $T_b$. Denote this tree by $T_j^{b*}$. The rows of $\overline{Z}_b$ are permuted within the partitions of the feature space determined by $T_j^{b*}$. The difference in predictive accuracy of $T_b$ before and after the OOB sample $\overline{Z}_b$ has been permuted is the infforest variable importance for the variable $X_j$ in the tree $T_b$. @owens2017 suggests then using the sampling distribution of $VI(X_j)$ obtained from the infforest partition-permute scheme to test the null hypothesis that the variable importance of $X_j$ is zero. On the other hand, if we let $VI_b(X_j)$ denote the infforest variable importance of the variable $X_j$ in the tree $T_b$, then we could define the infforest variable importance of $X_j$ in the forest ensemble $\{T_b\}_{b=1}^B$ to be $$VI(X_j)=\frac{1}{B}\sum_{b=1}^B VI_b(X_j).$$ In particular, this corresponds to the mean of the sampling distribution of the infforest variable importances of $X_j$ over the trees $T_b$. 

Originally, @owens2017 developed for random forests using CART trees as base learners. However, the infforest variable importance could certainly be used with random forest variants such as conditional inference forests used in @strobl2008. Another possible modification of the infforest variable importance algorithm is to only grow an auxilary tree predicting $X_j\sim X_{\alpha_1},\ldots,X_{\alpha_k}$, where $X_{\alpha_1},\ldots,X_{\alpha_k}$ are variables whose correlation with $X_j$ exceeds some reasonable threshold. This would combine the suggestion in @strobl2008 of considering the correlation structure of the predictor variables in setting up the grid for the conditional variable importance with the infforest partition-permute scheme.

One issue with infforest variable importance is the steep computational cost of the algorithm. The algorithm grows an auxilary tree $T_{j}^{b*}$ for each variable $X_j$ and bootstrap resample $b$ and then proceeds to permute the OOB sample with respect to each $T_j^{b*}$. Even implementing the infforest variable importance algorithm on small bootstrap resamples of around several hundred is computationally intensive. 

  \begin{algorithm}
    \caption{Infforest Variable Importance} \label{infforest variable importance}
      \begin{algorithmic}[1]
        \State Grow a forest ensemble $\{T_b\}_{b=1}^B$
          \For{ $j=1,\ldots,p$ }
            \For{ $b=1,\ldots,B$ }
              \State Compute $RSS(T_b,\overline{Z}_b)$.
              \State Grow a tree $T_j^{b*}$ predicting $X_j\sim X_{-j}$ using in-bag sample.
              \State Permute rows of the OOB sample $\overline{Z}_b$ with respect to the partitions of the feature space from $T_j^{b*}$ to obtain the permuted OOB sample $\overline{Z}_b^*$. 
              \State Compute $RSS(T_b,\overline{Z}_b^*)$.
              \State Compute the infforest variable importance of $X_j$ in $T_b$ to be $VI_b(X_j)=\frac{1}{\overline{Z}_b}\left(RSS(T_b,\overline{Z}_b)-RSS(T_b,\overline{Z}_b^*)\right)$.  
            \EndFor
            \State Test the null hypothesis that the variable importance of $X_j$ is zeror using the distribution of values $VI_b(X_j)$. 
          \EndFor
      \end{algorithmic}
  \end{algorithm}
  

# Variable Importance and Inference for Random Forests

## Introduction 

  In this chapter we focus on various approaches towards inference for random forests that have been developed. Generally, we could consider to be two approaches towards inference with random forests. 
  
  The first approach involves using variable importance measures of random forests to evaluate the relative importance of different variables in the construction of the forest. Ishwaran, Louppe, Owens, and Strobl et al. have been involved in work in this area. Louppe and Ishwaran focused on theoretical properties of variable importance measures while Owens, and Strobl and colleagues work is centered on developing less biased variable importance measures for hypothesis testing. 
  
  The second approach involves utilizing properties of the functional form of the random forest estimator to construct prediction intervals. Once predicton intervals have been constructed, depending on the context, confidence intervals and hypothesis tests can be produced. Mentch and Hooker's work involves interpreting the random forest ensemble as a particular type of U-statistics, and applying theoretical results about U-statistics to construct prediction intervals. Wager's work involves applying the infintisemal jackknife to the random forest estimator to produce prediction intervals. 
  With both approaches there are asymptotic results that we will discuss in some detail. In addition, we will discuss some of the consistency results that have been proven for random forests. It is worth noting here that in mathematically analyzing the random forest algorithm, there are trade-offs between fidelity to the CART algorithm and amenability to mathematical tool. To our best knowledge, at this time there are no theoretical results showing that random forests constructed using CART are consistent. Generally, simplifications need to be made to the random forest algorithm to allow for asymptotic analysis. These simplifications usually involve using a different partitioning scheme than CART and also in working a sampling without replacement framework. We will note when such assumptions are being made when necessary. Otherwise, we assume a setting in which the random forest is constructed using CART and bootstrap sampling with replacement. 
  
## Some Theory for Variable Importance Measures

In this section we discuss Louppe and Ishwaran's work exploring theoretical properties of variable importance measures. 

### Ishwaran's Variable Importance Measure

  Ishwaran's approach to variable importance measures of random forests, as developed in ``Variable Importance in Binary Regression Trees and Forests,'' differs from the original variable importance measures for random forests, so we require some additional vocabulary. 

Suppose $T$ is a binary recursive tree and suppose that $T$ has $M$ many terminal nodes. For each point $\mathbf{x}$ in the feature space, $T$ maps $\mathbf{x}$ to one of the $M$-many terminal nodes. In particular, if we let $\mathcal{X}$ denote the feature space, then $T$ is a function $\mathcal{X}\rightarrow \{1,\ldots, M\}$ defined by the equation $$T(\mathbf{x})=\sum_{m=1}^M m B_m(\mathbf{x}),$$ where $B_m(\mathbf{x})$ is a $0-1$ basis function which partition the feature space $\mathcal{X}$. 

Let $Z=\{(\mathbf{x}_i,Y_i)|i=1,\ldots,n\}$ denote the training data, where $\mathbf{x}_i$ is a covariate in the feature space and $Y_i$ is the response. We call $T$ a binary regression tree if it is a binary recursive tree grown from $Z$ using using binary recursive splits of the form $x_j\leq c$ and $x_j> c$ where split values $c$ are chosen based on the observed $\mathbf{x}_i$ in the training data $Z$. The value $a_m$ in the terminal node is the average response of the training observations falling in the $m$th node. That is, $$a_m=\frac{\sum_{i=1}^n \mathbb{I}\{T(\mathbf{x}_i)=m\} Y_i}{\sum_{i=1}^n \mathbb{I}\{T(\mathbf{x}_i)=m\}}.$$ Note that $\mathbf{x_i}$ denotes a row of covariates for the training data $Z$, while $x_j$ denotes the $j$th variable along the columns of the training data. 

For a binary regression tree, the basis functions $B_m(\mathbf{x})$ are product splines of the following form: $$B_m(\mathbf{x})=\prod_{l = 1}^{L_m} [x_{l(m)}-c_{l, m}]_{s_{l,m}},$$ where $L_m$ denotes the number of splits used to construct $B_m(\mathbf{x})$. For each split $l$, there is a splitting variable $\mathbf{x}_{l(m)}$ which denotes the $l(m)$th coordinate of $\mathbf{x}$ and a splitting value $c_{l,m}$. The $s_{l,m}$ are binary $\pm 1$ values, where for a given scalar $x$, $[x]_{+1}=\mathbb{I}(x>0)$ and $[x]_{-1}=\mathbb{I}(x\leq 0).$ Note that the basis functions satisfy an orthogonality property, which gives $B_m(\mathbf{x})B_{m'}(\mathbf{x})=0$ if $m\neq m'$. Note also that given a tree $T$, the predictor associated with the tree can be written as a linear combination of basis functions: $$\hat{\mu}(\mathbf{x})=\sum_{m=1}^M a_m B_m(\mathbf{x}).$$ We are now prepared to define Ishwaran's variable importance measure. 

Informally, the $MDA$ variable importance of a variable $x_j$ is the difference between $MSE$ of the tree $T$ when $x_j$ is randomly permuted and $MSE$ of the tree $T$ when $x_j$ is not permuted. As such a scheme of variable importance is difficult to analyze, Ishwaran proposes a surrogate measure. For the variable $x_j$, we drop $\mathbf{x}$ down the tree and follow the binary splits until either a terminal node is reached or a node with a split depending on $x_j$ is reached. If a node with a split depending on $x_j$ is reached, we then subsequently assign $\mathbf{x}$ randomly to either the left or right daughter node, whenever there is a split, until we reach a terminal node. The difference in $MSE$ between noising up $x_j$ and not noising up $x_j$ to be the variable importance of $x_j$ in the tree $T$. Denote the tree that results from noising up $x_j$ by $T_j$.

Such a scheme relies on the following heuristic: if we chose an adequete splitting rule to construct our tree, then we expect that variables that are split  earlier in the tree are more important, since prediction will suffer the most from noising up a variable higher up in the tree than a variable close to a terminal node. This is a behavior observed in CART trees and random forests based on CART: splits closer to the root node are more influential than splits close to terminal nodes, so the MDA or MDI variable importance of variables split on close to the root node are expected to be higher than otherwise.   

### Maximal Subtrees and Theoretical Results

Defining a structure on binary regression trees called subtrees, Ishwaran is able to write the predictor for the noised up tree as a deterministic component relying on terminal nodes for no parent nodes involve a split on $x_j$, and a random component involving terminal nodes for which there are parent nodes involving a split on $x_j$. The definition of the subtree is quite intuitive. We call $\tilde{T}_j$ a $j$-subtree of the tree $T$, if the root node of $\tilde{T}_j$ has daughters that depend on an $x_j$ split. A $j$-subtree $\tilde{T}_j$ is a maximal $j$-subtree of the tree $T$, if there are no larger $j$-subtrees continaing $\tilde{T}_j$. For a given tree $T$ and for each variable $x_j$, there is a set of $K_j$ many distinct maximal $j$-subtrees, which are denoted by $\tilde{T}_{1,j},\ldots, \tilde{T}_{K_j,j}$. Note each distinct $\tilde{T}_{k,j}$ maximal $j$-subtree contains a set of distinct terminal nodes $M_{k,j}$. Each $M_{k,j}$ is distinct for $k=1,\ldots,K_j$, since we are working with maximal $j$-subtrees. Define $$M_j=\bigcup_{k=1}^{K_j} M_{k,j}$$ to be the set of terminal nodes for which there is a parent node involving a split on $x_j$. Ishwaran proves the following lemma about the functional form of the predictor for the tree $T_j$. 

\begin{lemma}
Let $\hat{\mu}_j(\mathbf{x})$ denote the predictor for $T_j$. Then $$\hat{\mu}_j(\mathbf{x})=\sum_{m\notin M_j}a_m B_m(\mathbf{x})+\sum_{k=1}^{K_j} \tilde{a}_{k,j}\mathbb{I}\{T(\mathbf{x})\in M_{k,j}\},$$ where $\tilde{a}_{k,j}$ is the random terminal value assigned by $\tilde{T}_{k,j}$ under the random left right path through $\tilde{T}_{k,j}$. We write $\tilde{P}_{k,j}$ to denote the distribution of $\tilde{a}_{k,j}$. 
\end{lemma}

For a proof, the reader is referred to Ishwaran's paper. It is a bit surprising that the functional form of $\hat{mu}_j(\mathbf{x})$ can be separated into the two components. Given this lemma and the definition of $j$-subtrees, we can more formally define the variable importance of $x_j$ in the tree $T$. 

Let $g$ be a loss function. Often we use the squared error to evaluate loss, which corresponds to $MSE$, but there is no strict requirement in the definition. Denote the test data by $(Y,\mathbf{x})$. Then the prediction error of the predictor $\hat{\mu}$ is given by $\mathbb{E}(g(Y,\hat{\mu}(\mathbf{x})))$. As a reminder, we assume that there is an underlying regression function $$Y=\mu(\mathbf{x})+\varepsilon,$$ where $\varepsilon$ is independent error with zero mean and variance $\sigma^2>0$. Similarly, we can define the prediction error of the predictor $\hat{\mu}_j$ to be $\mathbb{E}(g(Y,\hat{\mu}_j(\mathbf{x}))).$ Set $g(Y,\hat{\mu}(\mathbf{x}))=(Y-\hat{\mu}(\mathbf{x}))^2$ to be the $L_2$ loss, which corresponds to $MSE$. Define the variable importance of the variable $x_j$ to be $$\Delta_j=\mathbb{E}((Y-\hat{\mu}_j(\mathbf{x}))^2)-\mathbb{E}((Y-\hat{\mu}(\mathbf{x}))^2).$$ Application of the lemma and some manipulation allows us to write $$\Delta_j=\mathbb{E}(R_j(\mathbf{x})^2)-2\mathbb{E}\left(R_j(\mathbf{x})[\mu(\mathbf{x})-\hat{\mu}(\mathbf{x})]\right),$$ where $$R_j(\mathbf{x})=\sum_{k=1}^{K_j}\sum_{m\in M_{k,j}} (\tilde{a}_{k,j} -a_m)B_m(\mathbf{x}).$$ 

To aid in his analysis, Ishwaran makes the assumption that the true regression function $\mu$ is of similar form to $T$. That is, assume $$\mu(\mathbf{x})=\sum_{m=1}^M a_{m,0} B_m(\mathbf{x}),$$ where $a_{m,0}$ are the true, but unknown, terminal values. Under this and some other large sample assumptions, Ishwaran finds that asymptotically, each maximal $j$-subtree will tend to contribute equally to the variable importance $\Delta_j$. In effect, Ishwaran finds that nodes closer to the root of a maximal $j$-subtree will have a larger effect on $\Delta_j$ than nodes closer to the terminal nodes. 


### Extension to Forest Ensembles

Ishwaran's framework extends naturally to forest ensembles and his theoretical result regarding forest ensembles provides some information of the behavior of variable importance measures for random forests. First for some notation, recall that in the forest ensemble setting, we draw $B$ many bootstrap resamples of the training data to obtain the bootstrap replicates $Z^{b}=\{(\mathbf{x}_i^b, Y_i^b)|i=1,\ldots,n\}$ of the training data for $b=1,\ldots, B$. We then construct a binary regression tree $T(\mathbf{x};b)$ on each bootstrap replicate of the data and have the forest $\hat{\mu}_F$ as the average of predictions over the trees $T(\mathbf{x};b)$: $$\hat{\mu}_F(\mathbf{x})=\frac{1}{B}\sum_{b=1}^B \hat{\mu}(\mathbf{x};b),$$ where $\hat{\mu}(\mathbf{x};b)$ denotes the predictor for the tree $T(\mathbf{x};b)$. Given that each $\hat{\mu}(\mathbf{x};b)$ is a linear combination of basis functions, we can write $$\hat{\mu_F}(\mathbf{x})=\frac{1}{B}\sum_{b=1}^B\sum_{m=1}^{M^b}a_m^b B_m(\mathbf{x};b).$$ Again assume that the $\mu(\mathbf{x})$ has a similar structure to $\mu(\mathbf{x})$. That is, $\mu(\mathbf{x})=\frac{1}{B}\sum_{b=1}^B\sum_{m=1}^{M^b}a_{m,0}^b B_m(\mathbf{x};b)$, where $a_{m,0}$ are the true, but unknown, terminal values. We denote the noised up forest predictor for the variable $x_j$ by $\hat{\mu}_{F_j}(\mathbf{x})$. As with the normal forest predictor, the noised up forest predictor is the average of the noised up predictors $\hat{\mu}_j$ over the bootstrap resamples: $$\hat{\mu}_j(\mathbf{x})=\sum_{b=1}^B \hat{\mu}_j(\mathbf{x};b).$$ As forest predictors are simply the average of individual trees, we can extend the lemma to $\hat{\mu}_{F_j}$ with the use of $b$'s in appropriate places denoting the usage of the $b$th bootstrap resample. That is, we can write $$\hat{\mu}_{F_j}(\mathbf{x})=\frac{1}{B}\sum_{b=1}^B\left(\sum_{m\notin M_j^b} a_m^b B_m(\mathbf{x};b)+\sum_{k=1}^{K_j^b} \tilde{a}_{k,j}^b \mathbb{I}\{T(\mathbf{x};b)\in M_{k,j}^b\} \right).$$ The variable importance of the variable $x_j$ is defined to be $$\Delta_{F_j}=\mathbb{E}((Y-\hat{\mu}_{F_j}(\mathbf{x}))^2)-\mathbb{E}((Y-\hat{\mu}(\mathbf{x}))^2).$$ Similar to the single tree case, the variable importance of the variable $x_j$ in the forest ensemble can be written as $$\Delta_{F_j}=\mathbb{E}(R_{F_j}(\mathbf{x})^2)-2\mathbb{E}\left(R_{F_j}(\mathbf{x})[\mu(\mathbf{x})-\hat{\mu}_F(\mathbf{x})]\right),$$ where $$R_{F_j}(\mathbf{x})=\frac{1}{B}\sum_{b=1}^b\sum_{k=1}^{K_j^b} \sum_{m\in M_{k,j}^b} (\tilde{a}_{k,j}^b-a_m^b)B_m(\mathbf{x}; b).$$ 

We are now ready to state a result about the asymptotic form of $\Delta_{f,j}$:
\begin{theorem}
Let $R_{F_j, 0}(\mathbf{x})$ be the function $R_{F_j}(\mathbf{x})$, in which each instance of $a_m^b$ has been replaced with $a_{m,0}^b$. Assume that $Y$ can be written in terms of a regression model $$Y=\mu(\mathbf{x})+\varepsilon$$ and that $$\mu(\mathbf{x})=\frac{1}{B}\sum_{b=1}^B\sum_{m=1}^{M^b}a_{m,0}^b B_m(\mathbf{x};b).$$ If $a_m^b\rightarrow a_{m,0}^b$ for each $m$ and $b$, then $$\Delta_{F_j}\rightarrow \mathbb{E}\left(R_{F_j,0}(\mathbf{x})^2\right)\leq\mathbb{E}\left(\frac{1}{B}\sum_{b=1}^b\sum_{k=1}^{K_j^b}\theta_0(k,j,b)\right),$$ where $$\theta_0(k,j,b)=\sum_{m\in M_{k,j}^b} \pi_{m,b}\tilde{P}_{k,j,0}^b\left(\tilde{a}_{k,j,0}^b-a_{m,0}^b\right)^2$$ is the node mean squared error for the $k$th maximal $v$-subtree of $T(\mathbf{x};b)$and $\pi_{m,b}=\mathbb{P}(B_m(\mathbf{x};b)=1|Z)$. 
\end{theorem}

[Note: explain node mean squared error. Also show a proof.]

In the above theorem, we 

As @ishwaran2007 notes, the bound in the above theorem becomes tighter as the trees in the forest become more and more orthogonal to each other. The above theorem is applicable if the forest predictor is consistent. This suggests that forest ensemble methods which are consistent and produce trees that are at least approximately orthogonal to each other allow for variable importance to be characterized through node mean squared error of subtrees. Variable importance as defined above differs from the $MDA$ variable importance for random forests, but the two follow a similar heuristic towards measuring variable importance via the loss of predictive accuracy when the variable $x_j$ is pertrubed. Therefore this result suggests that, perhaps, so long as the random forest estimator is consistent and the trees constructed are at least approximately orthogonal, the $MDA$ variable importance can be characterized via the mean squared error of some analogous structure to maximal subtrees. This is, of course, conjectural, but we are interested whether for simpler forest algorithms, results similar to the theorem can be proven. Asymptotic results for the $MDA$ and also $MDI$ variable importance measures are difficult to formulate and prove for several reasons. First, the $CART$ tree construction process combined with the bootstrapping and randomization procedure of the random forest is difficult to mathematically analyze. Second, it is still unknown whether random forests constructed via $CART$ are consistent. However, there are other random forest variants known to be consistent, so those variants may be a natural starting point to try to prove asymptotic results about $MDA$ and $MDI$ variable importances. Third, the definition of $MDA$ variable importance involves a permutation step that is difficult to analyze mathematically. The above results are mathematically amenable due to the noising-up procedure adopted allowing for a quite elegant analysis of the variable importance. One possible approach to dealing with this issue may be to adopt the above definition of variable importance in different random forest settings and see if similar results to @ishwaran2007 may be derived. Another approach, which is the one more or less taken by @ishwaran2007, is to to find a mathematically tractible definition of variable importance which is approximately the $MDA$ variable importance measure and proceed with analysis from there.  

[Lead into Strobl and the problem of collinearity and bias(bias as in which variables are chosen) in random forests]
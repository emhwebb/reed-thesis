# Added Variable Plot Importance

## Introduction

In the previous chapter, we discussed theoretical properties and observed behavior of random forest variable importance measures. In particular, we discussed issues of bias present in the MDA variable importance measure. @strobl2008 and @owens2017 proposed the conditional variable importance measure and the Infforest variable importance measures, respectively, as methods of accounting for bias in variable selection among correlated predictors when measuring variable importance in a forest ensemble. Conditional variable importance and Infforest variable importance aim at measuring the conditional importance of a particular variable by conditionally permuting OOB data according to some criteria measured from the data, whether that be empirical correlation in the case of conditional variable importance, or the partitions induced by a particular tree in the case of Infforest variable importance. In any case, we are primarily interested in the conditional importance of a predictor given the information provided by other predictors. While we would like to have a exact method of computing the conditional importance of a predictor in a random forest ensemble, in practice it is unclear how to construct an exact method given the complexity of the random forest ensemble. We can, however, estimate the effect of adding a predictor to the set of predictors that the forest ensemble can split on via a type of diagnostic plot called an added variable plot. \par

Added variable plots are a diagnostic plot which estimate the effect of adding a predictor variable to the model. In this chapter we propose a method of measuring the importance of each predictor in a random forest ensemble via a quantity we call the Added Variable Importance (AVI) that depends on the added variable plot of each predictor. The AVI attempts to estimate the effect of adding a predictor as a possible candidate in the splitting step of the random forest ensemble. To understand AVI, we regress from the random forest setting to the linear regression setting to explain how added variable plots work. \par

## Added Variable Plots in Linear Regression 

```{r results="asis", message = FALSE, echo = FALSE, warning = FALSE, fig.env = 'marginfigure', fig.cap = "\\label{AVPlinex}Linear regression added variable plot for example dataset"}

lm.avp.list <- siml.ex.list[[5]]

var1.avp <- ggplot(lm.avp.list[[1]], aes(x = x.res, y = y.res))+
  geom_point(size = 1, alpha = 0.3)+
  labs(title="AVP for V1")+
  theme(text = element_text(size = 10))
var2.avp <- ggplot(lm.avp.list[[2]], aes(x = x.res, y = y.res))+
  geom_point(size = 1, alpha = 0.3)+
  labs(title="AVP for V2")+
  theme(text = element_text(size = 10))
var3.avp <- ggplot(lm.avp.list[[3]], aes(x = x.res, y = y.res))+
  geom_point(size = 1, alpha = 0.3)+
  labs(title="AVP for V3")+
  theme(text = element_text(size = 10))

#blank <- grid.rect(gp=gpar(col="white"))

do.call("grid.arrange", c(list(var1.avp,var2.avp,var3.avp), ncol = 2))

```

Suppose that we have data $Z=\{(\mathbf{x}_i,Y_i)|i=1,\ldots,n\}$, where $\mathbf{x}_i$ is a covariate in the feature space, and that we fit a multiple linear regression model of the form $\mathbf{Y}_1=\mathbf{X}\beta_1+\mathbf{W}\alpha+\mathbf{\varepsilon}_1$ where $\mathbf{X}$ is a $n\times p$ matrix consisting of $p-1$ predictors, $\beta=(\beta_0,\beta_1,\ldots,\beta_p)^T$, $\mathbf{W}=(w_1,\ldots,w_n)$ is a predictor, $\alpha$ is a scalar, and $\text{Var}(\mathbf{\varepsilon})=\sigma^2 I$. \par

Say we are interested in estimating the effect of the predictor $\mathbf{W}$ in the regression model $\hat{\mathbf{Y}}_1$. We could first fit the models $\hat{\mathbf{Y}}_2=\mathbf{X}\beta_2$ and $\hat{\mathbf{W}}=\mathbf{X}\delta$, where $\beta_2$ and $\delta$ are coefficient vectors like $\beta_1$ and then plot the residuals $\mathbf{W}-\hat{\mathbf{W}}$ against the residuals $\mathbf{Y}-\hat{\mathbf{Y}}_2$. The plot we obtain by plotting $\mathbf{W}-\hat{\mathbf{W}}$ against $\mathbf{Y}-\hat{\mathbf{Y}}_2$ is called the added variable plot of $\mathbf{W}$ and measures the effect of $\mathbf{W}$ on $\mathbf{Y}$ once we have adjusted for the effect of $\mathbf{X}$ on $\mathbf{W}$ and $\mathbf{Y}$, respectively. In particular, the residuals that we plot for the added variable plot is the portion of $\mathbf{W}$ unexplained by $\mathbf{X}$ on the $x$-axis and the portion of $\mathbf{Y}$ unexplained by $\hat{\mathbf{Y}}_2=\mathbf{X}\beta_2$ on the $y$-axis. \par

In the linear regression setting the added variable plot has useful properties. In particular, let $\alpha_{AVP}$ denote the estimate of the slope from regressing $\mathbf{Y}-\hat{\mathbf{Y}}_2$ on $\mathbf{W}-\hat{\mathbf{W}}$. Then with some linear algebra it can be shown that $\alpha_{AVP}$ is equal to the least-squares estimate of $\alpha$ from $\hat{\mathbf{Y}}_1=\mathbf{X}\beta_1+\mathbf{W}\alpha$. For full details see @sheather2009 and @cook1982. Hence if we want to estimate the linear effect of the variable $\mathbf{W}$ on the linear regression $\hat{\mathbf{Y}}_1$, we can construct and visually inspect the trend of the added variable plot of $\mathbf{W}$. \par

We provide an example of an added variable plot. Recall our example of a simple linear response from chapter 1. That is, we have three independent normals $X_1,X_2,X_3\sim N(0,1)$ and a response defined by $Y=10X_1-5X_2+\varepsilon.$ We can plot the added variable plot of $X_1, X_2,$ and $X_3$. \par

As we can see from figure \ref{AVPlinex}, the AVP for $X_1$ is a positive line through the origin, the AVP for $X_2$ is a negative line through the origin, and the AVP for $X_3$ is a mass of points about the origin. This indicates that the conditional relationship of $X_1$ with $Y$ once we have taken into account the effects of $X_2$ and $X_3$ is positive and linear, while the conditional relationship of $X_2$ with $Y$ once we take into account the effects of $X_1$ and $X_3$ is negative and linear. On the other hand, there is no conditional relationship between $X_3$ and $Y$ once we have taken into account the effects of $X_1$ and $X_3$. Most the variability of $Y$ can be explained by $X_1$ and $X_2$ and once we take into account the effect of those two variables, the remaining portion of $Y$ is largely noise. \par


## Added Variable Plots in Random Forests

While added variable plots in the linear regression setting allows us to conditionally estimate the effect of the predictor on the response, the picture for more complex regression functions such as random forests is more complicated. In particular, if the relationship between the response and predictors is non-linear, or we are using a statistical learning method such as a random forest, then we do not have the nice linear algebra that under girds the interpretation of added variable plots of linear regression models. However, we can still construct added variable plots for random forests that are similar to the added variable plots for linear regression. \par

### Residual-Based Added Variable Plot

Suppose we are interested in the added variable effect of variable $X_k$. Let $X_{-k}$ denote the set of predictors excluding $X_k$. We could proceed as in the linear regression setting and fit $\hat{\theta}_{RF}(Y|X_{-k})$ which predicts the $Y$ given $X_{-k}$, and we would also fit $\hat{\theta}_{RF}(X_k|X_{-k})$ which predicts $X_k$ in terms of $X_{-k}$. We would then form the added variable plot of the predictor $X_k$ by plotting $$(X_k-\hat{\theta}_{RF}(X_k|X_{-k}),Y-\hat{\theta}_{RF}(Y|X_{-k})).$$ That is, we plot the part of $X_k$ unexplained by $X_{-k}$ against the part of the response $Y$ unexplained by $X_{-k}$ with respect to the random forest algorithm. We denote this method as a residual-based added variable plot. The heuristic of this plot is similar to the added variable plot in the linear regression setting. Namely, if we want to understand the conditional relationship of the predictor $X_k$ with respect to the response, then plotting the residuals as above provides the effect of $X_k$ on $Y$ once we have adjusted for the effect of $X_{-k}$ on $X_k$ and $Y$, respectively. \par

The output of the residual-based added variable plot for a predictor $X_k$ would depend on the relationship between $X_k$ and the response $Y$. If $X_k$ is uninformative to the response, then we would expect that $\hat{\theta}_{RF}(Y|X_{-k})$ would form a prediction for $Y$ that is similar to a prediction for $Y$ given by $\hat{\theta}_{RF}(Y|X)$. Hence we would expect that $Y-\hat{\theta}_{RF}(Y|X_{-k})$ to generally be close to zero. If on the other hand, $X_k$ is informative to the response, then we would expect that $\hat{\theta}_{RF}(Y|X_{-k})$ would form a worse prediction for $Y$ than that given by $\hat{\theta}_{RF}(Y|X)$, since we would be excluding $X_k$ as a splitting variable. Furthermore, we would be taking into account the amount of the response explainable by $X_{-k}$ using the random forest model, such that $Y-\hat{\theta}_{RF}(Y|X_{-k})$ would be the amount of the response unexplained by $X_{-k}$. The values of $X_k-\hat{\theta}_{RF}(X_k|X_{-k})$ will depend on the relationship between $X_k$ and $X_{-k}$. In general, the importance of subtracting $\hat{\theta}_{RF}(X_k|X_{-k})$ from $X_k$ is that this takes into account the part of $X_k$ that can be explained by $X_{-k}$ using the random forest model. This can be particularly useful if $X_k$ has a complex, dependent relationship with some of the predictors among the $X_{-k}$. However, if $X_k$ and $X_{-k}$ are independent, then $\hat{\theta}_{RF}(X_k|X_{-k})$ will likely be a noisy prediction of $X_k$. Hence, if $X_k$ is uninformative to the response, we would expect the added variable plot of $X_k$ to be de-trended mass of point centered about the origin. While if $X_k$ is informative, we would expect that the added variable plot of $X_k$ to have a non-zero, possibly non-linear trend depending on the conditional relationship of $X_k$ with the response $Y$. \par

```{r results = "asis", echo = FALSE, warning = FALSE, message = FALSE, fig.height = 4, fig.env = 'marginfigure', fig.cap = "\\label{AVPresex}Residual-based added variable plot for example dataset"}

res.avp.ex <- siml.ex.list[[7]]
res.ex.gg.list <- map(1:3, plot_add_var_excep, res.avp.ex[[2]])

do.call("grid.arrange", c(res.ex.gg.list, ncol = 2))

```

As we see in figure \ref{AVPresex}, the residual-based AVP of the example dataset indicates the conditional relationship of each respective predictor and the response, once we have taken into account the effect of other predictors. The residual-based AVP will tend to preserve the sign and magnitude of the relationship between each predictor and the response.  If a predictor has a non-linear relationship with the response, then the residual-based AVP will approximately capture the non-linear relationship. 

### Model-Based Added Variable Plot

An alternative method for added variable plots is proposed by @rendahl2008  for black box statistical learning methods such as random forests. In particular, an added variable plot we can form is to plot $Y-\mathbb{E}(Y|X_{-k})$ against $\mathbb{E}(Y|X)-\mathbb{E}(Y|X_{-k})$. That is, we can plot residuals of the model without the predictor $X_k$ against the difference in predictions between the full model and the model with $X_k$ removed. In the context of random forests, to obtain the added variable plot of the predictor $X_k$, we would plot $$(\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k}), Y-\hat{\theta}_{RF}(Y|X_{-k})).$$ We denote this method as a model-based added variable plot. \par


Depending on the relationship between the predictors and the response, there are several outputs we might expect from the added variable plots of the predictors in the random forest. If the predictor $X_k$ is simply noise, i.e., if a predictor is uninformative with respect to the response, then we expect that $\hat{\theta}_{RF}(Y|X)$ and $\hat{\theta}_{RF}(Y|X_{-k})$ to form similar predictions of $Y$ given $X=x$ and $X_{-k}=x_{-k}$, respectively. In this scenario then, suppose for a moment that $\hat{\theta}_{RF}(Y|X)$ and $\hat{\theta}_{RF}(Y|X_{-k})$ are consistent estimators of $\mathbb{E}(Y|X)$ and $\mathbb{E}(Y|X_{-k})$, respectively. Then by the Law of Total Expectation, we would expect that asymptotically $\mathbb{E}_X(\hat{\theta}_{RF}(Y|X))=Y$ and $\mathbb{E}_{X_{-k}}(\hat{\theta}_{RF}(Y|X_{-k}))=Y$. Hence in the limit, by linearity of expectation, $$\mathbb{E}(\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k}))=\mathbb{E}(\hat{\theta}_{RF}(Y|X))-\mathbb{E}(\hat{\theta}_{RF}(Y|X_{-k}))=Y-Y=0.$$ Similarly, $$\mathbb{E}(Y-\hat{\theta}_{RF}(Y|X_{-k}))=Y-Y=0.$$ Hence if a predictor $X_k$ is uninformative, provided we have grown adequately accurate forest ensembles to predict $Y$ given $X$ and $Y$ given $X_{-k}$, respectively, then we expect that the the added variable plot of $X_k$ to either be radially centered about the origin or to otherwise be arranged in a line with a slope and $y$-intercept of zero. \par

On the other hand, suppose that the predictor $X_k$ is informative with respect to the response. Then we would expect that when properly tuned the random forest ensemble $\hat{\theta}_{RF}(Y|X)$ would provide reasonably accurate predictions of $Y$. We would also expect that the forest ensemble $\hat{\theta}_{RF}(Y|X_{-k})$ would suffer in predictive performance due to the lack of information from $X_k$. Then in the limit, we would expect that $$(\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k})\neq 0 \text{ and } Y-\hat{\theta}_{RF}(Y|X_{-k}))\neq 0$$ when we evaluate $X=x$ and $X_{-k}=x_{-k}$ where $x,x_{-k}$ are training examples in the full dataset and the reduced dataset, respectively. Since the difference $\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k}$ and $Y-\hat{\theta}_{RF}(Y|X_{-k})$ is generally non-zero when we evaluate the random forest at a particular point in the feature space and these differences are precisely what we plot on the x and y-axis of the added variable plot, we would then expect the trend of the added variable plot of $X_k$ to be non-zero. In other words, in the case of informative predictors, we would expect there to be correlation between $\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k})$ and $Y-\hat{\theta}_{RF}(Y|X_{-k})$. The departure from a trend of approximately zero would depend on the relative informativeness of $X_k$. If $X_k$ is strongly informative then we would expect the difference between $Y$ and $\hat{\theta}_{RF}(Y|X_{-k})$ and the difference between $\hat{\theta}_{RF}(Y|X)$ and $\hat{\theta}_{RF}(Y|X_{-k})$ to often be large. If $X_k$ is weakly informative we would expect the two differences to often be of smaller magnitude than when $X_k$ is strongly informative. Hence visually the trend of the added variable plot of predictors used to grow a random forest ensemble offer a method of gauging the informativeness of different predictors. \par

```{r results = "asis", echo = FALSE, warning = FALSE, message = FALSE,  fig.env = 'marginfigure', fig.cap = "\\label{AVPmodex}Model-based added variable plot for example dataset"}

mod.avp.ex <- siml.ex.list[[6]]
mod.ex.gg.list <- map(1:3, plot_add_var_excep, mod.avp.ex[[2]])

do.call("grid.arrange", c(mod.ex.gg.list, ncol = 2))

```

As we see in figure \ref{AVPmodex}, the indicates the conditional relationship of each predictor with respect to the response, but does not, in general, preserve the sign of the relationship. In particular, we note that the relationship between variable 2 and the response $Y$ in the example dataset is negative and linear, but that the model-based AVP has a positive trend. This is as instead of plotting $X_k-\hat{\theta}_{RF}(X_k|X_{-k})$ on the x-axis as in the residual-based AVP, we plot $\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k})$, which will generally have a positive relationship with $Y-\hat{\theta}_{RF}(Y|X_{-k})$ on the y-axis of the added variable plot. \par 

### Discussion of Added Variable Plot

We note that in the above argument that we made an appeal to the random forest estimator $\hat{\theta}_{RF}(Y|X)$ being a consistent estimator of $\mathbb{E}(Y|X)$. As discussed earlier, the random forest algorithm as introduced and implemented by Breiman and collaborators has not yet been shown to be consistent. However, empirically the random forest algorithm often offers good predictions of the response, so we expect that the added variable plot as applied to the original random forest algorithm to have the properties discussed above. We also note that there are random forest variants such as Causal Forest as introduced by @wager2017 which have been shown to be consistent estimators of $\mathbb{E}(Y|X)$. Hence in such settings we expect that our discussion of added variable plots for forest ensembles to be fully valid. \par

One setting in which added variable plots for random forests are useful is in dealing with correlated predictors. As @strobl2008 notes, the random forest algorithm can have difficulty determining the relative importance of correlated predictors due to masking effects. Suppose $X_j$ and $X_k$ are correlated predictors with $X_j$ only weakly informative to the response. Then if both $X_j$ and $X_k$ are candidate splitting variables, $X_k$ could be chosen over $X_j$ as the splitting variable since $X_k$ may seem to be informative due to the correlation between $X_k$ and $X_j$. The better split would have been found over $X_j$, but the greedy nature of the CART algorithm means the algorithm does not look forward at possible splits further down the tree if $X_j$ is chosen over $X_k$. Note that the tree grown using $X_k$ at that particular node would likely have lower predictive performance than the tree grown using $X_j$ at that node. Hence WLOG suppose the subset $\{X_1,\ldots,X_m\}\subseteq \{X_1,\ldots,X_p\}$ of our predictors are correlated where $m\leq p$. If $X_j\in \{X_1,\ldots,X_m\}$ is not informative to the response, then we would expect $\hat{\theta}_{RF}(Y|X)$ and $\hat{\theta}_{RF}(Y|X_{-j})$ to provide similar predictions, while if $X_k\in \{X_1,\ldots,X_m\}$ is informative to the response, we would expect that $\hat{\theta}_{RF}(Y|X_{-k})$ will suffer a decrease in predictive performance in comparison to $\hat{\theta}_{RF}(Y|X)$. Then we would expect the added variable plot for $X_j$ to be a mass of points centered about the origin without a trend while the added variable plot for $X_k$ should have some sort of trend whose shape depends on the informativeness of $X_k$ and the predictive performance of $\hat{\theta}_{RF}(Y|X)$ and $\hat{\theta}_{RF}(Y|X_{-k})$. This is of course despite the fact that, depending on the correlation structure and relative informativeness of $X_k$, that $X_j$ may be chosen over $X_k$ when $X_j$ and $X_k$ are both candidate splitting variables in the tree growing process. The full random forest model $\hat{\theta}_{RF}(Y|X)$ may be biased in how the splitting variables are chosen due to the correlation structure of $\{X_1,\ldots,X_m\}$ as discussed in @strobl2008. However, absent the choice of $X_k$ in the random forest model $\hat{\theta}_{RF}(Y|X_{-k})$, the less informative split on $X_j$ has an increased probability of being chosen when growing the forest ensemble $\hat{\theta}_{RF}(Y|X_{-k})$ than in the full forest ensemble $\hat{\theta}_{RF}(Y|X)$. Hence the random forest model $\hat{\theta}_{RF}(Y|X_{-k})$ would have a decrease in predictive in performance due to the loss of information in the predictor $X_k$, but also due to the increased probability of irrelevant predictors being chosen as the splitting variables due to exclusion of $X_k$. \par  

## Added Variable Plot Importance 

In the previous section, we discussed the use of added variable plots for random forests including in settings where a subset of the predictors are correlated. Variable importance measures such as MDA variable importance and MDI variable importance have difficulties in dealing with correlated predictors. As discussed in the previous chapter, @strobl2008 proposed the conditional variable importance measure while @owens2017 proposed the Infforest variable importance measure as methods that try to account for the correlation structure of the predictors when measuring the importance of predictors in the forest ensemble. In this section we propose the Added Variable Plot Importance (AVPI) measure as an alternative variable importance measure to conditional variable importance and Infforest variable importance measures when accounting for correlated predictors. \par

To motivate the construction of AVPI, we return briefly to added variable plots in the linear regression setting. As mentioned earlier, when we construct an added variable plot for a predictor $\mathbf{W}$ in the linear regression model, the linear trend of the added variable plot for $\mathbf{W}$ corresponds to the least squares estimate of the linear effect $\alpha$ of $\mathbf{W}$ in the full model $\mathbf{Y}_1=\mathbf{X}\beta_1+\mathbf{W}\alpha+\mathbf{\varepsilon}_1$. Hence while we could simply visually inspect the trend of the added variable plot for $\mathbf{W}$, we could also run least squares regression on the added variable plot for $\mathbf{W}$ to obtain an estimate for $\alpha$ in the regression model $\mathbf{Y}_1$. The size and sign of the $\hat{\alpha}$ from running a regression model on the added variable plot of $\mathbf{W}$ then indicates the relative importance and effect of $\mathbf{W}$ on the response $\mathbf{Y}$. \par

In the random forest setting, we would like to have a similar procedure to determine the importance of the variable $X_k$ with respect to the response $Y$ once we have accounted for the effect of the other predictors. \par 

We have two options for plotting the added variable plot of $X_k$ using the random forest algorithm. We have a residuals based approach where we plot $$(X_k-\hat{\theta}_{RF}(X_k|X_{-k}),Y-\hat{\theta}_{RF}(Y|X_{-k})),$$ and we also have model based approach where we plot $$(\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k}), Y-\hat{\theta}_{RF}(Y|X_{-k})).$$ As discussed in the previous section, depending on the relative informativeness of the predictor $X_k$ there may or may not be a trend in the added variable plot for $X_k$ which we could then attempt to model. Let $W_k=Y-\hat{\theta}_{RF}(Y|X_{-k})$ and $U_k=\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k})$ if we are using the model-based AVP. If we are using the residual-based AVP, let $W_k=Y-\hat{\theta}_{RF}(Y|X_{-k})$ and $U_k=X_k-\hat{\theta}_{RF}(X_k|X_{-k})$. For the rest of the chapter, we assume that we are working with the model-based AVP. In particular, our discussion of AVPI for the model-based AVP should also hold for the residual-based AVP. \par 

We propose to train a bagged forest ensemble $\hat{\theta}_{BF}(W_k|U_k)$ and compute the MDA variable importance of $U_k$ in the bagged forest ensemble. We call the MDA variable importance of $U_k$ in the bagged forest ensemble $\hat{\theta}_{BF}(W_k|U_k)$ the added variable plot importance (AVPI) of $X_k$ and denote this quantity by $VI_{AVP}(X_k)$. Our choice of using a bagged forest to predict $W_k$ given $U_k$ is motivated by the fact that unlike in the linear regression setting, there is not a clear parametric model with which to predict $W_k$ given $U_k$. The bagged forest makes few assumptions on the form of the true regression function $W_k=f(U_k)+\varepsilon$ while also providing a metric in the form of MDA variable importance to assess the importance of $U_k$ in predicting $W_k$ once we have grown the ensemble $\hat{\theta}_{BF}(W_k|U_k)$. Furthermore, the AVPI of $X_k$ should reflect the relative importance of $X_k$ with respect to the response $Y$ given that the trend of the added variable plot, i.e. the degree to which $Y-\hat{\theta}_{RF}(Y|X_{-k})$ and $\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k})$ are correlated, visually indicates the informativeness of $X_k$.  

The purpose of the AVPI of $X_k$ is to provide a quantitative measure of the importance of the predictor $X_k$ with respect to $Y$ once we have taken into account the loss in predictive performance when $X_k$ is removed as a possible splitting variable. In particular, given that the added variable plots for random forests should reflect the informativeness of correlated predictors, the added variable plot importance for correlated predictors should be able to more accurately reflect the importance of correlated predictors with respect to the response than the MDA variable importance ran on the predictors in the full model $\hat{\theta}_{RF}(Y|X)$.   \par

  \begin{algorithm}
    \caption{Residual-Based Added Variable Plot Importance (AVPI)} \label{added variable importance}
      \begin{algorithmic}[1]
          \For{ $k=1,\ldots,p$ }
            \State Grow the random forest ensemble $\hat{\theta}_{RF}(Y|X_{-k})$ predicting $Y$ using the full set of predictors minus the predictor $X_k$.
            \State Grow the random forest ensemble $\hat{\theta}_{RF}(X_k|X_{-k})$ predicting $X_k$ using the set of predictors not the predictor $X_k$
            \State Compute $U_k=X_k-\hat{\theta}_{RF}(X_k|X_{-k})$ and $W_k=Y-\hat{\theta}_{RF}(Y|X_{-k})).$
            \State Grow the bagged forest ensemble $\hat{\theta}_{BF}(W_k|U_k)$ predicting $W_k$ using $U_k$. 
            \State Compute the added variable plot importance of $X_k$ to be the MDA variable importance of $U_k$: $VI_{AVP}(X_k)=VI_{MDA}(U_K)$.
          \EndFor
      \end{algorithmic}
  \end{algorithm}

\begin{algorithm}
    \caption{Model-Based Added Variable Plot Importance (AVPI)} \label{added variable importance}
      \begin{algorithmic}[1]
          \State Grow the random forest ensemble $\hat{\theta}_{RF}(Y|X)$ predicting $Y$ using the full set of predictors. 
          \For{ $k=1,\ldots,p$ }
            \State Grow the random forest ensemble $\hat{\theta}_{RF}(Y|X_{-k})$ predicting $Y$ using the full set of predictors minus the predictor $X_k$.
            \State Compute $U_k=\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k})$ and $W_k=Y-\hat{\theta}_{RF}(Y|X_{-k})).$
            \State Grow the bagged forest ensemble $\hat{\theta}_{BF}(W_k|U_k)$ predicting $W_k$ using $U_k$. 
            \State Compute the added variable plot importance of $X_k$ to be the MDA variable importance of $U_k$: $VI_{AVP}(X_k)=VI_{MDA}(U_K)$.
          \EndFor
      \end{algorithmic}
  \end{algorithm}
  
### Example of Added Variable Plot Importance

```{r results = "asis", echo = FALSE, warning = FALSE, message = FALSE, fig.height = 3, fig.env = 'marginfigure', fig.cap = "\\label{AVPIex}Residual and model based AVPI scores of example dataset"}


var_name <- sapply(1:3, FUN = function(i) paste0("V", as.character(i)))
x_axis_order <- sapply(1:3, FUN = function(i) paste0("V", as.character(4-i)))

mod.avp.ex.df <- data.frame(variable = var_name, AVPI = mod.avp.ex[[1]][,1])
res.avp.ex.df <- data.frame(variable = var_name, AVPI = res.avp.ex[[1]][,1])

mod.avp.ex.plot <- ggplot(mod.avp.ex.df, aes(x = variable, y = AVPI))+
  geom_bar(stat = 'identity')+
  coord_flip()+
  scale_x_discrete(limits = x_axis_order)+
  theme(text = element_text(size = 10), axis.title.y=element_blank())+
  labs(title = "Model-based AVPI")

res.avp.ex.plot <- ggplot(res.avp.ex.df, aes(x = variable, y = AVPI))+
  geom_bar(stat = 'identity')+
  coord_flip()+
  scale_x_discrete(limits = x_axis_order)+
  theme(text = element_text(size = 10), axis.title.y=element_blank())+
  labs(title = "Residual-based AVPI")

do.call("grid.arrange", c(list(res.avp.ex.plot, mod.avp.ex.plot), ncol = 2))


```

In figure \ref{AVPIex}, we present the residual-based and model-based AVPI, respectively, of the predictors in the example dataset. As expected, for both measures, variables 1 and 2 receive high AVPI scores, while variable 3 receives a low AVPI score.  

## Extensions of Added Variable Plot Importance 

Once we have obtained added variable plot importance values there are couple directions we can extend our framework. Once again suppose we have grown our full random forest ensemble $\hat{\theta}_{RF}(Y|X)$ and also the random forest ensemble $\hat{\theta}_{RF}(Y|X_{-k})$. Let $U_k=\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k})$ and $W_k=Y-\hat{\theta}_{RF}(Y|X_{-k})).$ \par

Once we have computed the AVPI of $X_k$, $VI_{AVP}(X_k)$, we can take a simulation approach to generating a null distribution for $VI_{AVP}(X_k)$. In particular, to generate a null distribution for the AVPI of $X_k$, permute $U_k$ to obtain $U_k^*$. Then grow the bagged forest ensemble $\hat{\theta}_{BF}(W_k|U_k^*)$ predicting $W_k$ using $U_k^*$, and compute the MDA variable importance of $U_k^*$ to obtain $VI_{AVP}^*(X_k)$ as the permuted AVPI for $X_k$. After having permuted $U_k$ and computed $VI_{AVP}^*(X_k)$ for enough iterations to generate a null distribution for $VI_{AVP}(X_k)$. We can then compute a two-sided p-value using the original $VI_{AVP}(X_k)$ as the observed test statistic. If we compute p-values for each predictor, then we could proceed to a hypothesis testing framework if we believe that the simulation to generate a null distribution for the AVPI of each predictor was successful. Of course depending on the number of predictors in the dataset and the relationship between the predictors, it may be necessary to control for multiple comparisons. In our opinion, as simulations in the next chapter will indicate, hypothesis testing using the AVPI is likely more sensitive to type-I errors than type-II errors, so an adjustment such as the Bonferroni correction may be appropriate. \par

We also note that while generating adequate null distributions of $VI_{AVP}(X_k)$ for each predictor $X_k$ is computationally expensive, each step of the process from growing the full ensemble $\hat{\theta}_{RF}(Y|X)$ to growing each $\hat{\theta}_{RF}(Y|X_{-k})$ to permuting $U_k$ and growing $\hat{\theta}_{BF}(W_k|U_k^*)$ to compute $VI_{AVP}^*(X_k)$ can be coded to run in parallel. So performance gains in generating good null distributions of $VI_{AVP}(X_k)$ for each predictor $X_k$ are easily attainable. \par


# Added Variable Plot Importance

## Introduction

In the previous chapter, we discussed theoretical properties and observed behavior of random forest variable importance measures. In particular, we discussed issues of bias present in the MDA variable importance measure. @strobl2008 and @owens2017 proposed the conditional variable importance measure and the Infforest variable importance measures, respectively, as methods of accounting for bias in variable selection among correlated predictors when measuring variable importance in a forest ensemble. Conditional variable importance and Infforest variable importance aim at measuring the conditional importance of a particular variable by conditionally permuting OOB data according to some criteria measured from the data, whether that be empirical correlation in the case of conditional variable importance, or the partitions induced by a particular tree inthe case of Infforest variable importance. In any case, we are primarily interested in the conditional importance of a predictor given the information provided by other predictors. While we would like to have a exact method of computing the conditional importance of a predictor in a random forest ensemble, in practice it is unclear how to construct an exact method given the complexity of the random forest ensemble. We can, however, estimate the effect of adding a predictor to the set of predictors that the forest ensemble can split on via a type of diagnostic plot called added variable plots. Added variable plots are a diagnostic plot arising from the linear regression which estimate the effect of adding a predictor variable to the model. In this chapter we propose a method of measuring the importance of each predictor in a random forest ensemble via a quantity we call the Added Variable Importance (AVI) that depends on the added variable plot of each predictor. The AVI attempts to estimate the effect of adding a predictor as a possible candidate in the splitting step of the random forest ensemble. To understand AVI, we regress from the random forest setting to the linear regression setting and explain how added variable plots work in the linear regression setting. 

## Added Variable Plots in Linear Regression 

Suppose that we have data $Z=\{(\mathbf{x}_i,Y_i)|i=1,\ldots,n\}$, where $\mathbf{x}_i$ is a covariate in the feature space, and that we fit a multiple linear regression model of the form $$\mathbf{Y}_1=\mathbf{X}\beta_1+\mathbf{W}\alpha+\mathbf{\varepsilon}_1$$ where $\mathbf{X}$ is a $n\times p$ matrix consisting of $p-1$ predictors, $\beta=(\beta_0,\beta_1,\ldots,\beta_p)^T$, $\mathbf{W}=(w_1,\ldots,w_n$ is a predictor, $\alpha$ is a scalar, and $\text{Var}(\mathbf{\varepsilon})=\sigma^2 I$. 

Say we are interested in estimating the effect of the predictor $\mathbf{W}$ in the regression model $\hat{\mathbf{Y}}_1$. We could first fit the models $\hat{\mathbf{Y}}_2=\mathbf{X}\beta_2+\mathbf{\varepsilon}_2$ and $\hat{\mathbf{W}}=\mathbf{X}\delta+\mathbf{\varepsilon}$, where $\beta_2$ and $\delta$ are coefficient vectors like $\beta_1$ and then plot the residuals $\mathbf{W}-\hat{\mathbf{W}}$ against the residuals $\mathbf{Y}-\hat{\mathbf{Y}}_2$. The plot we obtain by plotting $\mathbf{W}-\hat{\mathbf{W}}$ against $\mathbf{Y}-\hat{\mathbf{Y}}_2$ is called the added variable plot of $\mathbf{W}$ and measures the effect of $\mathbf{W}$ on $\mathbf{Y}$ once we have adjusted for the effect of $\mathbf{X}$ on $\mathbf{W}$ and $\mathbf{Y}$, respectively. In particular, the residuals that we plot for the added variable plot is the portion of $\mathbf{W}$ unexplained by $\mathbf{X}$ on the $x$-axis and the portion of $\mathbf{Y}$ unexplained by $\hat{\mathbf{Y}}_2=\mathbf{X}\beta_2+\mathbf{\varepsilon}$ on the $y$-axis. 

In the linear regression setting the added variable plot has some nice properties. In particular, let $\alpha_{AVP}$ denote the estimate of the slope from regressing $\mathbf{Y}-\hat{\mathbf{Y}}_2$ on $\mathbf{W}-\hat{\mathbf{W}}$. Then with some linear algebra it can be shown that $\alpha_{AVP}$ is equal to the least-squares estimate of $\alpha$ from $\hat{\mathbf{Y}}_1=\mathbf{X}\beta_1+\mathbf{W}\alpha+\mathbf{\varepsilon}_1$. For full details see @sheather2009 and @cook1982. Hence if we want to estimate the linear effect of the variable $\mathbf{W}$ on the linear regression $\hat{\mathbf{Y}}_1$, we can construct and visually inspect the trend of the added variable plot of $\mathbf{W}$. 

[Provide example of added variable plot]

## Added Variable Plots in Random Forests

While added variable plots in the linear regression setting allows us to conditionally estimate the effect of the predictor on the response, the picture for more complex regression functions such as random forests is more complicated. In particular, if the relationship between the response and predictors is non-linear, or we are using a statistical learning method such as a random forest, then we do not have the nice linear algebra that undergirds the interpretation of added variable plots of linear regression models. However, we can still construct a variant of added variable plots for random forests. 

Suppose we are interested in the added variable effect of variable $X_k$. Let $X_{-k}$ denote the set of predictors excluding $X_k$. As @rendahl2008 notes, in general, for black box statistical learning methods such as random forests the only appropriate added variable plot we can form is to plot $Y-\mathbb{E}(Y|X_{-k}$ against $\mathbb{E}(Y|X)-\mathbb{E}(Y|X_{-k})$. That is, we can plot residuals of the model without the predictor $X_k$ against the difference in predictions between the full model and the model with $X_k$ removed. In the context of random forests, to obtain the added variable plot of the predictor $X_k$, we would plot $$(\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k}), Y-\hat{\theta}_{RF}(Y|X_{-k})).$$ 

[Add example here]

Depending on the relationship between the predictors and the response, there are several outputs we might expect from the added variable plots of the predictors in the random forest. If the predictor $X_k$ is simply noise, i.e., if a predictor is uninformative with respect to the response, then we expect that $\hat{\theta}_{RF}(Y|X)$ and $\hat{\theta}_{RF}(Y|X_{-k})$ to form similar predictions of $Y$ given $X=x$ and $X_{-k}=x_{-k}$, respectively. In this scenario then, suppose for a moment that $\hat{\theta}_{RF}(Y|X)$ and $\hat{\theta}_{RF}(Y|X_{-k})$ are consistent estimators of $\mathbb{E}(Y|X)$ and $\mathbb{E}(Y|X_{-k})$, respectively. Then by the Law of Total Expectation, we would expect that asymptotically $\mathbb{E}(\hat{\theta}_{RF}(Y|X))=Y$ and $\mathbb{E}(\hat{\theta}_{RF}(Y|X_{-k}))=Y$. Hence in the limit, by linearity of expectation, $$\mathbb{E}(\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k}))=\mathbb{E}(\hat{\theta}_{RF}(Y|X))-\mathbb{E}(\hat{\theta}_{RF}(Y|X_{-k}))=Y-Y=0.$$ Similarly, $$\mathbb{E}(Y-\hat{\theta}_{RF}(Y|X_{-k}))=Y-Y=0.$$ Hence if a predictor $X_k$ is uninformative, provided we have grown adequetely accurate forest ensembles to predict $Y$ given $X$ and $Y$ given $X_{-k}$, respectively, then we expect that the the added variable plot of $X_k$ to be radially centered about the origin or to otherwise be arranged in a line with a slope and $y$-intercept of zero. 

On the other hand, suppose that the predictor $X_k$ is informative with respect to the response. Then we would expect that when properly tuned the random forest ensemble $\hat{\theta}_{RF}(Y|X)$ would provide reasonably accurate predictions of $Y$. We would also expect that the forest ensemble $\hat{\theta}_{RF}(Y|X_{-k})$ would suffer in predictive performance due to the lack of information from $X_k$. Then in the limit, we would expect that $$(\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k})\neq 0 \text{ and } Y-\hat{\theta}_{RF}(Y|X_{-k}))\neq 0.$$ That is, we would expect the trend of the added variable plot of $X_k$ to be non-zero. In other words, in the case of informative predictors, we would expect to be correlation between $\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k})$ and $Y-\hat{\theta}_{RF}(Y|X_{-k})$. The departure from a trend of approximately zero would depend on the relative informativeness of $X_k$. If $X_k$ is strongly informative then we would expect the difference between $Y$ and $\hat{\theta}_{RF}(Y|X_{-k})$ and the difference between $\hat{\theta}_{RF}(Y|X)$ and $\hat{\theta}_{RF}(Y|X_{-k})$ to often be large. If $X_k$ is weakly informative we would expect the two differnces to often be of smaller magnitude than when $X_k$ is strongly informative. Hence visually the trend of the added variable plot of predictors used to grow a random forest ensemble offer a method of gauging the informativeness of different predictors. We note that in the above argument that we made an appeal to the random forest estimator $\hat{\theta}_{RF}(Y|X)$ being a consistent estimator of $\mathbb{E}(Y|X)$. As discussed earlier, the random forest algorithm as introduced and implemented by Breiman and collaboraters has not yet been shown to be consistent. However, empirically the random forest algorithm often offers good predictions of the response, so we expect that the added variable plot as applied to the original random forest algorithm to have the properties discussed above. We also note that there are random forest variants such as Causal Forest as introduced by @wager2017 which have been shown to be consistent estimators of $\mathbb{E}(Y|X)$. Hence in such settings we expect that our discussion of added variable plots for forest ensembles to be fully valid. 

One setting in which added variable plots for random forests are useful is in dealing with correlated predictors. As @strobl2008 notes, the random forest algorithm can have difficulty determining the relative importance of correlated predictors due to masking effects. Suppose $X_j$ and $X_k$ are correlated predictors with $X_j$ only weakly informative to the response. Then if both $X_j$ and $X_k$ are candidate splitting variables, $X_k$ could be chosen over $X_j$ as the splitting variable since $X_k$ may seem to be informative due to the correlation between $X_k$ and $X_j$. The better split would have been found over $X_j$, but the greedy nature of the CART algorithm means the algorithm does not look forward at possible splits further down the tree if $X_j$ is chosen over $X_k$. Note that the tree grown using $X_k$ at that particular node would likely have lower predictive performance than the tree grown using $X_j$ at that node. Hence WLOG suppose the subset $\{X_1,\ldots,X_m\}\subseteq \{X_1,\ldots,X_p\}$ of our predictors are correlated where $m\leq p$. If $X_j\in \{X_1,\ldots,X_m\}$ is not informative to the response, then we would expect $\hat{\theta}_{RF}(Y|X)$ and $\hat{\theta}_{RF}(Y|X_{-j})$ to provide similar predictions, while if $X_k\in \{X_1,\ldots,X_m\}$ is informative to the response, we would expect that $\hat{\theta}_{RF}(Y|X_{-k})$ will suffer a decrease in predictive performance in comparison to $\hat{\theta}_{RF}(Y|X)$. Then we would expect the added variable plot for $X_j$ to be a mass of points centered about the origin without a trend while the added variable plot for $X_k$ should have some sort of trend whose shape depends on the informativeness of $X_k$ and the predictive performance of $\hat{\theta}_{RF}(Y|X)$ and $\hat{\theta}_{RF}(Y|X_{-k})$. This is of course despite the fact that, depending on the correlation structure and relative informativeness of $X_k$, that $X_j$ may be chosen over $X_k$ when $X_j$ and $X_k$ are both candidate splitting variables in the tree growing process. The full random forest model $\hat{\theta}_{RF}(Y|X)$ may be biased in how the splitting variables are chosen due to the correlation structure of $\{X_1,\ldots,X_m\}$ as discussed in @strobl2008. However, absent the choice of $X_k$ in the random forest model $\hat{\theta}_{RF}(Y|X_{-k})$, the less informative split on $X_j$ has an increased probability of being chosen when growing the forest ensemble $\hat{\theta}_{RF}(Y|X_{-k})$ than in the full forest ensemble $\hat{\theta}_{RF}(Y|X)$. Hence the random forest model $\hat{\theta}_{RF}(Y|X_{-k})$ would have a decrease in predictive in performance due to the loss of information in the predictor $X_k$, but also due to the increased probability of irrelevant predictors being chosen as the splitting variables due to exclusion of $X_k$.   

## Added Variable Plot Importance 

In the previous section, we discussed the use of added variable plots for random forests including in settings where a subset of the predictors are correlated. Variable importance measures such as MDA variable importance and MDI variable importance have difficulties in dealing with correlated predictors. As discussed in the previous chapter, @strobl2008 proposed the conditional variable importance measure while @owens2017 proposed the Infforest variable importance measure as methods that try to account for the correlation structure of the predictors when measuring the importance of predictors in the forest ensemble. In this section we propose the Added Variable Plot Importance (AVPI) measure as an alternative variable importance measure to conditional variable importance and Infforest variable importance measures in accounting for correlated predictors when measuring variable importance. 

To motivate the construction of AVPI, we return briefly to added variable plots in the linear regression setting. As mentioned earlier, when we construct an added variable plot for a predictor $\mathbf{W}$ in the linear regression model, the linear trend of the added variable plot for $\mathbf{W}$ corresponds to the least squares estimate of the linear effect $\alpha$ of $\mathbf{W}$ in the full model $\mathbf{Y}_1=\mathbf{X}\beta_1+\mathbf{W}\alpha+\mathbf{\varepsilon}_1$. Hence while we could simply visually inspect the trend of the added variable plot for $\mathbf{W}$, we could also run least squares regression on the added variable plot for $\mathbf{W}$ to obtain an estimate for $\alpha$ in the regression model $\mathbf{Y}_1$. The size and sign of the $\hat{\alpha}$ from running a regression model on the added variable plot of $\mathbf{W}$ then indicates the relative importance and effect of $\mathbf{W}$ on the response $\mathbf{Y}$. 

In the random forest setting, we would like to have a similar procedure to determine the importance of the variable $X_k$ with respect to the response $Y$ once we have accounted for the effect of the other predictors, but as noted before, we are restricted to plotting the added variable plot of $X_k$ using the random forest algorithm as $$(\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k}), Y-\hat{\theta}_{RF}(Y|X_{-k})).$$ As discussed in the previous section, depending on the relative informativeness of the predictor $X_k$ there may or may not be a trend in the added variable plot for $X_k$ which we could then attempt to model. Let $W_k=Y-\hat{\theta}_{RF}(Y|X_{-k})$ and $U_k=\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k})$. We propose to train a bagged forest ensemble $\hat{\theta}_{BF}(W_k|U_k)$ and computing the MDA variable importance of $U_k$ in the bagged forest ensemble. We call the MDA variable importance of $U_k$ in the bagged forest ensemble $\hat{\theta}_{BF}(W_k|U_k)$ the added variable plot importance (AVPI) of $X_k$ and denote this quantity by $VI_{AVP}(X_k)$. Our choice of using a bagged forest to predict $W_k$ given $U_k$ is motivated by the fact that unlike in the linear regression setting, there is not a clear parametric model with which to predict $W_k$ given $U_k$. The bagged forest makes few assumptions on the form of the true regression function $W_k=f(U_k)+\varepsilon$ while also providing a metric in the form of MDA variable importance to assess the importance of $U_k$ in predicting $W_k$ once we have grown the ensemble $\hat{\theta}_{BF}(W_k|U_k)$. Furthermore, the AVPI of $X_k$ should reflect the relative importance of $X_k$ with respect to the response $Y$ given that the trend of the added variable plot, i.e. the degree to which $Y-\hat{\theta}_{RF}(Y|X_{-k})$ and $\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k})$ are correlated, visually indicates the informativeness of $X_k$. 

The purpose of the AVPI of $X_k$ is to provide a quantitative measure of the importance of the predictor $X_k$ with respect to $Y$ once we have taken into account the loss in predictive performance when $X_k$ is removed as a possible splitting variable. In particular, given that the added variable plots for random forests should reflect the informativeness of correlated predictors, the added variable plot importance for correlated predictors should be able to more accurately reflect the importance of correlated predictors with respect to the response than the MDA variable importance ran on the predictors in the full model $\hat{\theta}_{RF}(Y|X)$.   

\begin{algorithm}
    \caption{Added Variable Plot Importance (AVPI)} \label{added variable importance}
      \begin{algorithmic}[1]
          \State Grow the random forest ensemble $\hat{\theta}_{RF}(Y|X)$ predicting $Y$ using the full set of predictors. 
          \For{ $k=1,\ldots,p$ }
            \State Grow the random forest ensemble $\hat{\theta}_{RF}(Y|X_{-k})$ predicting $Y$ using the full set of predictors minus the predictor $X_k$.
            \State Compute $U_k=\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k})$ and $W_k=Y-\hat{\theta}_{RF}(Y|X_{-k})).$
            \State Grow the bagged forest ensemble $\hat{\theta}_{BF}(W_k|U_k)$ predicting $W_k$ using $U_k$. 
            \State Compute the added variable plot importance of $X_k$ to be the MDA variable importance of $U_k$: $VI_{AVP}(X_k)=VI_{MDA}(U_K)$.
          \EndFor
      \end{algorithmic}
  \end{algorithm}

## Extensions of Added Variable Plot Importance 

Once we have obtained added variable plot importance values there are couple directions we can extend our framework. Once again suppose we have grown our full random forest ensemble $\hat{\theta}_{RF}(Y|X)$ and also the random forest ensemble $\hat{\theta}_{RF}(Y|X_{-k})$. Let $U_k=\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k})$ and $W_k=Y-\hat{\theta}_{RF}(Y|X_{-k})).$ 

Once we have computed the AVPI of $X_k$, $VI_{AVP}(X_k)$, we can take a simulation approach to generating a sampling distribution for $VI_{AVP}(X_k)$. In particular, to generate a sampling distribution for the AVPI of $X_k$, permute $U_k$ to obtain $U_k^*$. Then grow the bagged forest ensemble $\hat{\theta}_{BF}(W_k|U_k^*)$ predicting $W_k$ using $U_k^*$, and compute the MDA variable importance of $U_k^*$ to obtain $VI_{AVP}^*(X_k)$ as the permuted AVPI for $X_k$. After having permuted $U_k$ and computed $VI_{AVP}^*(X_k)$ for enough iterations to generate a sampling distribution for $VI_{AVP}(X_k)$. We can then compute a two-sided p-value using the original $VI_{AVP}(X_k)$ as the observed test statistic. If we compute p-values for each predictor, then we could proceed to a hypothesis testing framework if we believe that the simulation to generate a sampling distribution for the AVPI of each predictor was successful. Of course depending on the number of predictors in the dataset and the relationship between the predictors, it may be necessary to control for multiple comparisons. In our opinion, as simulations in the next chapter will indicate, hypothesis testing using the AVPI is likely more sensitive to type-I errors than type-II errors, so an adjustment such as the Bonferroni correction may be appropriate. 

We also note that while generating adequete sampling distributions of $VI_{AVP}(X_k)$ for each predictor $X_k$ is computationally expensive, each step of the process from growing the full ensemble $\hat{\theta}_{RF}(Y|X)$ to growing each $\hat{\theta}_{RF}(Y|X_{-k})$ to permuting $U_k$ and growing $\hat{\theta}_{BF}(W_k|U_k^*)$ to compute $VI_{AVP}^*(X_k)$ can be coded to run in parallel. So performance gains in generating good sampling distributions of $VI_{AVP}(X_k)$ for each predictor $X_k$ are easily attainable. 


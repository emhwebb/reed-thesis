# Simulation and Results

```{r, include = FALSE, warning = FALSE, message = FALSE, echo = FALSE}

library(randomForest)
library(tidyverse)
library(grid)
library(gridExtra)
library(mvtnorm)
library(knitr)
library(kableExtra)
library(highlight)

#first need to take our data frame and make the p+1 datasets that we apply the random forest to

#itr_col takes as input an integer and removes that column from the dataset
#note: should make itr_col more robust
itr_col <- function(i, data){
  select(data,-i)
}

#df_combs is a function which takes as input a dataframe and returns all iterations of the
#dataframe where one predictor has been removed. Output is a p+1 element list where each element
#is a dataframe with the ith variable removed. the last element of the list is the entire dataframe
df_combs <- function(data){
  p <- ncol(data)-1
  df.list <- map(1:p, itr_col, data = data)
  df.list[[p+1]] <- data
  df.list
}

#extract_rf_pred is a function which takes as input 
#a list of randomforest objects, an index value, 
#and the number of predictors in the model and 
#outputs the predicted values of the random forest in 
#a dataframe. extract_rf_pred is mainly for use in map_dfc, 
#which binds by column the predicted values in 
#data.list as a dataframe. Primarily for use in each_pred_rf
extract_rf_pred <- function(i, data.list, p){
 new.df <- as.data.frame(data.list[[i]]$predicted) 
 colnames(new.df) <- ifelse(i <= p, paste("PredWoVar", as.character(i), sep = ""),
                            "PredFullMod")
 new.df
}

#each_pred_rf is a function whose input is a list of dataframes 
#that come from output of df_combs and runs the 
#randomForest function on each dataframe using the map function. 
#Output is dataframe of predicted values along with actual value of Y as last column. 
#Second to last column is the predicted value of full model. 
each_pred_rf <- function(data.list, ntree1, replace = TRUE){
  p <- length(data.list)-1
  rf.list <- map(.x = data.list, function(x) 
    randomForest(Y~., data = x, ntree = ntree1, replace = replace, importance = TRUE))
  rf.df <- map_dfc(1:(p+1), extract_rf_pred, data.list = rf.list, p = p)
  Y <- data.list[[1]]$Y
  new.df <- cbind(rf.df, Y)
  imp <- importance(rf.list[[p+1]])
  list(new.df, imp)
}

#extract_add_var takes as input an index value and data frame and 
#outputs a dataframe of the basic added variable data frame where 
#x.res is the difference between predicted values of full model and 
#predicted values of model with out jth variable
#y.res is the residual of Y and predicted values of model without jth variable.
#For use with map in rf_add_var
extract_add_var <-function(i, df){
  PredFullMod <- as.name("PredFullMod")
  Y <- as.name("Y")
  V <- as.name(paste("PredWoVar", as.character(i), sep =""))
  x.res <- df[[PredFullMod]]-df[[V]]
  y.res <- df[[Y]]-df[[V]]
  new.df <- data.frame(x.res, y.res)
  colnames(new.df) <- c(paste("added.Var", as.character(i), sep = ""), "y.res")
  new.df
}

#rf_add_var takes as input the output of each_pred_rf and outputs a list of length p 
#in which each entry is a data frame corresponding
#to an added variable plot for the jth predictor in the 
#data set.
rf_add_var <- function(data.list){
  rf.df <- data.list[[1]]
  p <- length(rf.df)-2
  add.var.list <- map(1:p, extract_add_var, df = rf.df)
  add.var.list
}

#rf_add_var_imp takes as input a list of add_var df's from rf_add_var and runs
#a random forest on the y-residuals with x-residuals as input.
#output is a list of random forest objects. \
#Might change output to be just variable importance values. 
rf_add_var_imp <- function(data.list, ntree2, replace = TRUE){
  p <- length(data.list)
  rf.add.imp.list <- map(.x = data.list, function(x) 
    randomForest(y.res~., data = x, ntree = ntree2, replace = replace, importance = TRUE))
  rf.add.imp.list
}

rf_shallow_add_var_imp <- function(data.list, ntree2, replace = TRUE){
  p <- length(data.list)
  rf.add.imp.list <- map(.x = data.list, function(x) 
    randomForest(y.res~., data = x, ntree = ntree2, replace = replace, importance = TRUE, nodesize = 3))
  rf.add.imp.list
}

extract_imp <-function(i, data.list){
  new.df <- as.data.frame(t(importance(data.list[[i]])))
  new.df
}

extract_var_imp <- function(data.list){
  p <- length(data.list)
  new.df <- map_dfc(1:p, extract_imp, data.list = data.list)
  rownames(new.df) <- c("%IncMSE", "IncNodePurity")
  as.data.frame(t(new.df))
}


#Once rf has been run once on each added variable plot, we can try to assess
#importance via framework of p-values. In particular, we implement a 
#permutation test to obtain distribution of variable importance scores 
#rf_perm takes as input a list of added variable dataframes 
#(in particular output of rf_add_var
#and outputs a list of dataframes consisting of variable importance scores 
#obtained after permuting each added variable dataframe. 
#number of permutations is it input for rf_perm

#perm_rf takes as input a dataframe, permutes the dataframe
#runs a randomforest and returns the variable importance score
#variable importance score is MDA (mean decrease in accuracy) as a percentage change
perm_rf <-function(df, ntree3, replace){
  df.names <- colnames(df)
  df.mat <- as.matrix(df)
  x <- df.mat[sample(nrow(df.mat),replace = FALSE),1]
  y <- df.mat[,2]
  perm.df <- as.data.frame(cbind(x,y))
  colnames(perm.df) <- df.names
  perm.rf <- randomForest(y.res~., data = perm.df, replace = replace, importance = TRUE)
  importance(perm.rf, type = 1)
}

#perm_add_var takes as input an index value, the data.list, it the number of permutations, 
#ntree3 the number of trees to grow for each forest on the permuted data
perm_add_var <- function(i, data.list, it, ntree3, replace){
  df <- data.list[[i]]
  new.df <- as.data.frame(replicate(n = it, expr = perm_rf(df = df, 
                                                           ntree3 = ntree3, replace = replace)))
  colnames(new.df) <- paste("Var", as.character(i), "VI", sep = "")
  new.df
}

rf_perm_add_var <- function(data.list, it, ntree3, replace = TRUE){
  p <- length(data.list)
  new.list <- map(1:p, perm_add_var, data.list = data.list, 
      it = it, ntree3 = ntree3, replace = replace)
  new.list
}

#add_var_randomforest is a wrapper for the previous functions (exluding rf_perm_add_var)
add_var_randomforest <- function(data, ntree1, ntree2, replace = TRUE){
  #to get the copy of the data with one predictor removed
  df.list <- df_combs(data)
  #running initial randomFoest on each data frame
  rf.list <- each_pred_rf(df.list, ntree1 = ntree1, replace = replace)
  #tidying the data to compute added variable plots
  rf.add.var.list <- rf_add_var(rf.list)
  #running randomForests on each plot
  rf.add.imp.list <- rf_add_var_imp(rf.add.var.list, ntree2 = ntree2, replace = replace)
  #extracts variable importance values for each randomForest ran on added variable plots
  add.var.imp <- extract_var_imp(rf.add.imp.list)
  #output is list containing data frame of added variable plot variable importances, 
  #rf.add.var.list which are dataframe for added variable plots, and 
  #the variable importances from full model run in each_pred_rf
  list(add.var.imp, rf.add.var.list, rf.list[[2]])
}

#perm_add_var_randomforest is a wrapper for previous functions (including rf_perm_add_var)
perm_add_var_randomforest <- function(data, it, ntree1, ntree2, ntree3, replace = TRUE){
  df.list <- df_combs(data)
  #running initial randomFoest on each data frame
  rf.list <- each_pred_rf(df.list, ntree1 = ntree1, replace = replace)
  #tidying the data to compute added variable plots
  rf.add.var.list <- rf_add_var(rf.list)
  #running randomForests on each plot
  rf.add.imp.list <- rf_add_var_imp(rf.add.var.list, 
                                    ntree2 = ntree2, replace = replace)
  #extracts variable importance values for each randomForest ran on added variable plots
  add.var.imp <- extract_var_imp(rf.add.imp.list)
  #run permutations on each dataframe in rf.add.var.list to obtain 
  #distribution of importance values
  rf.perm.add.var.list <- rf_perm_add_var(data.list = rf.add.var.list, 
                                          it = it, ntree3 = ntree3, replace = replace)
  list(add.var.imp, rf.add.var.list, rf.list[[2]], rf.perm.add.var.list)
}

#input for rf_added_var_plot is output of 
#add_var_randomforest. Output is plot of added variable plots for 
#the random forest arranged in a grid.

plot_add_var <- function(i, df.list){
  df <- df.list[[i]]
  x.name <- colnames(df)[1]
  y.name <- colnames(df)[2]
  plot.obj <- ggplot(df, aes_string(x = x.name, y = y.name))+geom_point()
  plot.obj
}

rf_added_var_plot <- function(data.list){
    df.list <- data.list[[2]]
    p <- length(df.list)
    gg.list <- map(1:p, plot_add_var, df.list = df.list)
    nCol <- floor(sqrt(p))
    do.call("grid.arrange", c(gg.list, ncol = nCol))
}

#next make functions which plot distributions of the added variable importances and 
#adds in observed variable importance of added variable

plot_var_imp <- function(i, df.list, add.var.imp){
  df <- df.list[[i]]
  x.name <- colnames(df)
  obs.add.var <- add.var.imp[i,1]
  plot.obj <- ggplot(df, aes_string(x = x.name))+
    geom_histogram(bins = 50)+
    geom_vline(xintercept = obs.add.var, col = "Red", size = 1)
  plot.obj
}

plot_var_imp_excep <- function(i, df.list, add.var.imp){
  df <- df.list[[i]]
  x.name <- colnames(df)
  obs.add.var <- add.var.imp[i,1]
  plot.obj <- ggplot(df, aes_string(x = x.name))+
    geom_histogram(bins = 50)+
    geom_vline(xintercept = obs.add.var, col = "Red", size = 1)+
    theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        text = element_text(size = 10))+
    labs(x = paste(paste0("V", as.character(i)), 
                   "AVPI", sep = " "))
  plot.obj
}

rf_plot_var_imp <- function(data.list){
 df.list  <-data.list[[4]]
 add.var.imp <- data.list[[1]]
 p <- length(df.list)
 gg.list <- map(1:p, plot_var_imp, df.list = df.list, add.var.imp = add.var.imp)
 nCol <- floor(sqrt(p))
 do.call("grid.arrange", c(gg.list, ncol = nCol))
}

rf_plot_var_imp_excep <- function(data.list){
 df.list  <-data.list[[4]]
 add.var.imp <- data.list[[1]]
 p <- length(df.list)
 gg.list <- map(1:p, plot_var_imp_excep, df.list = df.list, add.var.imp = add.var.imp)
 nCol <- floor(sqrt(p))
 do.call("grid.arrange", c(gg.list, ncol = nCol))
}

#Next define a function which takes the output of perm_add_var_randomforest
#and computes p-values of variable importances.

perm_pval <- function(i, var.imp.df, var.imp.list){
  obs.var.imp <- var.imp.df[i,1]
  perm.var.imp <- var.imp.list[[i]]
  n.perm <- nrow(perm.var.imp)
  right.tail <- (sum(obs.var.imp <= perm.var.imp)+1)/(n.perm+1)
  left.tail <- (sum(perm.var.imp <= obs.var.imp)+1)/(n.perm+1)
  p.val <- ifelse(right.tail<=left.tail, 2*right.tail, 2*left.tail)
  p.val <- ifelse(1<p.val, 1, p.val)
  names(p.val) <- paste("p.val.Var.", as.character(i), sep = "")
  p.val
}

add_var_pval <- function(data.list){
  var.imp.df <- data.list[[1]]
  var.imp.list <- data.list[[4]]
  p <- nrow(var.imp.df)
  pval.df <- sapply(1:p, perm_pval, var.imp.df = var.imp.df, var.imp.list = var.imp.list)
  as.data.frame(pval.df)
}


```

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "testing"}
load(file = "data/MarChp4SimlResults.Rdata")
load(file = "data/avp_res_siml.Rdata")
list.siml.results <- list.siml.data.results[9:24]
res.siml.results <- avp_orig_list

siml1.wrep <- list.siml.results[[1]]
siml1.worep <- list.siml.results[[2]]
siml2.wrep <- list.siml.results[[3]]
siml2.worep <- list.siml.results[[4]]
siml3.wrep <- list.siml.results[[5]]
siml3.worep <- list.siml.results[[6]]
siml4.wrep <- list.siml.results[[7]]
siml4.worep <- list.siml.results[[8]]
siml5.wrep <- list.siml.results[[9]]
siml5.worep <- list.siml.results[[10]]
siml6.wrep <- list.siml.results[[11]]
siml6.worep <- list.siml.results[[12]]
siml7.wrep <- list.siml.results[[13]]
siml7.worep <- list.siml.results[[14]]
siml8.wrep <- list.siml.results[[15]]
siml8.worep <- list.siml.results[[16]]

list.wrep <- list(siml1.wrep,siml2.wrep, siml3.wrep,siml4.wrep,siml5.wrep,siml6.wrep,siml7.wrep, siml8.wrep)
list.worep <- list(siml1.worep, siml2.worep, siml3.worep, siml4.worep, siml5.worep, siml6.worep, siml7.worep, siml8.worep)

extract_table_col <- function(i, data.list,rf.type){
 data.col <- data.list[[i]][[rf.type]][,1] 
 names(data.col) <- paste("Simulation", as.character(i), sep = "")
 data.col
}

extract_matrix <- function(numvar, numsiml, data.list, rf.type){
  mat <- matrix(NA, ncol = numsiml, nrow = numvar)
  for(i in 1:numsiml){
    mat[,i] <- extract_table_col(i, data.list, rf.type) 
  }
  
  var_name <- sapply(1:numvar, FUN = function(i) paste("Variable", as.character(i), sep = " "))
  siml_name <- sapply(1:numsiml, FUN = function(i) paste("Simulation", as.character(i), sep = " "))
  
  colnames(mat) <- siml_name
  rownames(mat) <- var_name
  mat
}

extract_pval_matrix <- function(numvar, numsiml, data.list){
  mat <- matrix(NA, ncol = 8, nrow = 12)
  for(i in 1:8){
    mat[,i] <- unlist(add_var_pval(list.wrep[[i]]))
  }
  
  var_name <- sapply(1:numvar, FUN = function(i) paste("Variable", as.character(i), "P-Value", sep = " "))
  siml_name <- sapply(1:numsiml, FUN = function(i) paste("Simulation", as.character(i), sep = " "))
  
  colnames(mat) <- siml_name
  rownames(mat) <- var_name
  mat
}

v1 <- c(1,0.9,0.9,0.9)
v2 <- c(0.9,1,0.9,0.9)
v3 <- c(0.9,0.9,1,0.9)
v4 <- c(0.9,0.9,0.9,1)
w <- rep(0, times = 8)
u1 <- c(rep(0, times = 4), 1, rep(0, times = 7))
u2 <- c(rep(0, times = 5), 1, rep(0, times = 6))
u3 <- c(rep(0, times = 6), 1, rep(0, times = 5))
u4 <- c(rep(0, times = 7), 1, rep(0, times = 4))
u5 <- c(rep(0, times = 8), 1, rep(0, times = 3))
u6 <- c(rep(0, times = 9), 1, 0,0)
u7 <- c(rep(0, times = 10), 1, 0)
u8 <- c(rep(0, times = 11), 1)

sigma_name <- c("X1", "X2", "X3", "X4", "X5", "X6", "X7", "X8", "X9", "X10", "X11", "X12")

Sigma2 <- matrix(c(v1,w,v2,w,v3,w,v4,w,
                   u1,u2,u3,u4,u5,u6,u7,u8), nrow = 12, ncol = 12)

colnames(Sigma2) <- sigma_name
rownames(Sigma2) <- sigma_name



plot_AVPI <-function(i, df){
  AVPI_vec <- df[,i]
  var_name <- sapply(1:12, FUN = function(i) paste0("V", as.character(i)))
  x_axis_order <- sapply(1:12, FUN = function(i) paste0("V", as.character(13-i)))
  plot_df <-data.frame(variable = var_name, AVPI = AVPI_vec)
  
  plot_obj <- ggplot(plot_df, aes(x = variable, y = AVPI))+
  geom_bar(stat = 'identity')+
  coord_flip()+
  scale_x_discrete(limits = x_axis_order)+
  labs(y = paste("Simulation", as.character(i), sep = " "), x = "")+
  theme(text = element_text(size = 9))
  #theme(axis.title.y=element_blank())
  plot_obj
}

```

## Introduction 

In this chapter, we present simulation results of the random forest added variable plot and added variable plot importance methods. The design of our simulation follows closely to @strobl2008. In particular, we are interested in answering the following questions we layed out in Chapter 1: 
\begin{enumerate}
  \item Can AVPI handle correlated predictors in the data? 
  \item Can we extend AVPI to a hypothesis testing framework via permutation tests?
  \end{enumerate}
  \par

## Simulation Design 

We set up a simulation to test the added variable plot importance method on several different regression models. In particular, we were interested in how the added variable plot importance method would perform in situations where the relationship of the response to the predictors was linear, polynomial, and non-linear, and when there was and was not a correlation structure in the predictors. In addition, we were interested in comparing the effect of sampling with replacement versus sampling without replacement in the forest growing process as @strobl2008 indicates that there should be a difference. In particular, based on simulation results from @strobl2008, we expect that sampling without replacement to perform better than sampling with replacement. In the case of the independent random variables, we drew twelve predictors from a standard normal distribution. For the case of random variables with a correlation structure, we drew $X_1,\ldots,X_{12}$ from a multivariate normal distribution with mean 0 and covariance matrix $\Sigma$ where the first four variables $X_1,\ldots,X_4$ are block-correlated with a value of 0.9, and with the other eight variables independent. \par 

```{r kable, results="asis", echo = FALSE, warning = FALSE, message = FALSE}

kable(Sigma2, format = "latex", booktabs = TRUE,
      caption = "Covariance Matrix for Correlated Predictors") %>%
          kable_styling(latex_options = "scale_down")


```

We note that our covariance is the same covariance matrix used by @strobl2008 for their simulation study on the effect of correlation on variable selection in random forests in the regression setting. We tested the AVPI on four scenarios. The first was when the response was a linear combination of some of the predictors. The second scenario was when the response was a sum of polynomial terms of the predictors. The third scenario was when the response was the sum of two interaction terms. The fourth scenario was when the response was a sum of non-linear terms involving some of the predictors. In each scenario, we added gaussian noise $N(0, 0.05)$ to the response. We ran each scenario, in effect, four times. Once with independent predictors and sampling with replacement, then with independent predictors and sampling without replacement. We then ran the simulation on correlated predictors and sampling with replacement, then with correlated predictors and sampling without replacement. We illustrate the relationship between predictors and response for each of the four scenarios below. \par

\begin{figure}[h]
\begin{align}
\text{Scenario 1: } & Y=5X_1+5X_2+2X_3-5X_5-5X_6-2X_7+\varepsilon \nonumber \\
\text{Scenario 2: } & Y=5X_1^4+5X_2^3+6X_3^4+5X_5^3+\varepsilon \nonumber \\
\text{Scenario 3: } & Y=8X_1X_2+7X_5X_6+\varepsilon \nonumber \\
\text{Scenario 4: } & Y=3^{X_1}+2^{X_2}+4^{X_5}+\varepsilon \nonumber
\end{align}
\caption{Functional Form of the Response With Respect to the Predictors}
\label{ScenarioForms}
\end{figure}

In each scenario, variables 1 and 2 are always informative towards the predictor. In scenarios 1 and 2, variable 3 is informative to the predictor, while it is not informative for the scenario 3 and 4. In all scenarios, variable 4 is uninformative. In this way, we can see the effect of correlated predictors on AVPI. For variables 5 through 12, variable 5 is informative for each scenario, while variable 6 is informative in scenarios 1 and 2. Variable 7 is informative only in the scenario 1. Variables 8 through 12 are uninformative for each scenario. For reference, we illustrate which variables are informative and uninformative for each scenario. An $X$ indicates that a variable is informative for that scenario while a $-$ indicates that a variable is uninformative. \par

```{r results="asis", echo = FALSE, warning = FALSE, message = FALSE}

scenario_names <- c("Scenario 1", "Scenario 2", "Scenario 3", "Scenario 4")
var_names <- sapply(1:12, FUN = function(i) paste("Var", as.character(i), sep = ""))

Scenario1 <- c("X", "X", "X", "-", "X", "X", "X", "-", "-", "-", "-", "-")
Scenario2 <- c("X", "X", "X", "-", "X", "-", "-", "-", "-", "-", "-", "-")
Scenario3 <- c("X", "X", "-", "-", "X", "X", "-", "-", "-", "-", "-", "-")
Scenario4 <- c("X", "X", "-", "-", "X", "-", "-", "-", "-", "-", "-", "-")

var_inf_mat <- matrix(data = NA, nrow = 4, ncol = 12, dimnames = list(scenario_names, var_names))
var_inf_mat[1,] <- Scenario1 
var_inf_mat[2,] <- Scenario2
var_inf_mat[3,] <- Scenario3
var_inf_mat[4,] <- Scenario4

kable(var_inf_mat, format = "latex", booktabs = TRUE,
      caption = "Informative and Uninformative Variables For Each Scenario") %>%
    kable_styling(latex_options = "scale_down")

```

Each simulation corresponds to a particular scenario and type of predictors. We ran each scenario with first independent predictors then predictors with the correlation structure from above. In addition, we first ran each simulation using sampling with replacement in the random forest ensemble, then re-ran the simulation using sampling without replacement. In addition,  \par

```{r results="asis", echo = FALSE, warning = FALSE, message = FALSE, eval = FALSE}

scen_vec_names <- c("Scenario 1 with independent predictors",
"Scenario 1 with correlated predictors",
"Scenario 2 with independent predictors",
"Scenario 2 with correlated predictors",
"Scenario 3 with independent predictors",
"Scenario 3 with correlated predictors",
"Scenario 4 with independent predictors",
"Scenario 4 with correlated predictors")

siml_name <- sapply(1:8, FUN = function(i) paste("Simulation", as.character(i), sep = " "))

kable(matrix(scen_vec_names, nrow = 8, ncol = 1, dimnames = list(siml_name)),
      format = "latex", booktabs = TRUE, 
      caption = "Which Scenario and Set of Predictors Each Simulation Corresponds to") %>%
  kable_styling(latex_options = "scale_down")


```


\begin{figure}[h]
\begin{align}
\text{ Simulation 1: } & \text{ Scenario 1 with Independent Predictors} \nonumber \\
\text{ Simulation 2: } & \text{ Scenario 1 with Correlated Predictors} \nonumber \\
\text{ Simulation 3: } & \text{ Scenario 2 with Independent Predictors} \nonumber \\
\text{ Simulation 4: } & \text{ Scenario 2 with Correlated Predictors} \nonumber \\
\text{ Simulation 5: } & \text{ Scenario 3 with Independent Predictors} \nonumber \\
\text{ Simulation 6: } & \text{ Scenario 3 with Correlated Predictors} \nonumber \\
\text{ Simulation 7: } & \text{ Scenario 4 with Independent Predictors} \nonumber \\
\text{ Simulation 8: } & \text{ Scenario 4 with Correlated Predictors} \nonumber
\end{align}
\caption{Which Scenario and Set of Predictors Each Simulation Corresponds to}
\label{ScenarioPredictors}
\end{figure}

For each simulated dataset, we drew 2000 entries. We then ran the added variable plot importance scheme with the permutation scheme. For each stage of AVPI, we trained forest ensembles with 1000 trees, and to generate simulated null distributions of AVPI, we ran 1000 iterations of AVPI with permuted values for the x-axis of the added variable plot. In addition, the AVPI values we computed for the above simulations were for the model-based AVPI. We only ran the residuals-based AVPI on simulations 1 through 8 using sampling without replacement. \par

## Simulation Results

### Model-Based AVPI Simulation Results

```{r  echo = FALSE, warning = FALSE, message = FALSE, fig.height = 6, fig.env = 'marginfigure', fig.cap= "\\label{AVPIwrep}Model-based AVPI Simulation Results Using Sampling With Replacement"}

#addvar w replacement

add.var.wrep.siml <- as.data.frame(extract_matrix(numvar = 12, numsiml = 8, 
                                                  data.list = list.wrep, rf.type = 1))
avp.wrep.siml.plots <- map(1:8, plot_AVPI, add.var.wrep.siml)
avp.wrep.siml.plots[[1]] <- avp.wrep.siml.plots[[1]]+
  ggtitle("Independent Predictors")+
  labs(x = "Scenario 1")+
  theme(plot.title = element_text(hjust = 0.5))
avp.wrep.siml.plots[[2]] <- avp.wrep.siml.plots[[2]]+
  ggtitle("Correlated Predictors")+
  theme(plot.title = element_text(hjust = 0.5))
avp.wrep.siml.plots[[3]] <- avp.wrep.siml.plots[[3]]+
  labs(x = "Scenario 2")
avp.wrep.siml.plots[[5]] <- avp.wrep.siml.plots[[5]]+
  labs(x = "Scenario 3")
avp.wrep.siml.plots[[7]] <- avp.wrep.siml.plots[[7]]+
  labs(x = "Scenario 4")

do.call("grid.arrange", c(avp.wrep.siml.plots, ncol = 2, nrow = 4))

```

In Figure \ref{AVPIwrep}, we have the results of running the model-based added variable plot importances on simulated data sets when sampling with replacement. We first note that the AVPI is quite noisy in this case. For example, in the first simulation, the relevant variables were variables 1, 2, 3, 5, 6, and 7. The AVPI reflects the importance of these variables by assigning high AVPI scores to these variables. However, irrelevant variables also had AVPI scores between 70 and 95. Simulation 2 is a similar set-up as Simulation 1, except that the predictors have the correlation structure discussed above. We see that in this case that confounding between correlated predictors is still an issue as variable 3 is assigned a AVPI score of 76 -- which is lower than the scores assigned to variables 8 through 12. \par


We also extracted the MDA variable importance values for each simulation as a comparison to the AVPI values and summarize the results in Figure \ref{MDAwrep}. In particular, we note that in simulation 1, the MDA variable importance values and AVPI values more or less agree on which variables are more important to the response. Looking at the MDA variable importance values for simulation 2, we see that the full random forest model assigned MDA scores of around zero to variables 8 through 12, so we would be disinclined to believe in the AVPI scores for those variables. However, we see that there is some confounding between variables 1 through 4 due to the correlation structure betweeen those variables. In this case, the AVPI properly deflates the importance of variable 4 in comparison to variables 1, 2, and 3, which is what we would have expected. \par

```{r results="asis", echo = FALSE, warning = FALSE, message = FALSE, fig.height = 6, fig.env = 'marginfigure', fig.cap= "\\label{MDAwrep}MDA Simulation Results Using Sampling With Replacement"}

#mda w replacement

add.var.wrep.mda.siml <- as.data.frame(extract_matrix(numvar = 12, numsiml = 8, 
                             data.list = list.wrep, rf.type = 3))

mda.wrep.siml.plots <- map(1:8, plot_AVPI, add.var.wrep.mda.siml)

mda.wrep.siml.plots[[1]] <- mda.wrep.siml.plots[[1]]+
  ggtitle("Independent Predictors")+
  labs(x = "Scenario 1")+
  theme(plot.title = element_text(hjust = 0.5))
mda.wrep.siml.plots[[2]] <- mda.wrep.siml.plots[[2]]+
  ggtitle("Correlated Predictors")+
  theme(plot.title = element_text(hjust = 0.5))
mda.wrep.siml.plots[[3]] <- mda.wrep.siml.plots[[3]]+
  labs(x = "Scenario 2")
mda.wrep.siml.plots[[5]] <- mda.wrep.siml.plots[[5]]+
  labs(x = "Scenario 3")
mda.wrep.siml.plots[[7]] <- mda.wrep.siml.plots[[7]]+
  labs(x = "Scenario 4")

change_font_size <- function(i, data.list){
 data.list[[i]] <- data.list[[i]]+theme(text =  element_text(size = 9))
 data.list
}

#mda.wrep.siml.plots <- map(1:8, change_font_size, mda.wrep.siml.plots)

do.call("grid.arrange", c(mda.wrep.siml.plots, ncol = 2, nrow = 4))

```

In Figure \ref{AVPIwrep}, for simulation 3 , we see that AVPI matches with the MDA variable importance in matching which variables are informative. However, with simulation 4, while variable 2 has a higher score than variable 4 with AVPI and MDA variable importance, both variables have similar scores with respect to AVPI and MDA variable importance.  For simulation 5, the AVPI scores for variables 1, 2, 5, and 6 were highest although the uninformative predicotrs had relatively high AVPI scores. For simulation 6, which was the interaction terms with correlated predictors, the AVPI correctly downweights the importance of variables 3 and 4, while accounting for the importance of variables 1 and 2. With simulation 7 and 8, which was a highly non-linear response, there was small signal with respect to variable 2, so both AVPI and MDA importance computed low scores for variable 2. In particular, variable 2 in simulation 8 for AVPI has a particularly low score. However, variables 1 and 5 have high AVPI scores as expected. \par 

```{r results="asis", echo = FALSE, warning = FALSE, message = FALSE, fig.height = 6, fig.env = 'marginfigure', fig.cap= "\\label{AVPIworep}Model-based AVPI Simulation Results Using Sampling Without Replacement"}

#addvar wo replacement

add.var.worep.siml <- as.data.frame(extract_matrix(numvar = 12, numsiml = 8, 
                             data.list = list.worep, rf.type = 1))

avp.worep.siml.plots <- map(1:8, plot_AVPI, add.var.worep.siml)

avp.worep.siml.plots[[1]] <- avp.worep.siml.plots[[1]]+
  ggtitle("Independent Predictors")+
  labs(x = "Scenario 1")+
  theme(plot.title = element_text(hjust = 0.5))
avp.worep.siml.plots[[2]] <- avp.worep.siml.plots[[2]]+
  ggtitle("Correlated Predictors")+
  theme(plot.title = element_text(hjust = 0.5))
avp.worep.siml.plots[[3]] <- avp.worep.siml.plots[[3]]+
  labs(x = "Scenario 2")
avp.worep.siml.plots[[5]] <- avp.worep.siml.plots[[5]]+
  labs(x = "Scenario 3")
avp.worep.siml.plots[[7]] <- avp.worep.siml.plots[[7]]+
  labs(x = "Scenario 4")

do.call("grid.arrange", c(avp.worep.siml.plots, ncol = 2, nrow = 4))

```

Looking at the column of plots corresponding to independent predictors in Figure \ref{AVPIworep}, we see that in simulations 1, 3, and 5, the AVPI was able to correctly discern the importance of informative variables. However, with simulation 7, we see that in Figure \ref{MDAworep1}, that the MDA variable importance outperformed the AVPI variable importance and was able to find which variables were informative while AVPI had difficulty with this particular simulation. However, looking at the column of plots corresponding to correlated predictors in Figure \ref{AVPIworep}, we see that with the exception of simulation 8, that AVPI using sampling without replacement generally performed as well if not better than using MDA variable importance with sampling without replacement. We also note in comparing Figure \ref{AVPIworep} to Figure \ref{AVPIwrep}, that using AVPI with sampling without replacement appears to generally perform better than using AVPI with sampling with replacement with respect to correctly finding the informative variables among the correlated predictors. \par

```{r results="asis", echo = FALSE, warning = FALSE, message = FALSE, fig.height = 6, fig.env = 'marginfigure', fig.cap= "\\label{MDAworep1}MDA Simulation Results Using Sampling Without Replacement"}

#MDA wo replacement 

add.var.worep.mda.siml <- as.data.frame(extract_matrix(numvar = 12, numsiml = 8, 
                             data.list = list.worep, rf.type = 3))

mda.worep.siml.plots <- map(1:8, plot_AVPI, add.var.worep.mda.siml)

mda.worep.siml.plots[[1]] <- mda.worep.siml.plots[[1]]+
  ggtitle("Independent Predictors")+
  labs(x = "Scenario 1")+
  theme(plot.title = element_text(hjust = 0.5))
mda.worep.siml.plots[[2]] <- mda.worep.siml.plots[[2]]+
  ggtitle("Correlated Predictors")+
  theme(plot.title = element_text(hjust = 0.5))
mda.worep.siml.plots[[3]] <- mda.worep.siml.plots[[3]]+
  labs(x = "Scenario 2")
mda.worep.siml.plots[[5]] <- mda.worep.siml.plots[[5]]+
  labs(x = "Scenario 3")
mda.worep.siml.plots[[7]] <- mda.worep.siml.plots[[7]]+
  labs(x = "Scenario 4")

do.call("grid.arrange", c(mda.worep.siml.plots, ncol = 2, nrow = 4))

```

### Residuals-Based AVPI Simulation Results

Looking at Figure \ref{resAVPI}, we see that while the residuals-based AVPI can handle a linear response with both independent and correlated predictors, the residuals-based AVPI has difficulty with other types of responses. In particular, the residuals-based AVPI cannot handle a response defined by an interaction term (scenario 3) across both independent and correlated predictors.  The residuals-based AVPI also cannot handle correlated polynomial terms. With simulations 5 and 6, we expected to see that variables 1, 2, 5, and 6 to have the larges bars on the plot of AVPI values, however, what we see is that in both of these simulations, is that these variables have AVPI scores similar in value to uninformative variables. For simulation 8, we see that while variables 1 and 8 properly receive high AVPI scores, variable 2 received a similar AVPI score as variable 12, which was an uninformative. We do note, however, that relative to other correlated predictors in simulation 8, variables 1 and 2 receieved higher AVPI scores than variables 3 or 4, which is what we expected to see. \par

```{r results="asis", echo = FALSE, warning = FALSE, message = FALSE, fig.height = 6, fig.env = 'marginfigure', fig.cap= "\\label{resAVPI}Residuals-based AVPI Simulation Results Using Sampling Without Replacement"}

#avpi residuals simulation w/o replacement

avp.res.worep.siml <- as.data.frame(extract_matrix(numvar = 12, numsiml = 8, 
                             data.list = res.siml.results, rf.type = 1))

avp.res.worep.plots <- map(1:8, plot_AVPI, avp.res.worep.siml)

avp.res.worep.plots[[1]] <- avp.res.worep.plots[[1]]+
  ggtitle("Independent Predictors")+
  labs(x = "Scenario 1")+
  theme(plot.title = element_text(hjust = 0.5))
avp.res.worep.plots[[2]] <- avp.res.worep.plots[[2]]+
  ggtitle("Correlated Predictors")+
  theme(plot.title = element_text(hjust = 0.5))
avp.res.worep.plots[[3]] <- avp.res.worep.plots[[3]]+
  labs(x = "Scenario 2")
avp.res.worep.plots[[5]] <- avp.res.worep.plots[[5]]+
  labs(x = "Scenario 3")
avp.res.worep.plots[[7]] <- avp.res.worep.plots[[7]]+
  labs(x = "Scenario 4")

do.call("grid.arrange", c(avp.res.worep.plots, ncol = 2, nrow = 4))

```


```{r results="asis", echo = FALSE, warning = FALSE, message = FALSE, fig.height = 6, fig.env = 'marginfigure', fig.cap= "\\label{MDAwrep2}MDA Simulation Results Using Sampling Without Replacement on same data as in Figure \\ref{resAVPI}"}

#mda residuals simulation w/o replacement

mda.res.worep.siml <- as.data.frame(extract_matrix(numvar = 12, numsiml = 8, 
                             data.list = res.siml.results, rf.type = 3))

mda.res.worep.plots <- map(1:8, plot_AVPI, mda.res.worep.siml)

mda.res.worep.plots[[1]] <- mda.res.worep.plots[[1]]+
  ggtitle("Independent Predictors")+
  labs(x = "Scenario 1")+
  theme(plot.title = element_text(hjust = 0.5))
mda.res.worep.plots[[2]] <- mda.res.worep.plots[[2]]+
  ggtitle("Correlated Predictors")+
  theme(plot.title = element_text(hjust = 0.5))
mda.res.worep.plots[[3]] <- mda.res.worep.plots[[3]]+
  labs(x = "Scenario 2")
mda.res.worep.plots[[5]] <- mda.res.worep.plots[[5]]+
  labs(x = "Scenario 3")
mda.res.worep.plots[[7]] <- mda.res.worep.plots[[7]]+
  labs(x = "Scenario 4")

do.call("grid.arrange", c(mda.res.worep.plots, ncol = 2, nrow = 4))

```

### AVPI Simulation Results Discussion

Overall, simulation results indicate that AVPI can determine the informativeness of predictors with fairly strong signal, although in general MDA variable importance will offer similar results without the addition of uninformative variables receiving non-zero scores. In some situations with correlated predictors, the AVPI appears to find which of the correlated predictors are informative, so long as the signal in the informative predictor is relatively strong. This indicates that AVPI may be most effective as a tool employed alongside MDA variable importance when there is some correlation structure present in the predictors. In general, MDA variable importance will be able to evaluate the importance of independent predictors, and if there is some correlation structure that leads to unstable MDA variable importance values of the correlated predictors, then AVPI can be used on just the correlated predictors to determine which of the correlated predictors are important. We also note that based on our simulation results, the model-based AVPI appears to offer more stable answers to a larger class of response types than the residuals-based AVPI. Furthermore, using sampling without replacement with AVPI appears to offer better results than sampling with replacement, which is what we expected to see based on simulation results in @strobl2008. \par 

### Issues with AVPI and AVPI Null Distribution Simulation Results 

One issue with AVPI is the relative noisiness of the method. As observed in the simulations using sampling with replacement in Figure \ref{AVPIwrep}, uninformative variables can be assigned positive AVPI scores. Looking at results from re-running the simulation using sampling without replacement in \ref{AVPIworep}, we see that the same sort-of noisiness is also present in AVPI computed via random forests using sampling without replacement. Therefore the issue of noisiness for the AVPI method is not simply a matter of sampling with replacement versus sampling without replacement. The particular issue with the noisiness of the AVPI method is that it prevents us from readily employing permutation tests to determine importance of variables using AVPI as a test statistic in a hypothesis testing framework. In particular, looking at the tables of AVPI p-values for both sampling with replacement and sampling without replacement, we see that both tables are essentially identical, and that under a hypothesis testing framework, we would be committing type I errors across all simulations using a significance level of $\alpha = 0.05$. Furthermore, looking at the simulated null distribution of AVPI values, we see that the null distributions are generally quite similar. The simulated null distribution of AVPI values is generally symmetric with the lower end of the tails at around an importance score of 25. So any AVPI score above 30 would likely appear statistically significant using a permutation test with AVPI as the test statistic. \par

```{r results="asis", echo = FALSE, warning = FALSE, fig.env = 'marginfigure'}

kable(extract_pval_matrix(numvar = 12, numsiml = 8, data.list = list.wrep), 
      format = "latex", booktabs = TRUE,
      caption = "Added Variable Importance P-values for Sampling with Replacement") %>%
          kable_styling(latex_options = "scale_down")

```

```{r results="asis", echo = FALSE, warning = FALSE, fig.env = 'marginfigure'}

kable(extract_pval_matrix(numvar = 12, numsiml = 8, data.list = list.worep), 
      format = "latex", booktabs = TRUE,
      caption = "Added Variable Importance P-values for Sampling without Replacement") %>%
          kable_styling(latex_options = "scale_down")

```

```{r echo = FALSE, warning = FALSE, fig.env = 'marginfigure', fig.cap= "\\label{AVPIdistsiml}Simulated Null Distributions of AVPI for Each Predictor of Simulation 1 When Sampling Without Replacement"}

rf_plot_var_imp_excep(list.worep[[1]])

```

Thus while the AVPI may not be a stable test statistic for use in a hypothesis testing framework, our simulation results suggest that when used in conjunction with the MDA variable importance, the AVPI can be a useful tool for dealing with correlated predictors in a regression setting. \par

We conclude this chapter by returning to the questions we posed at the beginning of the chapter, namely, 
\begin{enumerate}
  \item Can AVPI handle correlated predictors in the data? 
  \item Can we extend AVPI to a hypothesis testing framework via permutation tests?
\end{enumerate}
With respect to the first question, we found that AVPI, in certain situations, can handle correlated predictors in the data better than the MDA variable importance measure. We also found that the model-based AVPI tends to provide more consistent scores than the residuals-based AVPI across a greater range of response types. On the other hand, the AVPI method is quite noisy. We were not able to determine the exact source of the noise, but we suspect that it has some relation with the correlation between $\hat{\theta}_{RF}(Y|X)-\hat{\theta}_{RF}(Y|X_{-k})$ and $Y-\hat{\theta}_{RF}(Y|X_{-k})$ in the model-based added variable plot. In particular, each of these forest ensembles tend to be highly correlated with each other and the response, such that when we run AVPI, we are capturing some of the effect of this correlation structure, thus leading to non-zero AVPI scores for non-informative variables. As will be discussed in the next chapter, one method we may try to alleviate this noisiness in AVPI is to define a joint added variable plot importance (JAVPI) measure, in which we try to determine in which subsets of predictors there is signal. \par 

With respect to the second question, due to the noisiness of AVPI, we find that the AVPI measure as it currently is, is an unreliable test statistic to compute a null distribution for via a permutation test. If the issue of noisiness could be sorted out, then AVPI would be a reliable test statistic, and we could enter into a hypothesis testing framework for random forests via simulating a null distribution for AVPI values. We also note that depending on the size of the dataset, computing adequete null distributions of the AVPI values of predictors can be computationally intensive.  \par

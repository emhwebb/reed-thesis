# Simulation and Results

```{r, include = FALSE, warning = FALSE, message = FALSE, echo = FALSE}

library(randomForest)
library(tidyverse)
library(grid)
library(gridExtra)
library(mvtnorm)
library(knitr)
library(kableExtra)

#first need to take our data frame and make the p+1 datasets that we apply the random forest to

#itr_col takes as input an integer and removes that column from the dataset
#note: should make itr_col more robust
itr_col <- function(i, data){
  select(data,-i)
}

#df_combs is a function which takes as input a dataframe and returns all iterations of the
#dataframe where one predictor has been removed. Output is a p+1 element list where each element
#is a dataframe with the ith variable removed. the last element of the list is the entire dataframe
df_combs <- function(data){
  p <- ncol(data)-1
  df.list <- map(1:p, itr_col, data = data)
  df.list[[p+1]] <- data
  df.list
}

#extract_rf_pred is a function which takes as input 
#a list of randomforest objects, an index value, 
#and the number of predictors in the model and 
#outputs the predicted values of the random forest in 
#a dataframe. extract_rf_pred is mainly for use in map_dfc, 
#which binds by column the predicted values in 
#data.list as a dataframe. Primarily for use in each_pred_rf
extract_rf_pred <- function(i, data.list, p){
 new.df <- as.data.frame(data.list[[i]]$predicted) 
 colnames(new.df) <- ifelse(i <= p, paste("PredWoVar", as.character(i), sep = ""),
                            "PredFullMod")
 new.df
}

#each_pred_rf is a function whose input is a list of dataframes 
#that come from output of df_combs and runs the 
#randomForest function on each dataframe using the map function. 
#Output is dataframe of predicted values along with actual value of Y as last column. 
#Second to last column is the predicted value of full model. 
each_pred_rf <- function(data.list, ntree1, replace = TRUE){
  p <- length(data.list)-1
  rf.list <- map(.x = data.list, function(x) 
    randomForest(Y~., data = x, ntree = ntree1, replace = replace, importance = TRUE))
  rf.df <- map_dfc(1:(p+1), extract_rf_pred, data.list = rf.list, p = p)
  Y <- data.list[[1]]$Y
  new.df <- cbind(rf.df, Y)
  imp <- importance(rf.list[[p+1]])
  list(new.df, imp)
}

#extract_add_var takes as input an index value and data frame and 
#outputs a dataframe of the basic added variable data frame where 
#x.res is the difference between predicted values of full model and 
#predicted values of model with out jth variable
#y.res is the residual of Y and predicted values of model without jth variable.
#For use with map in rf_add_var
extract_add_var <-function(i, df){
  PredFullMod <- as.name("PredFullMod")
  Y <- as.name("Y")
  V <- as.name(paste("PredWoVar", as.character(i), sep =""))
  x.res <- df[[PredFullMod]]-df[[V]]
  y.res <- df[[Y]]-df[[V]]
  new.df <- data.frame(x.res, y.res)
  colnames(new.df) <- c(paste("added.Var", as.character(i), sep = ""), "y.res")
  new.df
}

#rf_add_var takes as input the output of each_pred_rf and outputs a list of length p 
#in which each entry is a data frame corresponding
#to an added variable plot for the jth predictor in the 
#data set.
rf_add_var <- function(data.list){
  rf.df <- data.list[[1]]
  p <- length(rf.df)-2
  add.var.list <- map(1:p, extract_add_var, df = rf.df)
  add.var.list
}

#rf_add_var_imp takes as input a list of add_var df's from rf_add_var and runs
#a random forest on the y-residuals with x-residuals as input.
#output is a list of random forest objects. \
#Might change output to be just variable importance values. 
rf_add_var_imp <- function(data.list, ntree2, replace = TRUE){
  p <- length(data.list)
  rf.add.imp.list <- map(.x = data.list, function(x) 
    randomForest(y.res~., data = x, ntree = ntree2, replace = replace, importance = TRUE))
  rf.add.imp.list
}

rf_shallow_add_var_imp <- function(data.list, ntree2, replace = TRUE){
  p <- length(data.list)
  rf.add.imp.list <- map(.x = data.list, function(x) 
    randomForest(y.res~., data = x, ntree = ntree2, replace = replace, importance = TRUE, nodesize = 3))
  rf.add.imp.list
}

extract_imp <-function(i, data.list){
  new.df <- as.data.frame(t(importance(data.list[[i]])))
  new.df
}

extract_var_imp <- function(data.list){
  p <- length(data.list)
  new.df <- map_dfc(1:p, extract_imp, data.list = data.list)
  rownames(new.df) <- c("%IncMSE", "IncNodePurity")
  as.data.frame(t(new.df))
}


#Once rf has been run once on each added variable plot, we can try to assess
#importance via framework of p-values. In particular, we implement a 
#permutation test to obtain distribution of variable importance scores 
#rf_perm takes as input a list of added variable dataframes 
#(in particular output of rf_add_var
#and outputs a list of dataframes consisting of variable importance scores 
#obtained after permuting each added variable dataframe. 
#number of permutations is it input for rf_perm

#perm_rf takes as input a dataframe, permutes the dataframe
#runs a randomforest and returns the variable importance score
#variable importance score is MDA (mean decrease in accuracy) as a percentage change
perm_rf <-function(df, ntree3, replace){
  df.names <- colnames(df)
  df.mat <- as.matrix(df)
  x <- df.mat[sample(nrow(df.mat),replace = FALSE),1]
  y <- df.mat[,2]
  perm.df <- as.data.frame(cbind(x,y))
  colnames(perm.df) <- df.names
  perm.rf <- randomForest(y.res~., data = perm.df, replace = replace, importance = TRUE)
  importance(perm.rf, type = 1)
}

#perm_add_var takes as input an index value, the data.list, it the number of permutations, 
#ntree3 the number of trees to grow for each forest on the permuted data
perm_add_var <- function(i, data.list, it, ntree3, replace){
  df <- data.list[[i]]
  new.df <- as.data.frame(replicate(n = it, expr = perm_rf(df = df, 
                                                           ntree3 = ntree3, replace = replace)))
  colnames(new.df) <- paste("Var", as.character(i), "VI", sep = "")
  new.df
}

rf_perm_add_var <- function(data.list, it, ntree3, replace = TRUE){
  p <- length(data.list)
  new.list <- map(1:p, perm_add_var, data.list = data.list, 
      it = it, ntree3 = ntree3, replace = replace)
  new.list
}

#add_var_randomforest is a wrapper for the previous functions (exluding rf_perm_add_var)
add_var_randomforest <- function(data, ntree1, ntree2, replace = TRUE){
  #to get the copy of the data with one predictor removed
  df.list <- df_combs(data)
  #running initial randomFoest on each data frame
  rf.list <- each_pred_rf(df.list, ntree1 = ntree1, replace = replace)
  #tidying the data to compute added variable plots
  rf.add.var.list <- rf_add_var(rf.list)
  #running randomForests on each plot
  rf.add.imp.list <- rf_add_var_imp(rf.add.var.list, ntree2 = ntree2, replace = replace)
  #extracts variable importance values for each randomForest ran on added variable plots
  add.var.imp <- extract_var_imp(rf.add.imp.list)
  #output is list containing data frame of added variable plot variable importances, 
  #rf.add.var.list which are dataframe for added variable plots, and 
  #the variable importances from full model run in each_pred_rf
  list(add.var.imp, rf.add.var.list, rf.list[[2]])
}

#perm_add_var_randomforest is a wrapper for previous functions (including rf_perm_add_var)
perm_add_var_randomforest <- function(data, it, ntree1, ntree2, ntree3, replace = TRUE){
  df.list <- df_combs(data)
  #running initial randomFoest on each data frame
  rf.list <- each_pred_rf(df.list, ntree1 = ntree1, replace = replace)
  #tidying the data to compute added variable plots
  rf.add.var.list <- rf_add_var(rf.list)
  #running randomForests on each plot
  rf.add.imp.list <- rf_add_var_imp(rf.add.var.list, 
                                    ntree2 = ntree2, replace = replace)
  #extracts variable importance values for each randomForest ran on added variable plots
  add.var.imp <- extract_var_imp(rf.add.imp.list)
  #run permutations on each dataframe in rf.add.var.list to obtain 
  #distribution of importance values
  rf.perm.add.var.list <- rf_perm_add_var(data.list = rf.add.var.list, 
                                          it = it, ntree3 = ntree3, replace = replace)
  list(add.var.imp, rf.add.var.list, rf.list[[2]], rf.perm.add.var.list)
}

#input for rf_added_var_plot is output of 
#add_var_randomforest. Output is plot of added variable plots for 
#the random forest arranged in a grid.

plot_add_var <- function(i, df.list){
  df <- df.list[[i]]
  x.name <- colnames(df)[1]
  y.name <- colnames(df)[2]
  plot.obj <- ggplot(df, aes_string(x = x.name, y = y.name))+geom_point()
  plot.obj
}

rf_added_var_plot <- function(data.list){
    df.list <- data.list[[2]]
    p <- length(df.list)
    gg.list <- map(1:p, plot_add_var, df.list = df.list)
    nCol <- floor(sqrt(p))
    do.call("grid.arrange", c(gg.list, ncol = nCol))
}

#next make functions which plot distributions of the added variable importances and 
#adds in observed variable importance of added variable

plot_var_imp <- function(i, df.list, add.var.imp){
  df <- df.list[[i]]
  x.name <- colnames(df)
  obs.add.var <- add.var.imp[i,1]
  plot.obj <- ggplot(df, aes_string(x = x.name))+
    geom_histogram(bins = 50)+
    geom_vline(xintercept = obs.add.var, col = "Red", size = 1)
  plot.obj
}

rf_plot_var_imp <- function(data.list){
 df.list  <-data.list[[4]]
 add.var.imp <- data.list[[1]]
 p <- length(df.list)
 gg.list <- map(1:p, plot_var_imp, df.list = df.list, add.var.imp = add.var.imp)
 nCol <- floor(sqrt(p))
 do.call("grid.arrange", c(gg.list, ncol = nCol))
}

#Next define a function which takes the output of perm_add_var_randomforest
#and computes p-values of variable importances.

perm_pval <- function(i, var.imp.df, var.imp.list){
  obs.var.imp <- var.imp.df[i,1]
  perm.var.imp <- var.imp.list[[i]]
  n.perm <- nrow(perm.var.imp)
  right.tail <- (sum(obs.var.imp <= perm.var.imp)+1)/(n.perm+1)
  left.tail <- (sum(perm.var.imp <= obs.var.imp)+1)/(n.perm+1)
  p.val <- ifelse(right.tail<=left.tail, 2*right.tail, 2*left.tail)
  p.val <- ifelse(1<p.val, 1, p.val)
  names(p.val) <- paste("p.val.Var.", as.character(i), sep = "")
  p.val
}

add_var_pval <- function(data.list){
  var.imp.df <- data.list[[1]]
  var.imp.list <- data.list[[4]]
  p <- nrow(var.imp.df)
  pval.df <- sapply(1:p, perm_pval, var.imp.df = var.imp.df, var.imp.list = var.imp.list)
  as.data.frame(pval.df)
}


```

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "testing"}
load(file = "data/MarChp4SimlResults.Rdata")
list.siml.results <- list.siml.data.results[9:24]

siml1.wrep <- list.siml.results[[1]]
siml1.worep <- list.siml.results[[2]]
siml2.wrep <- list.siml.results[[3]]
siml2.worep <- list.siml.results[[4]]
siml3.wrep <- list.siml.results[[5]]
siml3.worep <- list.siml.results[[6]]
siml4.wrep <- list.siml.results[[7]]
siml4.worep <- list.siml.results[[8]]
siml5.wrep <- list.siml.results[[9]]
siml5.worep <- list.siml.results[[10]]
siml6.wrep <- list.siml.results[[11]]
siml6.worep <- list.siml.results[[12]]
siml7.wrep <- list.siml.results[[13]]
siml7.worep <- list.siml.results[[14]]
siml8.wrep <- list.siml.results[[15]]
siml8.worep <- list.siml.results[[16]]

list.wrep <- list(siml1.wrep,siml2.wrep, siml3.wrep,siml4.wrep,siml5.wrep,siml6.wrep,siml7.wrep, siml8.wrep)
list.worep <- list(siml1.worep, siml2.worep, siml3.worep, siml4.worep, siml5.worep, siml6.worep, siml7.worep, siml8.worep)

extract_table_col <- function(i, data.list,rf.type){
 data.col <- data.list[[i]][[rf.type]][,1] 
 names(data.col) <- paste("Simulation", as.character(i), sep = "")
 data.col
}

extract_matrix <- function(numvar, numsiml, data.list, rf.type){
  mat <- matrix(NA, ncol = numsiml, nrow = numvar)
  for(i in 1:numsiml){
    mat[,i] <- extract_table_col(i, data.list, rf.type) 
  }
  
  var_name <- sapply(1:numvar, FUN = function(i) paste("Variable", as.character(i), sep = " "))
  siml_name <- sapply(1:numsiml, FUN = function(i) paste("Simulation", as.character(i), sep = " "))
  
  colnames(mat) <- siml_name
  rownames(mat) <- var_name
  mat
}

extract_pval_matrix <- function(numvar, numsiml, data.list){
  mat <- matrix(NA, ncol = 8, nrow = 12)
  for(i in 1:8){
    mat[,i] <- unlist(add_var_pval(list.wrep[[i]]))
  }
  
  var_name <- sapply(1:numvar, FUN = function(i) paste("Variable", as.character(i), "P-Value", sep = " "))
  siml_name <- sapply(1:numsiml, FUN = function(i) paste("Simulation", as.character(i), sep = " "))
  
  colnames(mat) <- siml_name
  rownames(mat) <- var_name
  mat
}

v1 <- c(1,0.9,0.9,0.9)
v2 <- c(0.9,1,0.9,0.9)
v3 <- c(0.9,0.9,1,0.9)
v4 <- c(0.9,0.9,0.9,1)
w <- rep(0, times = 8)
u1 <- c(rep(0, times = 4), 1, rep(0, times = 7))
u2 <- c(rep(0, times = 5), 1, rep(0, times = 6))
u3 <- c(rep(0, times = 6), 1, rep(0, times = 5))
u4 <- c(rep(0, times = 7), 1, rep(0, times = 4))
u5 <- c(rep(0, times = 8), 1, rep(0, times = 3))
u6 <- c(rep(0, times = 9), 1, 0,0)
u7 <- c(rep(0, times = 10), 1, 0)
u8 <- c(rep(0, times = 11), 1)

sigma_name <- c("X1", "X2", "X3", "X4", "X5", "X6", "X7", "X8", "X9", "X10", "X11", "X12")

Sigma2 <- matrix(c(v1,w,v2,w,v3,w,v4,w,
                   u1,u2,u3,u4,u5,u6,u7,u8), nrow = 12, ncol = 12)

colnames(Sigma2) <- sigma_name
rownames(Sigma2) <- sigma_name

```

## Introduction 

In this chapter, we present simulation results of the random forest added variable plot and added variable plot importance methods. The design of our simulation follows closely to @strobl2008. \par

## Simulation Design 

We set up a simulation to test the added variable plot importance method on several different regression models. In particular, we were interested in how the added variable plot importance method would perform in situations where the relationship of the response to the predictors was linear, polynomial, and non-linear, and when there was and was not a correlation structure in the predictor. In addition, we were interested in comparing the effect of sampling with replacement versus sampling without replacement in the forest growing process. In the case of the independent random variables, we drew twelve predictors from a standard normal distribution. For the case of random variables with a correlation structure, we drew $X_1,\ldots,X_{12}$ from a multivariate normal distribution with mean 0 and covariance matrix $\Sigma$ where the first four variables $X_1,\ldots,X_4$ are block-correlated with a value of 0.9, and with the other eight variables independent. \par 

```{r kable, results="asis", echo = FALSE, warning = FALSE, message = FALSE}

kable(Sigma2, format = "latex", booktabs = TRUE,
      caption = "Covariance Matrix for Correlated Predictors") %>%
          kable_styling(latex_options = "scale_down")


```

We note that our covariance is the same covariance matrix used by @strobl2008 for their simulation study on the effect of correlation on variable selection in random forests in the regression setting. We tested the AVPI on four scenario. The first was when the response was a linear combination of some of the predictors. The second scenario was when the response was a sum of polynomial terms of the predictors. The third scenario was when the response was the sum of two interaction terms. The fourth scenario was when the response was a sum of non-linear terms involving some of the predictors. In each scenario, we added gaussian noise $N(0, 0.05)$ to the response. We ran each scenario, in effect, four times. Once with independent predictors and sampling with replacement, then with independent predictors and sampling without replacement. We then ran the simulation on correlated predictors and sampling with replacement, then with correlated predictors and sampling without replacement. We illustrate the relationship between predictors and response for each of the four scenarios below. \par

```{r results="asis", echo = FALSE, warning = FALSE, message = FALSE}

resp_pred_vec <- c("Y=5*X1+5*X2+2*X3-5*X5-5*X6-2*V7+err", "Y = 5*X1^4+5*X2^3+6*X3^4+5*X5^3+err",
  "Y=8*X1*X2+7*X5*X6+err", "Y = 3^X1+2^X2+4^X5+err")
scenario_names <- c("Scenario 1", "Scenario 2", "Scenario 3", "Scenario 4")
relationship_bw_pred_names <- "Relationship Between Predictors and Response"  

kable(matrix(resp_pred_vec, nrow = 4, ncol = 1, 
       dimnames = list(scenario_names, relationship_bw_pred_names)),
      format = "latex", booktabs = TRUE) %>%
    kable_styling(latex_options = "scale_down")

```

In each scenario, variables 1 and 2 are always informative towards the predictor. In scenarios 1 and 2, variable 3 is informative to the predictor, while it is not informative for the scenario 3 and 4. In all scenarios, variable 4 is uninformative. In this way, we can see the effect of correlated predictors on AVPI. For variables 5 through 12, variable 5 is informative for each scenario, while variable 6 is informative in scenarios 1 and 2. Variable 7 is informative only in the scenario 1. Variables 8 through 12 are uninformative for each scenario. For reference, we illustrate which variables are informative and uninformative for each scenario. An $X$ indicates that a variable is informative for that scenario while a $-$ indicates that a variable is uninformative. \par

```{r results="asis", echo = FALSE, warning = FALSE, message = FALSE}


var_names <- sapply(1:12, FUN = function(i) paste("Var", as.character(i), sep = ""))

Scenario1 <- c("X", "X", "X", "-", "X", "X", "X", "-", "-", "-", "-", "-")
Scenario2 <- c("X", "X", "X", "-", "X", "-", "-", "-", "-", "-", "-", "-")
Scenario3 <- c("X", "X", "-", "-", "X", "X", "-", "-", "-", "-", "-", "-")
Scenario4 <- c("X", "X", "-", "-", "X", "-", "-", "-", "-", "-", "-", "-")

var_inf_mat <- matrix(data = NA, nrow = 4, ncol = 12, dimnames = list(scenario_names, var_names))
var_inf_mat[1,] <- Scenario1 
var_inf_mat[2,] <- Scenario2
var_inf_mat[3,] <- Scenario3
var_inf_mat[4,] <- Scenario4

kable(var_inf_mat, format = "latex", booktabs = TRUE,
      caption = "Informative and Uninformative Variables For Each Scenario") %>%
    kable_styling(latex_options = "scale_down")

```

Each simulation corresponds to a particular scenario and type of predictors. We ran each scenario with first independent predictors then predictors with the correlation structure from above. In addition, we first ran each simulation using sampling with replacement in the random forest ensemble, then re-ran the simulation using sampling without replacement. \par

```{r results="asis", echo = FALSE, warning = FALSE, message = FALSE}

scen_vec_names <- c("Scenario 1 with independent predictors",
"Scenario 1 with correlated predictors",
"Scenario 2 with independent predictors",
"Scenario 2 with correlated predictors",
"Scenario 3 with independent predictors",
"Scenario 3 with correlated predictors",
"Scenario 4 with independent predictors",
"Scenario 4 with correlated predictors")

siml_name <- sapply(1:8, FUN = function(i) paste("Simulation", as.character(i), sep = " "))

kable(matrix(scen_vec_names, nrow = 8, ncol = 1, dimnames = list(siml_name)),
      format = "latex", booktabs = TRUE, 
      caption = "Which Scenario and Set of Predictors Each Simulation Corresponds to") %>%
  kable_styling(latex_options = "scale_down")


```

For each simulated dataset, we drew 2000 entries. We then ran the added variable plot importance scheme with the permutation scheme. For each stage of AVPI, we trained forest ensembles with 1000 trees, and to generate simulated null distributions of AVPI, we ran 1000 iterations of AVPI with permuted values for the x-axis of the added variable plot. \par


## Simulation Results

In the table below, we have the results of running the added variable plot importances on simulated data sets when sampling with replacement. We first note that the AVPI is quite noisy in this case. For example, in the first simulation, the relevant variables were variables 1, 2, 3, 5, 6, and 7. The AVPI reflects the importance of these variables by assigning high AVPI scores to these variables. However, irrelevant variables also had AVPI scores between 70 and 95. Simulation 2 is a similar set-up as Simulation 1, except that the predictors have the correlation structure discussed above. We see that in this case that confounding between correlated predictors is still an issue as variable 3 is assigned a AVPI score of 76 -- which is lower than the scores assigned to variables 8 through 12. We also extracted the MDA variable importance values for each simulation as a comparison to the AVPI values. In particular, we note that in simulation 1, the MDA variable importance values and AVPI values more or less agree on which variables are more important to the response. Looking at the MDA variable importance values for simulation 2, we see that the full random forest model assigned MDA scores of around zero to variables 8 through 12, so we would be disinclined to believe in the AVPI scores for those variables. However, we see that there is some confounding between variables 1 through 4 due to the correlation structure betweeen those variables. In this case, the AVPI properly deflates the importance of variable 4 in comparison to variables 1, 2, and 3, which is what we would have expected. \par


```{r results="asis", echo = FALSE, warning = FALSE, message = FALSE}

#addvar w replacement

kable(extract_matrix(numvar = 12, numsiml = 8, data.list = list.wrep, rf.type = 1),
      format = "latex", booktabs = TRUE,
      caption = "Simulation Results for Added Variable Plot Importances for Sampling with Replacement") %>%
          kable_styling(latex_options = "scale_down")

```

```{r results="asis", echo = FALSE, warning = FALSE, message = FALSE}

#mda w replacement

kable(extract_matrix(numvar = 12, numsiml = 8, data.list = list.wrep, rf.type = 3),
      format = "latex", booktabs = TRUE,
      caption = "Simulation Results for MDA Variable Importances for Sampling with Replacement") %>%
          kable_styling(latex_options = "scale_down")

```

For simulation 3 we see that AVPI matches with the MDA variable importance in matching which variables are informative. However, with simulation 4, while variable 2 has a higher score than variable 4 with AVPI and MDA variable importance, both variables have similar scores with respect to AVPI and MDA variable importance.  For simulation 5, the AVPI scores for variables 1, 2, 5, and 6 were highest although the uninformative predicotrs had relatively high AVPI scores. For simulation 6, which was the interaction terms with correlated predictors, the AVPI correctly downweights the importance of variables 3 and 4, while accounting for the importance of variables 1 and 2. With simulation 7 and 8, which was a highly non-linear response, there was small signal with respect to variable 2, so both AVPI and MDA importance computed low scores for variable 2. In particular, variable 2 in simulation 8 for AVPI has a particularly low score. However, for variables 1 and 5 have high AVPI scores as expected. \par 

```{r results="asis", echo = FALSE, warning = FALSE, message = FALSE}

#addvar wo replacement

kable(extract_matrix(numvar = 12, numsiml = 8, data.list = list.worep, rf.type = 1),
      format = "latex", booktabs = TRUE,
      caption = "Simulation Results for Added Variable Plot Importances for Sampling without Replacement") %>%
          kable_styling(latex_options = "scale_down")

```

```{r results="asis", echo = FALSE, warning = FALSE, message = FALSE}

#MDA wo replacement 

kable(extract_matrix(numvar = 12, numsiml = 8, data.list = list.worep, rf.type = 3),
      format = "latex", booktabs = TRUE,
      caption = "Simulation Results for MDA Variable Importances for Sampling without Replacement") %>%
          kable_styling(latex_options = "scale_down")

```

Overall, simulation results indicate that AVPI can determine the informativeness of predictors with fairly strong signal, although in general MDA variable importance will offer similar results without the addition of uninformative variables receiving non-zero scores. In some situations with correlated predictors, the AVPI appears to find which of the correlated predictors are informative, so long as the signal in the informative predictor is relatively strong. This indicates that AVPI may be most effective as a tool employed alongside MDA variable importance when there is some correlation structure present in the predictors. In general, MDA variable importance will be able to evaluate the importance of independent predictors, and if there is some correlation structure that leads to unstable MDA variable importance values of the correlated predictors, then AVPI can be used on just the correlated predictors to determine which of the correlated predictors are important. \par 

One issue with the AVPI method is the relative noisiness of the method. As observed in the simulation using sampling with replacement, uninformative variables can be assigned positive AVPI scores. Looking at results from re-running the simulation using sampling without replacement, we see that the same sort-of noisiness is also present in AVPI computed via random forests using sampling without replacement. Therefore the issue of noisiness for the AVPI method is not simply a matter of sampling with replacement versus sampling without replacement. The particular issue with the noisiness of the AVPI method is that it prevents us from readily employing permutation tests to determine importance of variables using AVPI as a test statistic in a hypothesis testing framework. In particular, looking at the tables of AVPI p-values for both sampling with replacement and sampling without replacement, we see that both tables are essentially identical, and that under a hypothesis testing framework, we would be committing type I errors across all simulations using a significance level of $\alpha = 0.05$. Furthermore, looking at the simulated null distribution of AVPI values, we see that the null distributions are generally quite similar. The simulated null distribution of AVPI values is generally symmetric with the lower end of the tails at around an importance score of 25. So any AVPI score above 30 would likely appear statistically significant using a permutation test with AVPI as the test statistic. \par

```{r results="asis", echo = FALSE, warning = FALSE}

kable(extract_pval_matrix(numvar = 12, numsiml = 8, data.list = list.wrep), 
      format = "latex", booktabs = TRUE,
      caption = "Added Variable Importance P-values for Sampling with Replacement") %>%
          kable_styling(latex_options = "scale_down")

```

```{r results="asis", echo = FALSE, warning = FALSE}

kable(extract_pval_matrix(numvar = 12, numsiml = 8, data.list = list.worep), 
      format = "latex", booktabs = TRUE,
      caption = "Added Variable Importance P-values for Sampling without Replacement") %>%
          kable_styling(latex_options = "scale_down")

```

```{r echo = FALSE, warning = FALSE, fig.height=3, fig.width=6}

rf_plot_var_imp(list.worep[[1]])

```

Thus while the AVPI may not be a stable test statistic for use in a hypothesis testing framework, our simulation results suggest that when used in conjunction with the MDA variable importance, the AVPI can be a useful tool for dealing with correlated predictors in a regression setting. \par


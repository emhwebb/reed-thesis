---
author: 'Emerson Hikari Webb'
date: 'May 2018'
institution: 'Reed College'
division: 'Mathematics and Natural Sciences'
advisor: 'Andrew Bray'
#altadvisor: 'Your Other Advisor'
# Delete line 6 if you only have one advisor
department: 'Mathematics'
degree: 'Bachelor of Arts'
title: 'Added Variable Plot Importance and Joint Added Variable Plot Importance Measures for Random Forests'
knit: "bookdown::render_book"
site: bookdown::bookdown_site
output: 
  thesisdown::thesis_pdf: default
#  thesisdown::thesis_gitbook: default
#  thesisdown::thesis_word: default
#  thesisdown::thesis_epub: default
# If you are creating a PDF you'll need to write your preliminary content here or
# use code similar to line 20 for the files.  If you are producing in a different
# format than PDF, you can delete or ignore lines 20-31 in this YAML header.
abstract: |
  `r if(knitr:::is_latex_output()) paste(readLines("00-abstract.Rmd"), collapse = '\n  ')`
# If you'd rather include the preliminary content in files instead of inline
# like below, use a command like that for the abstract above.  Note that a tab is 
# needed on the line after the |.
acknowledgements: |
  This thesis would not have been possible to write without a great deal of support and encouragement from friends and family. In particular, I would like to thank Mom and Dad for helping get me this far. I love you two. I would like to thank Claire and Alli, along with Doctor Watson and Charlie, for all the games of pool, dumb jokes, tomfoolery and shenanigans this past year. I would like to thank Reid, Sam, Aidan, and Jacob for the years of friendship, laughs, and adventures. Here's to many more. I would like to thank Josh, Chloe, Harris, Harry, JR, and Suki for always keeping me on my feet and being good friends. I would like to thank Andrew for his guidance throughout this project, and on the advice throughout the year.  I also would like to thank all my friends in the Math department for the years of fun mathematics.  
bibliography: bib/thesis.bib
# Download your specific bibliography database file and refer to it in the line above.
csl: csl/apa.csl
# Download your specific csl file and refer to it in the line above.
lot: true
lof: true
#space_between_paragraphs: true
# Delete the # at the beginning of the previous line if you'd like
# to have a blank new line between each paragraph
header-includes:
- \usepackage{tikz, algorithm}
- \usepackage[noend]{algpseudocode}
---

<!--
Above is the YAML (YAML Ain't Markup Language) header that includes a lot of metadata used to produce the document.  Be careful with spacing in this header!

If you'd prefer to not include a Dedication, for example, simply delete lines 17 and 18 above or add a # before them to comment them out.  If you have other LaTeX packages you would like to include, delete the # before header-includes and list the packages after hyphens on new lines.

If you'd like to include a comment that won't be produced in your resulting file enclose it in a block like this.
-->

<!--
If you receive a duplicate label error after knitting, make sure to delete the index.Rmd file and then knit again.
-->

```{r include_packages, include = FALSE}
# This chunk ensures that the thesisdown package is
# installed and loaded. This thesisdown package includes
# the template files for the thesis.
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")
if(!require(thesisdown))
  devtools::install_github("ismayc/thesisdown")
library(thesisdown)
```

<!-- You'll need to include the order that you'd like Rmd files to appear in the _bookdown.yml file for
PDF files and also delete the # before rmd_files: there.  You'll want to not include 00(two-hyphens)prelim.Rmd
and 00-abstract.Rmd since they are handled in the YAML above differently for the PDF version.
-->

<!-- The {.unnumbered} option here means that the introduction will be "Chapter 0." You can also use {-} for no numbers
on chapters.
-->

# Introduction {.unnumbered}

This thesis is about utilizing the predictive capabilities of random forests for statistical inference via variable importance measures. The random forest is a statistical learning algorithm developed by Leo Breiman and his collaborators in the early 2000's that leverages bagging and CART (Classification and Regression Trees) methodology to produce quite good predictions of an underlying classification or regression surface. Random forests often outperform classical linear models, are simple to set-up and train, and come with few assumptions about the underlying data generating process. For this thesis, we are interested in the use of random forest variable importance measures for statistical inference. More specifically, random forest variable importance measures are often unstable with respect to correlated predictors, so our focus is on developing variable importance measures that can produce stable results even when there are correlated predictors in the dataset.   \par

  Machine learning approaches to regression can often produce models with good predictive accuracy compared to parametric modelling approaches. However, machine learning algorithms often lack in the interpretability and inferential capabilities of more traditional statistical modelling. The challenge of developing descriptive and inferential tools for machine learning algorithms is in finding a balance between complexity and interpretability. While we may want a variable importance measure that fully utilizes the predictive capabilities of the algorithm, we also want the variable importance measure to be simple enough for statistical practitioners to use and interpret. \par
  
Throughout this thesis we will be primarily interested in the regression settings for random forests. While random forests are capable of handling both regression and classification responses, our discussion will be simpler if we focus just on the regression setting. Also, we will be assuming that for the data at hand, there exists an underlying regression function $Y=f(X)+\varepsilon$ where $f(X)$ is an arbitrary, not necessarily linear function, and $\varepsilon$ is generally assumed to be Gaussian error, but may have some other error structure. \par
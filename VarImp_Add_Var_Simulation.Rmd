---
title: "Simulation for Added Variable Plot then Variable Importance Scheme"
author: "Emerson Webb"
date: "3/14/2018"
output: github_document
---

```{r, echo=FALSE, message=FALSE, warnings = FALSE}

library(randomForest)
library(tidyverse)
library(grid)
library(gridExtra)
library(mvtnorm)
library(partykit)

```


Next want to write a function which takes a dataframe and applys the added-variable plot then permute scheme to each predictor in the dataset. 

```{r}

#first need to take our data frame and make the p+1 datasets that we apply the random forest to

#itr_col takes as input an integer and removes that column from the dataset
#note: should make itr_col more robust
itr_col <- function(i, data){
  select(data,-i)
}

#df_combs is a function which takes as input a dataframe and returns all iterations of the
#dataframe where one predictor has been removed. Output is a p+1 element list where each element
#is a dataframe with the ith variable removed. the last element of the list is the entire dataframe
df_combs <- function(data){
  p <- ncol(data)-1
  df.list <- map(1:p, itr_col, data = data)
  df.list[[p+1]] <- data
  df.list
}

#extract_rf_pred is a function which takes as input 
#a list of randomforest objects, an index value, 
#and the number of predictors in the model and 
#outputs the predicted values of the random forest in 
#a dataframe. extract_rf_pred is mainly for use in map_dfc, 
#which binds by column the predicted values in 
#data.list as a dataframe. Primarily for use in each_pred_rf
extract_rf_pred <- function(i, data.list, p){
 new.df <- as.data.frame(data.list[[i]]$predicted) 
 colnames(new.df) <- ifelse(i <= p, paste("PredWoVar", as.character(i), sep = ""),
                            "PredFullMod")
 new.df
}

#each_pred_rf is a function whose input is a list of dataframes 
#that come from output of df_combs and runs the 
#randomForest function on each dataframe using the map function. 
#Output is dataframe of predicted values along with actual value of Y as last column. 
#Second to last column is the predicted value of full model. 
each_pred_rf <- function(data.list, ntree1, replace = TRUE){
  p <- length(data.list)-1
  rf.list <- map(.x = data.list, function(x) 
    randomForest(Y~., data = x, ntree = ntree1, replace = replace, importance = TRUE))
  rf.df <- map_dfc(1:(p+1), extract_rf_pred, data.list = rf.list, p = p)
  Y <- data.list[[1]]$Y
  new.df <- cbind(rf.df, Y)
  imp <- importance(rf.list[[p+1]])
  list(new.df, imp)
}

#extract_add_var takes as input an index value and data frame and 
#outputs a dataframe of the basic added variable data frame where 
#x.res is the difference between predicted values of full model and 
#predicted values of model with out jth variable
#y.res is the residual of Y and predicted values of model without jth variable.
#For use with map in rf_add_var
extract_add_var <-function(i, df){
  PredFullMod <- as.name("PredFullMod")
  Y <- as.name("Y")
  V <- as.name(paste("PredWoVar", as.character(i), sep =""))
  x.res <- df[[PredFullMod]]-df[[V]]
  y.res <- df[[Y]]-df[[V]]
  new.df <- data.frame(x.res, y.res)
  colnames(new.df) <- c(paste("added.Var", as.character(i), sep = ""), "y.res")
  new.df
}

#rf_add_var takes as input the output of each_pred_rf and outputs a list of length p 
#in which each entry is a data frame corresponding
#to an added variable plot for the jth predictor in the 
#data set.
rf_add_var <- function(data.list){
  rf.df <- data.list[[1]]
  p <- length(rf.df)-2
  add.var.list <- map(1:p, extract_add_var, df = rf.df)
  add.var.list
}

#rf_add_var_imp takes as input a list of add_var df's from rf_add_var and runs
#a random forest on the y-residuals with x-residuals as input.
#output is a list of random forest objects. \
#Might change output to be just variable importance values. 
rf_add_var_imp <- function(data.list, ntree2, replace = TRUE){
  p <- length(data.list)
  rf.add.imp.list <- map(.x = data.list, function(x) 
    randomForest(y.res~., data = x, ntree = ntree2, replace = replace, importance = TRUE))
  rf.add.imp.list
}

rf_shallow_add_var_imp <- function(data.list, ntree2, replace = TRUE){
  p <- length(data.list)
  rf.add.imp.list <- map(.x = data.list, function(x) 
    randomForest(y.res~., data = x, ntree = ntree2, replace = replace, importance = TRUE, nodesize = 3))
  rf.add.imp.list
}

extract_imp <-function(i, data.list){
  new.df <- as.data.frame(t(importance(data.list[[i]])))
  new.df
}

extract_var_imp <- function(data.list){
  p <- length(data.list)
  new.df <- map_dfc(1:p, extract_imp, data.list = data.list)
  rownames(new.df) <- c("%IncMSE", "IncNodePurity")
  as.data.frame(t(new.df))
}


#Once rf has been run once on each added variable plot, we can try to assess
#importance via framework of p-values. In particular, we implement a 
#permutation test to obtain distribution of variable importance scores 
#rf_perm takes as input a list of added variable dataframes 
#(in particular output of rf_add_var
#and outputs a list of dataframes consisting of variable importance scores 
#obtained after permuting each added variable dataframe. 
#number of permutations is it input for rf_perm

#perm_rf takes as input a dataframe, permutes the dataframe
#runs a randomforest and returns the variable importance score
#variable importance score is MDA (mean decrease in accuracy) as a percentage change
perm_rf <-function(df, ntree3, replace){
  df.names <- colnames(df)
  df.mat <- as.matrix(df)
  x <- df.mat[sample(nrow(df.mat),replace = FALSE),1]
  y <- df.mat[,2]
  perm.df <- as.data.frame(cbind(x,y))
  colnames(perm.df) <- df.names
  perm.rf <- randomForest(y.res~., data = perm.df, replace = replace, importance = TRUE)
  importance(perm.rf, type = 1)
}

#perm_add_var takes as input an index value, the data.list, it the number of permutations, 
#ntree3 the number of trees to grow for each forest on the permuted data
perm_add_var <- function(i, data.list, it, ntree3, replace){
  df <- data.list[[i]]
  new.df <- as.data.frame(replicate(n = it, expr = perm_rf(df = df, 
                                                           ntree3 = ntree3, replace = replace)))
  colnames(new.df) <- paste("Var", as.character(i), "VI", sep = "")
  new.df
}

rf_perm_add_var <- function(data.list, it, ntree3, replace = TRUE){
  p <- length(data.list)
  new.list <- map(1:p, perm_add_var, data.list = data.list, 
      it = it, ntree3 = ntree3, replace = replace)
  new.list
}

#add_var_randomforest is a wrapper for the previous functions (exluding rf_perm_add_var)
add_var_randomforest <- function(data, ntree1, ntree2, replace = TRUE){
  #to get the copy of the data with one predictor removed
  df.list <- df_combs(data)
  #running initial randomFoest on each data frame
  rf.list <- each_pred_rf(df.list, ntree1 = ntree1, replace = replace)
  #tidying the data to compute added variable plots
  rf.add.var.list <- rf_add_var(rf.list)
  #running randomForests on each plot
  rf.add.imp.list <- rf_add_var_imp(rf.add.var.list, ntree2 = ntree2, replace = replace)
  #extracts variable importance values for each randomForest ran on added variable plots
  add.var.imp <- extract_var_imp(rf.add.imp.list)
  #output is list containing data frame of added variable plot variable importances, 
  #rf.add.var.list which are dataframe for added variable plots, and 
  #the variable importances from full model run in each_pred_rf
  list(add.var.imp, rf.add.var.list, rf.list[[2]])
}

#perm_add_var_randomforest is a wrapper for previous functions (including rf_perm_add_var)
perm_add_var_randomforest <- function(data, it, ntree1, ntree2, ntree3, replace = TRUE){
  df.list <- df_combs(data)
  #running initial randomFoest on each data frame
  rf.list <- each_pred_rf(df.list, ntree1 = ntree1, replace = replace)
  #tidying the data to compute added variable plots
  rf.add.var.list <- rf_add_var(rf.list)
  #running randomForests on each plot
  rf.add.imp.list <- rf_add_var_imp(rf.add.var.list, 
                                    ntree2 = ntree2, replace = replace)
  #extracts variable importance values for each randomForest ran on added variable plots
  add.var.imp <- extract_var_imp(rf.add.imp.list)
  #run permutations on each dataframe in rf.add.var.list to obtain 
  #distribution of importance values
  rf.perm.add.var.list <- rf_perm_add_var(data.list = rf.add.var.list, 
                                          it = it, ntree3 = ntree3, replace = replace)
  list(add.var.imp, rf.add.var.list, rf.list[[2]], rf.perm.add.var.list)
}

#input for rf_added_var_plot is output of 
#add_var_randomforest. Output is plot of added variable plots for 
#the random forest arranged in a grid.

plot_add_var <- function(i, df.list){
  df <- df.list[[i]]
  x.name <- colnames(df)[1]
  y.name <- colnames(df)[2]
  plot.obj <- ggplot(df, aes_string(x = x.name, y = y.name))+geom_point()
  plot.obj
}

rf_added_var_plot <- function(data.list){
    df.list <- data.list[[2]]
    p <- length(df.list)
    gg.list <- map(1:p, plot_add_var, df.list = df.list)
    nCol <- floor(sqrt(p))
    do.call("grid.arrange", c(gg.list, ncol = nCol))
}

#next make functions which plot distributions of the added variable importances and 
#adds in observed variable importance of added variable

plot_var_imp <- function(i, df.list, add.var.imp){
  df <- df.list[[i]]
  x.name <- colnames(df)
  obs.add.var <- add.var.imp[i,1]
  plot.obj <- ggplot(df, aes_string(x = x.name))+
    geom_histogram(bins = 50)+
    geom_vline(xintercept = obs.add.var, col = "Red", size = 1)
  plot.obj
}

rf_plot_var_imp <- function(data.list){
 df.list  <-data.list[[4]]
 add.var.imp <- data.list[[1]]
 p <- length(df.list)
 gg.list <- map(1:p, plot_var_imp, df.list = df.list, add.var.imp = add.var.imp)
 nCol <- floor(sqrt(p))
 do.call("grid.arrange", c(gg.list, ncol = nCol))
}

#Next define a function which takes the output of perm_add_var_randomforest
#and computes p-values of variable importances.

perm_pval <- function(i, var.imp.df, var.imp.list){
  obs.var.imp <- var.imp.df[i,1]
  perm.var.imp <- var.imp.list[[i]]
  n.perm <- nrow(perm.var.imp)
  right.tail <- (sum(obs.var.imp <= perm.var.imp)+1)/(n.perm+1)
  left.tail <- (sum(perm.var.imp <= obs.var.imp)+1)/(n.perm+1)
  p.val <- ifelse(right.tail<=left.tail, 2*right.tail, 2*left.tail)
  p.val <- ifelse(1<p.val, 1, p.val)
  names(p.val) <- paste("p.val.Var.", as.character(i), sep = "")
  p.val
}

add_var_pval <- function(data.list){
  var.imp.df <- data.list[[1]]
  var.imp.list <- data.list[[4]]
  p <- nrow(var.imp.df)
  pval.df <- sapply(1:p, perm_pval, var.imp.df = var.imp.df, var.imp.list = var.imp.list)
  as.data.frame(pval.df)
}

```


Now ready to run simulations. First and second simulated datasets are from Strobl (2008). 
First simulated dataset is Strobl's with iid variables.
Second simulated dataset is Strobl's with correlated variables.
Both are cases of a response in which the effect of added variables is linear. 

```{r}
#variables for simulations

n <- 2000
p <- 12
ntree.sim1 <- 300
ntree.sim2 <- 300
ntree.sim3 <- 300
it.sim <- 1000

```


```{r}

set.seed(13)
#iid
#n1 is number of observations in dataset
n1 <- n
#p1 is number of variables
p1 <- p
#our weights are given by wt_vec <- c(5,5,2,0,-5,-5,-2,0,0,0,0,0) 
err1 <- rnorm(n1, mean = 0, sd = 0.05)
siml.data1 <- as.data.frame(matrix(rnorm(n1*p1, mean = 0, sd = 1), nrow = n1))
siml.data1 <- siml.data1 %>% 
  mutate(Y = 5*V1+5*V2+2*V3+-5*V5+-5*V6+-2*V7+err1)

#correlated
#n2 is number of observations in dataset
n2 <- n
#p2 is number of variables
#p2 <- 12
#our weights are given by wt_vec <- c(5,5,2,0,-5,-5,-2,0,0,0,0,0) 
#covariance matrix
v1 <- c(1,0.9,0.9,0.9)
v2 <- c(0.9,1,0.9,0.9)
v3 <- c(0.9,0.9,1,0.9)
v4 <- c(0.9,0.9,0.9,1)
w <- rep(0, times = 8)
u1 <- c(rep(0, times = 4), 1, rep(0, times = 7))
u2 <- c(rep(0, times = 5), 1, rep(0, times = 6))
u3 <- c(rep(0, times = 6), 1, rep(0, times = 5))
u4 <- c(rep(0, times = 7), 1, rep(0, times = 4))
u5 <- c(rep(0, times = 8), 1, rep(0, times = 3))
u6 <- c(rep(0, times = 9), 1, 0,0)
u7 <- c(rep(0, times = 10), 1, 0)
u8 <- c(rep(0, times = 11), 1)

Sigma2 <- matrix(c(v1,w,v2,w,v3,w,v4,w,
                   u1,u2,u3,u4,u5,u6,u7,u8), nrow = 12, ncol = 12)
err2 <- rnorm(n2, mean = 0, sd = 0.05)
siml.data2 <- as.data.frame(rmvnorm(n = n2, mean = rep(0, times = 12), sigma = Sigma2))
siml.data2 <- siml.data2 %>% 
  mutate(Y = 5*V1+5*V2+2*V3+-5*V5+-5*V6+-2*V7+err2)

#run the simulations with and without replacement for bootstrapping
siml1.wrep.results <- perm_add_var_randomforest(data = siml.data1, it = it.sim, ntree1 = ntree.sim1, 
                                                ntree2 = ntree.sim2, ntree3 = ntree.sim3)
siml1.worep.results <- perm_add_var_randomforest(data = siml.data1, it = it.sim, ntree1 = ntree.sim1, 
                                                 ntree2 = ntree.sim2, ntree3 = ntree.sim3, replace = FALSE)
siml2.wrep.results <- perm_add_var_randomforest(data = siml.data2, it = it.sim, ntree1 = ntree.sim1, 
                                                ntree2 = ntree.sim2, ntree3 = ntree.sim3)
siml2.worep.results <- perm_add_var_randomforest(data = siml.data2, it = it.sim, ntree1 = ntree.sim1, 
                                                 ntree2 = ntree.sim2, ntree3 = ntree.sim3, replace = FALSE)


```

```{r}

siml1.wrep.results[[1]]
siml1.wrep.results[[3]]
rf_added_var_plot(siml1.wrep.results)
rf_plot_var_imp(siml1.wrep.results)
add_var_pval(siml1.wrep.results)

```

Above are results for the first simulated dataset with replacement. The added variable plot does seem to be useful in capturing linear relationships, even with using the random forest mechanism. In terms of variable importance scores, the original forest performs better. Part of this seems to be that even non-significant variables will be assigned a non-trivial importance scores within the forest that splits only on the difference between the full model and the model excluding the particular variable. 

```{r}
siml1.worep.results[[1]]
siml1.worep.results[[3]]
rf_added_var_plot(siml1.worep.results)
rf_plot_var_imp(siml1.worep.results)
add_var_pval(siml1.worep.results)

```

Sampling without replacement as opposed to bootstrap withreplacement seems to improve performance of added variable then variable importance scheme. Overall, if the variables are independent, then the random forest variable importance score seems quite reliable. 

```{r}

siml2.wrep.results[[1]]
siml2.wrep.results[[3]]
rf_added_var_plot(siml2.wrep.results)
rf_plot_var_imp(siml2.wrep.results)
add_var_pval(siml2.wrep.results)

```

With correlated predictors, the added variable plots does seem to improve the performance of the variable importance somewhat. Variables 1 through 4 are correlated, so their raw variable importance score is dampened by that fact. When using added variable then variable importance scheme, variable 1 and 2 look particularly important, which we expect. The score for variable 3 increases, but the scoresfor variables 8 through 12 also increase. 

```{r}

siml2.worep.results[[1]]
siml2.worep.results[[3]]
rf_added_var_plot(siml2.worep.results)
rf_plot_var_imp(siml2.worep.results)
add_var_pval(siml2.worep.results)

```

Sampling without replacement improves the performance of the added variable then variable importance scheme, although the issue of irrelvant variables (particularly variables 8 through 12) persists. 

```{r}

#independent variables with a non-linear response
#n3 is number of observations
n3 <- n
#p3 is numberof variables
p3 <- p
#some error to add in 
err3 <- rnorm(n3, mean = 0, sd = 0.05)
siml.data3 <- as.data.frame(matrix(rnorm(n3*p3, mean = 0, sd = 1), nrow = n3))
siml.data3 <- siml.data3 %>% 
  mutate(Y = 5*V1^4+5*V2^3+6*V3^4+5*V5^3+err3)

#correlated variables with a non-linear response
n4 <- 500
err4 <- rnorm(n4, mean = 0, sd = 0.05)
siml.data4 <- as.data.frame(rmvnorm(n = n4, mean = rep(0, times = 12), sigma = Sigma2))
siml.data4 <- siml.data4 %>%
  mutate(Y = 5*V1^4+5*V2^3+6*V3^4+5*V5^3+err4)

siml3.wrep.results <- perm_add_var_randomforest(data = siml.data3, it = it.sim, ntree1 = ntree.sim1, 
                                                ntree2 = ntree.sim2, ntree3 = ntree.sim3)
siml3.worep.results <- perm_add_var_randomforest(data = siml.data3, it = it.sim, ntree1 = ntree.sim1, 
                                                 ntree2 = ntree.sim2, ntree3 = ntree.sim3, replace = FALSE)
siml4.wrep.results <- perm_add_var_randomforest(data = siml.data4, it = it.sim, ntree1 = ntree.sim1, 
                                                ntree2 = ntree.sim2, ntree3 = ntree.sim3)
siml4.worep.results <- perm_add_var_randomforest(data = siml.data4, it = it.sim, ntree1 = ntree.sim1, 
                                                 ntree2 = ntree.sim2, ntree3 = ntree.sim3, replace = FALSE)

```


```{r}
#Simulation of non-linear response with iid variables and bootstrap resampling 
#with replacement

siml3.wrep.results[[1]]
siml3.wrep.results[[3]]
rf_added_var_plot(siml3.wrep.results)
rf_plot_var_imp(siml3.wrep.results)
add_var_pval(siml3.wrep.results)

```
Appears to be in this scenario, that full random forest offers better interpretation of important variables than added variable importance scheme. This is likely due to variability introduced by bootstrap resampling with replacement, 

```{r}

#Simulation of non-linear response with iid variables and 
#bootstrap resampling without replacement
siml3.worep.results[[1]]
siml3.worep.results[[3]]
rf_added_var_plot(siml3.worep.results)
rf_plot_var_imp(siml3.worep.results)
add_var_pval(siml3.worep.results)

```

In this scenario, appears that added variable importance and the full random forest offer similar interpretations of which the influential variables are. In particular, we see that variables 1, 2, 3, and 5 are all assigned high variable importance scores as expected. 

```{r}

#simulation with non-linear response and correlated predictors
#using bootstrap resampling with replacement
siml4.wrep.results[[1]]
siml4.wrep.results[[3]]
rf_added_var_plot(siml4.wrep.results)
rf_plot_var_imp(siml4.wrep.results)
add_var_pval(siml4.wrep.results)

```

With correlated predictors and non-linear response, the full random forest variable importance has trouble determining which variables are important, in particular, variable 4 appears to be more important than it actually is. The added variable importance is able to separate out the importance of variables 1, 3, and 5, but has difficulties with determining variable 2 is important. Examining the added variable plots, we see that the added variable plot for variable 2 has a similar shape to the added variable plot for variable 4.  

```{r}

#simulation with non-linear response and correlated predictors
#using bootstrap resampling without replacement
siml4.worep.results[[1]]
siml4.worep.results[[3]]
rf_added_var_plot(siml4.worep.results)
rf_plot_var_imp(siml4.worep.results)
add_var_pval(siml4.worep.results)

```



```{r}
#Interactions 

#iid dataset for interactions 
n5 <- n
p5 <- p
err5 <- rnorm(n5, mean = 0, sd = 0.05)
siml.data5 <- as.data.frame(matrix(rnorm(n5*p5, mean = 0, sd = 1), nrow = n5))
siml.data5 <- siml.data5 %>%
  mutate(Y = 8*V1*V2+7*V5*V6+err5)


#Correlated variables for interactions 
n6 <- n
err6 <- rnorm(n6, mean = 0 , sd = 0.05)
siml.data6 <- as.data.frame(rmvnorm(n = n6, mean = rep(0, times = 12), sigma = Sigma2))
siml.data6 <- siml.data6 %>%
  mutate(Y = 8*V1*V2+7*V5*V6+err6)

siml5.wrep.results <- perm_add_var_randomforest(data = siml.data5, it = it.sim, ntree1 = ntree.sim1, 
                                                ntree2 = ntree.sim2, ntree3 = ntree.sim3)
siml5.worep.results <- perm_add_var_randomforest(data = siml.data5, it = it.sim, ntree1 = ntree.sim1, 
                                                 ntree2 = ntree.sim2, ntree3 = ntree.sim3, replace = FALSE)
siml6.wrep.results <- perm_add_var_randomforest(data = siml.data6, it = it.sim, ntree1 = ntree.sim1, 
                                                ntree2 = ntree.sim2, ntree3 = ntree.sim3)
siml6.worep.results <- perm_add_var_randomforest(data = siml.data6, it = it.sim, ntree1 = ntree.sim1, 
                                                 ntree2 = ntree.sim2, ntree3 = ntree.sim3, replace = FALSE)

```

```{r}

#simulation of interactions with iid predictors
#and bootstrap resampling with replacement
siml5.wrep.results[[1]]
siml5.wrep.results[[3]]
rf_added_var_plot(siml5.wrep.results)
rf_plot_var_imp(siml5.wrep.results)
add_var_pval(siml5.wrep.results)

```



```{r}

#simulation of interactions with iid predictors
#and bootstrap resampling without replacement
siml5.worep.results[[1]]
siml5.worep.results[[3]]
rf_added_var_plot(siml5.worep.results)
rf_plot_var_imp(siml5.worep.results)
add_var_pval(siml5.worep.results)

```

For interactions with iid predictors, the full random forest variable importance and added variable importance perform similarly in terms of determining which variables are important. Bootstrap resampling without replacement does not seem to offer performance benefits in this particular scenario. 

```{r}

#simulation of interactions with correlated predictors
#and bootstrap resampling with replacement
siml6.wrep.results[[1]]
siml6.wrep.results[[3]]
rf_added_var_plot(siml6.wrep.results)
rf_plot_var_imp(siml6.wrep.results)
add_var_pval(siml6.wrep.results)

```


```{r}

#simulation of interactions with correlated predictors
#and bootstrap resampling without replacement
siml6.worep.results[[1]]
siml6.worep.results[[3]]
rf_added_var_plot(siml6.worep.results)
rf_plot_var_imp(siml6.worep.results)
add_var_pval(siml6.worep.results)

```

With interacting terms using correlated predictors, the random forest seems to overinflate of the irrelevant predictors that are correlated with variables 1 and 2 (which are actually important). In this regard, the added variable importance correctly deflates the importance of the irrelevant correlated predictors. Sampling with and without replacement seem to offer similar performance in this scenario. 

```{r}

#independent non-linear Predictors
n7 <- n
p7 <- p
err7 <- rnorm(n7, mean = 0, sd = 0.05)
siml.data7 <- as.data.frame(matrix(rnorm(n7*p7, mean = 0, sd = 1), nrow = n7))
siml.data7 <- siml.data7 %>%
  mutate(Y = 3^V1+2^V2+4^V5+err7)


#Correlated variables for strongly non-linear predictors 
n8 <- n
err8 <- rnorm(n8, mean = 0 , sd = 0.05)
siml.data8 <- as.data.frame(rmvnorm(n = n8, mean = rep(0, times = 12), sigma = Sigma2))
siml.data8 <- siml.data8 %>%
  mutate(Y = 3^V1+2^V2+4^V5+err8)


```


```{r}

siml7.wrep.results <- perm_add_var_randomforest(data = siml.data7, it = it.sim, ntree1 = ntree.sim1, 
                                                ntree2 = ntree.sim2, ntree3 = ntree.sim3)
siml7.worep.results <- perm_add_var_randomforest(data = siml.data7, it = it.sim, ntree1 = ntree.sim1, 
                                                 ntree2 = ntree.sim2, ntree3 = ntree.sim3, replace = FALSE)
siml8.wrep.results <- perm_add_var_randomforest(data = siml.data8, it = it.sim, ntree1 = ntree.sim1, 
                                                ntree2 = ntree.sim2, ntree3 = ntree.sim3)
siml8.worep.results <- perm_add_var_randomforest(data = siml.data8, it = it.sim, ntree1 = ntree.sim1, 
                                                 ntree2 = ntree.sim2, ntree3 = ntree.sim3, replace = FALSE)

```

```{r}

#independent strongly non-linear predictors
#with sampling with replacement
siml7.wrep.results[[1]]
siml7.wrep.results[[3]]
rf_added_var_plot(siml7.wrep.results)
rf_plot_var_imp(siml7.wrep.results)
add_var_pval(siml7.wrep.results)

```

```{r}

#independent strongly non-linear predictors 
#with sampling without replacement
siml7.worep.results[[1]]
siml7.worep.results[[3]]
rf_added_var_plot(siml7.worep.results)
rf_plot_var_imp(siml7.worep.results)
add_var_pval(siml7.worep.results)

```

```{r}

#correlated strongly non-linear predictors 
#with sampling with replacement
siml8.wrep.results[[1]]
siml8.wrep.results[[3]]
rf_added_var_plot(siml8.wrep.results)
rf_plot_var_imp(siml8.wrep.results)
add_var_pval(siml8.wrep.results)

```

```{r}

#correlated strongly non-linear predictors
#with sampling without replacement
siml8.worep.results[[1]]
siml8.worep.results[[1]]
rf_added_var_plot(siml8.worep.results)
rf_plot_var_imp(siml8.worep.results)
add_var_pval(siml8.worep.results)

```

```{r}

full.rand.forest <- randomForest(Y~., data = siml.data1, ntree = 2000, replace = FALSE, importance = TRUE)

cor(x = full.rand.forest$predicted, siml.data1$Y)

rand.for.siml.1 <- add_var_randomforest(data = siml.data1, ntree1 = 2000, ntree2 = 2000, replace = FALSE)

siml.data5.list <- df_combs(siml.data5)
siml.data6.list <- df_combs(siml.data6)
each.rf.5 <- each_pred_rf(data.list = siml.data5.list, ntree1 = 2000, replace = FALSE)
each.rf.6 <- each_pred_rf(data.list = siml.data6.list, ntree1 = 2000, replace = FALSE)
  
rand.for.siml.5 <- add_var_randomforest(data = siml.data5, ntree1 = 2000, ntree2 = 2000, replace = FALSE)
rand.for.siml.6 <- add_var_randomforest(data = siml.data6, ntree1 = 2000, ntree2 = 2000, replace = FALSE)

siml.data1.list <- df_combs(siml.data1)

each.rf <- each_pred_rf(data.list = siml.data1.list, ntree1 = 2000, replace = FALSE)

cor(x = each.rf[[1]]$PredFullMod, y = each.rf[[1]]$PredWoVar9)


add_var_corr <- function(i, add.var.list){
  df <- add.var.list[[i]]
  as.data.frame(cor(x = df[,1], y = df[,2]))
}

add_var_cov <- function(i, add.var.list){
  df <- add.var.list[[i]]
  as.data.frame(cov(x = df[,1], y = df[,2]))
}

rand_for_corr <- function(i, rand.for.df){
 as.data.frame(cor(x = rand.for.df[,i], y = rand.for.df$Y))
}

rand_for_cov <- function(i, rand.for.df){
 as.data.frame(cov(x = rand.for.df[,i], y = rand.for.df$Y))
}

rand_for_var <- function(i, rand.for.df){
  as.data.frame(var( x = rand.for.df[,i]))
}

full_mod_cov <- function(i, rand.for.df){
 as.data.frame(cov(x = rand.for.df[,i], y = rand.for.df$PredFullMod)) 
}

full_mod_cor <- function(i, rand.for.df){
 as.data.frame(cor(x = rand.for.df[,i], y = rand.for.df$PredFullMod)) 
}

map_dfr(1:(length(each.rf[[1]])-1), rand.for.df = each.rf[[1]], rand_for_corr)
map_dfr(1:(length(each.rf[[1]])-1), rand.for.df = each.rf[[1]], rand_for_cov)
map_dfr(1:(length(each.rf[[1]])), rand.for.df = each.rf[[1]], rand_for_var)
map_dfr(1:(length(each.rf[[1]])-2), rand.for.df = each.rf[[1]], full_mod_cor)
map_dfr(1:(length(each.rf[[1]])-2), rand.for.df = each.rf[[1]], full_mod_cov)
map_dfr(1:length(rand.for.siml.1[[2]]), add.var.list = rand.for.siml.1[[2]], add_var_corr)
map_dfr(1:length(rand.for.siml.1[[2]]), add.var.list = rand.for.siml.1[[2]], add_var_cov)

map_dfr(1:(length(each.rf.5[[1]])-1), rand.for.df = each.rf.5[[1]], rand_for_corr)
map_dfr(1:(length(each.rf.5[[1]])-1), rand.for.df = each.rf.5[[1]], rand_for_cov)
map_dfr(1:(length(each.rf.5[[1]])), rand.for.df = each.rf.5[[1]], rand_for_var)
map_dfr(1:(length(each.rf.5[[1]])-2), rand.for.df = each.rf.5[[1]], full_mod_cor)
map_dfr(1:(length(each.rf.5[[1]])-2), rand.for.df = each.rf.5[[1]], full_mod_cov)
map_dfr(1:length(rand.for.siml.5[[2]]), add.var.list = rand.for.siml.5[[2]], add_var_corr)
map_dfr(1:length(rand.for.siml.5[[2]]), add.var.list = rand.for.siml.5[[2]], add_var_cov)

map_dfr(1:(length(each.rf.6[[1]])-1), rand.for.df = each.rf.6[[1]], rand_for_corr)
map_dfr(1:(length(each.rf.6[[1]])-1), rand.for.df = each.rf.6[[1]], rand_for_cov)
map_dfr(1:(length(each.rf.6[[1]])), rand.for.df = each.rf.6[[1]], rand_for_var)
map_dfr(1:(length(each.rf.6[[1]])-2), rand.for.df = each.rf.6[[1]], full_mod_cor)
map_dfr(1:(length(each.rf.6[[1]])-2), rand.for.df = each.rf.6[[1]], full_mod_cov)
map_dfr(1:length(rand.for.siml.6[[2]]), add.var.list = rand.for.siml.6[[2]], add_var_corr)
map_dfr(1:length(rand.for.siml.6[[2]]), add.var.list = rand.for.siml.6[[2]], add_var_cov)

```

```{r}
set.seed(7)
n9 <- 100
#p1 is number of variables
p9 <- 4
#our weights are given by wt_vec <- c(5,5,2,0,-5,-5,-2,0,0,0,0,0) 
err9 <- rnorm(n9, mean = 0, sd = 100)
siml.data9 <- as.data.frame(matrix(rnorm(n9*p9, mean = 0, sd = 1), nrow = n9))
siml.data9 <- siml.data9 %>% 
  mutate(Y = 20*V1+20*V2-14*V3-2*V4^12+err9)

full.rf <- randomForest(Y~., data = siml.data9, ntree = 1000, importance = TRUE, replace = FALSE)
wo.v1 <- randomForest(Y~V2+V3+V4, data = siml.data9, ntree = 1000, importance = TRUE, replace = FALSE)
wo.v2 <- randomForest(Y~V1+V3+V4, data = siml.data9, ntree = 1000, importance = TRUE, replace = FALSE)
wo.v3 <- randomForest(Y~V1+V2+V4, data = siml.data9, ntree = 1000, importance = TRUE, replace = FALSE)
wo.v4 <- randomForest(Y~V1+V2+V3, data = siml.data9, ntree = 1000, importance = TRUE, replace = FALSE)
of.v1 <- randomForest(V1~V2+V3+V4, data = siml.data9, ntree = 1000, importance = TRUE, replace = FALSE)
of.v2 <- randomForest(V2~V1+V3+V4, data = siml.data9, ntree = 1000, importance = TRUE, replace = FALSE)
of.v3 <- randomForest(V3~V1+V2+V4, data = siml.data9, ntree = 1000, importance = TRUE, replace = FALSE)
of.v4 <- randomForest(V4~V1+V2+V3, data = siml.data9, ntree = 1000, importance = TRUE, replace = FALSE)

full.lm <- lm(Y~., data = siml.data9)
lm.wo.v1 <- lm(Y~V2+V3+V4, data = siml.data9)
lm.wo.v2 <- lm(Y~V1+V3+V4, data = siml.data9)
lm.wo.v3 <- lm(Y~V1+V2+V4, data = siml.data9)
lm.wo.v4 <- lm(Y~V1+V2+V3, data = siml.data9)
lm.of.v1 <- lm(V1~V2+V3+V4, data = siml.data9)
lm.of.v2 <- lm(V2~V1+V3+V4, data = siml.data9)
lm.of.v3 <- lm(V3~V1+V2+V4, data = siml.data9)
lm.of.v4 <- lm(V4~V1+V2+V3, data = siml.data9)

v1 <- siml.data9$V1
v2 <- siml.data9$V2
v3 <- siml.data9$V3
v4 <- siml.data9$V4

df.v1 <- as.data.frame(cbind(v1-of.v1$predicted, full$predicted-wo.v1$predicted))
df.v2 <- as.data.frame(cbind(v2-of.v2$predicted, full$predicted-wo.v2$predicted))
df.v3 <- as.data.frame(cbind(v3-of.v3$predicted, full$predicted-wo.v3$predicted))
df.v4 <- as.data.frame(cbind(v4-of.v4$predicted, full$predicted-wo.v4$predicted))

colnames(df.v1) <- c("x.res", "y.res")
colnames(df.v2) <- c("x.res", "y.res")
colnames(df.v3) <- c("x.res", "y.res")
colnames(df.v4) <- c("x.res", "y.res")

dfalt.list <- list(df.v1,df.v2,df.v3,df.v4)

rfalt.list <- rf_add_var_imp(dfalt.list, ntree2 = 1000, replace = FALSE) 

ggv1 <- ggplot(df.v1, aes(x = x.res, y = y.res))+geom_point()
ggv2 <- ggplot(df.v2, aes(x = x.res, y = y.res))+geom_point()
ggv3 <- ggplot(df.v3, aes(x = x.res, y = y.res))+geom_point()
ggv4 <- ggplot(df.v4, aes(x = x.res, y = y.res))+geom_point()
do.call("grid.arrange", c(list(ggv1,ggv2,ggv3,ggv4), ncol = 2))

extract_var_imp(rfalt.list)
importance(full.rf)

siml9.wo <- add_var_randomforest(siml.data9, ntree1 = 1000, ntree2 = 1000, replace = FALSE)
rf_added_var_plot(siml9.wo)
siml9.wo[[1]]
siml9.wo[[3]]


lm.df.v1 <- as.data.frame(cbind(v1-lm.of.v1$fitted.values, full.lm$fitted.values-lm.wo.v1$fitted.values))
lm.df.v2 <- as.data.frame(cbind(v2-lm.of.v2$fitted.values, full.lm$fitted.values-lm.wo.v2$fitted.values))
lm.df.v3 <- as.data.frame(cbind(v3-lm.of.v3$fitted.values, full.lm$fitted.values-lm.wo.v3$fitted.values))
lm.df.v4 <- as.data.frame(cbind(v4-lm.of.v4$fitted.values, full.lm$fitted.values-lm.wo.v4$fitted.values))

colnames(lm.df.v1) <- c("x.res", "y.res")
colnames(lm.df.v2) <- c("x.res", "y.res")
colnames(lm.df.v3) <- c("x.res", "y.res")
colnames(lm.df.v4) <- c("x.res", "y.res")

ggv1.lm <- ggplot(lm.df.v1, aes(x = x.res, y = y.res))+geom_point()
ggv2.lm <- ggplot(lm.df.v2, aes(x = x.res, y = y.res))+geom_point()
ggv3.lm <- ggplot(lm.df.v3, aes(x = x.res, y = y.res))+geom_point()
ggv4.lm <- ggplot(lm.df.v4, aes(x = x.res, y = y.res))+geom_point()
do.call("grid.arrange", c(list(ggv1.lm,ggv2.lm,ggv3.lm,ggv4.lm), ncol = 2))

```

